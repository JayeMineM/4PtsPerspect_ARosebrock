# -*- coding: utf-8 -*-
"""splitTrain4groundTruth29Apr-May4rd@tfDeco_review2025Apr16-23nice_UoB_FDP_pyFile_workingOn_bbFACEbin_TurningCNNimgClass_intoObjDetect_KerasTFlowOpenCV.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oXvRi8M3wrFzGFfAV-c8fKVP2EakyqhK

#Turning any CNN image classifier into an object detector with Keras, TensorFlow, and OpenCV
by Adrian Rosebrock on June 22, 2020

https://www.pyimagesearch.com/2020/06/22/turning-any-cnn-image-classifier-into-an-object-detector-with-keras-tensorflow-and-opencv/

Click here to download the source code to this post

In this tutorial, you will learn how to take any pre-trained deep learning image classifier and turn it into an object detector using Keras, TensorFlow, and OpenCV.

Today, we’re starting a four-part series on deep learning and object detection:

* Part 1: Turning any deep learning image classifier into an object detector with Keras and TensorFlow (today’s post)
*Part 2: OpenCV Selective Search for Object Detection
*Part 3: Region proposal for object detection with OpenCV, Keras, and TensorFlow
*Part 4: R-CNN object detection with Keras and TensorFlow

The goal of this series of posts is to obtain a deeper understanding of how deep learning-based object detectors work, and more specifically:

1. How traditional computer vision object detection algorithms can be combined with deep learning
2. What the motivations behind end-to-end trainable object detectors and the challenges associated with them are
3. **And most importantly**, how the seminal Faster R-CNN architecture came to be (we’ll be building a variant of the R-CNN architecture throughout this series)

Today, we’ll be starting with the fundamentals of object detection, including how to take a pre-trained image classifier and utilize **image pyramids, sliding windows, and non-maxima suppression** to build a basic object detector (think **HOG**(Histogram of Oriented Gradients) + Linear **SVM**(Support Vector Machine)-inspired).

In the first part of this tutorial, we’ll discuss the key differences between image classification and object detection tasks.

I’ll then show you how you can **take ANY Convolutional Neural Network trained for image classification and then turn it into an object detector,** all in ~200 lines of code.

From there, we’ll implement the code necessary to take an image classifier and turn it into an object detector using Keras, TensorFlow, and OpenCV.

Finally, we’ll review the results of our work, noting some of the problems and limitations with our implementation, including how we can improve this method.

**Image classification vs. object detection**
![](https://www.pyimagesearch.com/wp-content/uploads/2018/05/gentle_guide_obj_det_cls_vs_det.jpg)

**Figure 1:** Left: Image classification. Right: Object detection. In this blog post, we will learn how to turn any deep learning image classifier CNN into an object detector with Keras, TensorFlow, and OpenCV.


[...] --object detection does require a specialized network architecture.

Anyone who has read papers on Faster R-CNN, Single Shot Detectors (SSDs), YOLO, RetinaNet, etc. knows that object detection networks are more complex, more involved, and take multiple orders of magnitude and more effort to implement compared to traditional image classification.

That said, there is a hack we can leverage to turn our CNN image classifier into an object detector — and the secret sauce lies in traditional computer vision algorithms.

Back before deep learning-based object detectors, the state-of-the-art was to use HOG + Linear SVM to detect objects in an image.

We’ll be borrowing elements from [HOG + Linear SVM](https://www.pyimagesearch.com/2014/11/10/histogram-oriented-gradients-object-detection/) to convert any deep neural network image classifier into an object detector.

###**The first key ingredient from HOG + Linear SVM is to use _image pyramids_.**

An “image pyramid” is a **multi-scale representation** of an image:
![](https://www.pyimagesearch.com/wp-content/uploads/2020/06/keras_classifier_object_detector_pyramid_example_2.png)

**Figure 2:** Image pyramids allow us to produce images at different scales. When turning an image classifier into an object detector, it is important to classify windows at multiple scales. We will learn how to write an image pyramid Python generator and put it to work in our Keras, TensorFlow, and OpenCV script.

And at each subsequent layer, **the image is resized (subsampled)** and optionally smoothed (usually via Gaussian blurring).

**The image is progressively subsampled until some stopping criterion is met**, which is normally when a minimum size has been reached and no further subsampling needs to take place.

###**The second key ingredient we need is sliding windows:**
![](https://www.pyimagesearch.com/wp-content/uploads/2014/10/sliding_window_example.gif)
**Figure 3:** We will classify regions of our multi-scale image representations. These regions are generated by means of sliding windows. `The combination of image pyramids and sliding windows allow us to turn any image classifier into an object detector` using Keras, TensorFlow, and OpenCV.

As the name suggests, a sliding window is a fixed-size rectangle that slides from left-to-right and top-to-bottom within an image. (As Figure 3 demonstrates, our sliding window could be used to detect the face in the input image).

At each stop of the window we would:
```
1. Extract the ROI
2. Pass it through our image classifier (ex., Linear SVM, CNN, etc.)
3. Obtain the output predictions
```
**Combined with image pyramids, sliding windows allow us to localize objects at different locations and multiple scales of the input image:




![Turning any CNN image classifier into an object detector with Keras, TensorFlow, and OpenCV - PyImageSearch](<p><a href="https://www.pyimagesearch.com/2020/06/22/turning-any-cnn-image-classifier-into-an-object-detector-with-keras-tensorflow-and-opencv/?wvideo=3g5zxb6jec"><img src="https://embed-fastly.wistia.com/deliveries/240c4fa1e17cb3d6b5f9e9b98b2d160485fa69a4.jpg?image_play_button_size=2x&amp;image_crop_resized=960x421&amp;image_play_button=1&amp;image_play_button_color=1e71e7e0" width="400" height="175" style="width: 400px; height: 175px;"></a></p><p><a href="https://www.pyimagesearch.com/2020/06/22/turning-any-cnn-image-classifier-into-an-object-detector-with-keras-tensorflow-and-opencv/?wvideo=3g5zxb6jec"></a></p>)

###**Combining traditional computer vision with deep learning to build an object detector**
![](https://www.pyimagesearch.com/wp-content/uploads/2020/06/keras_classifier_object_detector_steps.png)
**Figure 6:** The steps to turn a deep learning classifier into an object detector using Python and libraries such as TensorFlow, Keras, and OpenCV.
**BlockDiagram**

In order to take any Convolutional Neural Network trained for image classification and instead utilize it for object detection, we’re going to utilize the three key ingredients for traditional computer vision:

1. **Image pyramids:** Localize objects at different scales/sizes.
2. **Sliding windows:** Detect exactly where in the image a given object is.
3. **Non-maxima suppression:** Collapse weak, overlapping bounding boxes.

              -----------------------------------------
**DJSc_Appendix:**
* HOG
The Histogram of Oriented Gradients (HOG) algorithm is a feature descriptor used in computer vision for object detection and image processing. It works by capturing the structure and texture of an image by focusing on the gradients, which represent changes in intensity. HOG analyzes the orientation and magnitude of these gradients within small, localized regions of an image, then uses these gradients to create a feature vector that represents the image's overall shape and structure.

A histogram of oriented gradient (HOG) is a common descriptor used in computer vision and image processing. It involves decomposing images into a dense array of cells and calculating a histogram of gradients for each cell, which is then normalized by the overlapping of local cell contrast.

##**Configuring your development environment**

Install TensorFlow 2.0

####**JaySTRATEGY**
"""

#JayADD...
#import tensorflow as tf
#device_name = tf.test.gpu_device_name()
#if device_name != '/device:GPU:0':
  #raise SystemError('GPU device not found')
#print('Found GPU at: {}'.format(device_name))

from tensorflow.python.client import device_lib
device_lib.list_local_devices()

!pip show tensorflow

#pip install -U --pre tensorflow=='2.4'

"""##**Project structure**
Once you extract the .zip from the “Downloads” section of this blog post, your directory will be organized as follows:
```
.
├── images
│   ├── hummingbird.jpg
│   ├── lawn_mower.jpg
│   └── stingray.jpg
├── pyimagesearch
│   ├── __init__.py
│   └── detection_helpers.py
└── detect_with_classifier.py
```

Today’s pyimagesearch module contains a Python file — detection_helpers.py — consisting of two helper functions:

* **image_pyramid:** assists in generating copies of our image at different scales so that we can find objects of different sizes
***sliding_window:** helps us find where in the image an object is by sliding our classification window from left-to-right (column-wise) and top-to-bottom (row-wise)

Using the helper functions, our **```detect_with_classifier.py```** Python driver script accomplishes object detection by means of a classifier (using a sliding window and image pyramid approach). The classifier we’re using is a pre-trained ResNet50 CNN (JayN...**my baseline_CNN_32cL21VGGnet trained on Wider_face & INTEL dataset to use...!!!**) trained on the ImageNet dataset. The ImageNet dataset consists of 1,000 classes of objects(DJSc_7classes).

Three images/ are provided for testing purposes. **You should also test this script with images of your own** — given that our classifier-based object detector can recognize 1,000 types of classes, most everyday objects and animals can be recognized. Have fun with it!

##**Implementing our image pyramid and sliding window utility functions**
In order to turn our **CNN image classifier** into an **object detector,** we must first implement **helper utilities** to construct ***sliding windows and image pyramids.**

Let’s implement this helper functions now — open up the **detection_helpers.py** file in the pyimagesearch module, and insert the following code:

https://stackoverflow.com/questions/42249982/systemexit-2-error-when-calling-parse-args/42250234
```
import sys
sys.argv=['']
del sys
```
"""

# import the necessary packages
import imutils

def sliding_window(image, step, ws):
	# slide a window across the image
	for y in range(0, image.shape[0] - ws[1], step):
		for x in range(0, image.shape[1] - ws[0], step):
			# yield the current window
			yield (x, y, image[y:y + ws[1], x:x + ws[0]])

"""For more information on our sliding windows implementation, please refer to my previous [Sliding Windows for Object Detection with Python and OpenCV](https://www.pyimagesearch.com/2015/03/23/sliding-windows-for-object-detection-with-python-and-opencv/) article.

source
```
def image_pyramid(image, scale=1.5, minSize=(224, 224)):
	# yield the original image
	yield image
	# keep looping over the image pyramid
	while True:
		# compute the dimensions of the next image in the pyramid
		w = int(image.shape[1] / scale)
		image = imutils.resize(image, width=w)
		# if the resized image does not meet the supplied minimum
		# size, then stop constructing the pyramid
		if image.shape[0] < minSize[1] or image.shape[1] < minSize[0]:
			break
		# yield the next image in the pyramid
		yield image

```
"""



def image_pyramid(image, scale=1.2, minSize=(224, 224)):
	# yield the original image
	yield image
	# keep looping over the image pyramid
	while True:
		# compute the dimensions of the next image in the pyramid
		w = int(image.shape[1] / scale)
		image = imutils.resize(image, width=w)
		# if the resized image does not meet the supplied minimum
		# size, then stop constructing the pyramid
		if image.shape[0] < minSize[1] or image.shape[1] < minSize[0]:
			break
		# yield the next image in the pyramid
		yield image

import os

"""**To retrieve ```pymagesearch```**"""

if os.path.exists('CNN2objDetect'):
  !rm * CNN2objDetect
!git clone https://github.com/JayeMineM/CNN2objDetect.git

ls

ls CNN2objDetect/pyimagesearch/

pwd

cd CNN2objDetect/

pwd

"""##**Using Keras and TensorFlow to turn a pre-trained image classifier into an object detector**
With our sliding_window and image_pyramid functions implemented, let’s now use them to take a deep neural network trained for image classification and turn it into an object detector.

Open up a new file, name it **detect_with_classifier.py**, and let’s begin coding:
"""

ls

cat pyimagesearch/detection_helpers.py

#!pip install pyimagesearch.detection

#detect_with_classifier.py
# import the necessary packages
#
import cv2
from tensorflow.keras.applications import ResNet50 #use JayN baseline_CNN_34VGGnet()
from tensorflow.keras.applications.resnet import preprocess_input
from tensorflow.keras.preprocessing.image import img_to_array
from tensorflow.keras.applications import imagenet_utils
from imutils.object_detection import non_max_suppression
from pyimagesearch.detection_helpers import sliding_window
from pyimagesearch.detection_helpers import image_pyramid
import numpy as np
import argparse
import imutils
import time

from google.colab.patches import cv2_imshow


#to avoid ...ModuleNotFoundError: No module named 'tensorflow.python.keras.engine.base_layer_v1'
from tensorflow import keras

"""This script begins with a selection of imports including:

* **ResNet50:** The popular ResNet Convolutional Neural Network (CNN) classifier by He et al. introduced in their 2015 paper, [Deep Residual Learning for Image Recognition.](https://arxiv.org/abs/1512.03385) **We will load this CNN with pre-trained [ImageNet](http://www.image-net.org/) weights.**
***non_max_suppression**: An implementation of NMS in my [imutils package](https://github.com/jrosebr1/imutils).
***sliding_window:** Our sliding window generator function as described in the previous section.
***image_pyramid:** The image pyramid generator that we defined previously.

Now that our imports are taken care of, let’s parse command line arguments:

```
# construct the argument parse and parse the arguments
ap = argparse.ArgumentParser()
ap.add_argument("-i", "--image", required=True,
	help="path to the input image")
ap.add_argument("-s", "--size", type=str, default="(200, 150)",
	help="ROI size (in pixels)")
ap.add_argument("-c", "--min-conf", type=float, default=0.9,
	help="minimum probability to filter weak detections")
ap.add_argument("-v", "--visualize", type=int, default=-1,
	help="whether or not to show extra visualizations for debugging")
args = vars(ap.parse_args())
```
"""

import os
import sys
import argparse

'''
ap = argparse.ArgumentParser()
ap.add_argument("-i", "--image", required=True,	help="path to the input image")
ap.add_argument("-s", "--size", type=str, default="(200, 150)",	help="ROI size (in pixels)")
ap.add_argument("-c", "--min-conf", type=float, default=0.9,	help="minimum probability to filter weak detections")
ap.add_argument("-v", "--visualize", type=int, default=-1,	help="whether or not to show extra visualizations for debugging")
args = vars(ap.parse_args())
'''

"""The following arguments must be supplied to this Python script at runtime from your terminal:

* --image: The path to the input image for classification-based object detection.
*--size: A tuple describing the size of the sliding window. This tuple must be surrounded by quotes for our argument parser to grab it directly from the command line.
*--min-conf: The minimum probability threshold to filter weak detections.
*--visualize: A switch to determine whether to show additional visualizations for debugging.

We now have a handful of constants to define for our object detection procedures:

"""

HEIGHT=224
WIDTH = 224
PYR_SCALE = 1.2
#PYR_SCALE = 1.5
#In practice, It is recommended trying values of 4 and 8 to start with (depending on the dimensions of your input and your minSize).
WIN_STEP = 4

INPUT_SIZE = (HEIGHT, WIDTH)

#ROI_SIZE = eval(args["size"])
ROI_SIZE = INPUT_SIZE #for now...

"""Our classifier-based object detection methodology constants include:

* WIDTH: Given that the selection of images/ for testing (refer to the “Project Structure” section) are all slightly different in size, we set a constant width here for later resizing purposes. By ensuring our images have a consistent starting width, we know that the image will fit on our screen.
*PYR_SCALE: Our image pyramid scale factor. This value controls how much the image is resized at each layer. Smaller scale values yield more layers in the pyramid, and larger scales yield fewer layers. The fewer layers you have, the faster the overall object detection system will operate, potentially at the expense of accuracy.
*WIN_STEP: Our sliding window step size, which indicates how many pixels we are going to “skip” in both the (x, y) directions. Remember, the smaller your step size is, the more windows you’ll need to examine, which leads to a slower overall object detection execution time. In practice, I would recommend trying values of 4 and 8 to start with (depending on the dimensions of your input and your minSize).
*ROI_SIZE: Controls the aspect ratio of the objects we want to detect; if a mistake is made setting the aspect ratio, it will be nearly impossible to detect objects. Additionally, this value is related to the image pyramid minSize value — giving our image pyramid generator a means of exiting. As you can see, this value comes directly from our --size command line argument.
*INPUT_SIZE: The classification CNN dimensions. Note that the tuple defined here on Line 32 heavily depends on the CNN you are using (in our case, it is ResNet50). If this notion doesn’t resonate with you, I suggest you read this tutorial and, more specifically the section entitled “Can I make the input dimensions [of a CNN] anything I want?”

Understanding what each of the above constants controls is crucial to your understanding of how to turn an image classifier into an object detector with Keras, TensorFlow, and OpenCV. Be sure to mentally distinguish each of these before moving on.

Let’s load our ResNet classification CNN and input image:

"""

pip show tensorflow

from google.colab import files
#files.upload()
#from C:\Users\JayMnM\LOCAL_GgleDrv\DataSc\BigDataTech_UoB\colabDisserto_OpenCV
#mCHAN2021JanLIONS_VID20210120WA0003_1.mp4

ls

ls originalPics/2002/07/20/big

import pathlib
#os.path.abspath('img_109.jpg')
#os.path.realpath('img_109.jpg')
#os.path.relpath('img_109.jpg')
#UseLess4now...

imgDIR='./originalPics/2002/07/20/big'
imgDIR

#os.listdir(imgDIR)

imgDIR_list=os.listdir(imgDIR)
#imgDIR_list
np.array(imgDIR_list)

imgDIR_path_list=[os.path.join(imgDIR,img) for img in imgDIR_list]
imgDIR_path_list
#np.array(imgDir_Path_list)

np.random.choice(imgDIR_path_list)
imgBingo_path=np.random.choice(imgDIR_path_list)
imgBingo_path

imgBingo_path.dtype #OUTPUT: dtype('<U41') [2025Apr5th REVIEW] path weird data type considered as 'NoneType'

imgBingo_path

imgBingo = cv2.imread(imgBingo_path)
imgBingo

imgBingo.shape

cv2_imshow(imgBingo, )

imgBingo.shape, imgBingo.shape[1]

cv2_imshow(imgBingo)

scale=1.5
w = int(imgBingo.shape[1] / scale)
image = imutils.resize(imgBingo, width=w)
cv2_imshow(image)

"""Our image_pyramid function accepts three parameters as well:

* **image:** The input image for which we wish to generate multi-scale representations.
***scale:** Our scale factor controls how much the image is resized at each layer. Smaller scale values yield more layers in the pyramid, and larger scale values yield fewer layers.
***minSize:** Controls the minimum size of an output image (layer of our pyramid). This is important because we could effectively construct progressively smaller scaled representations of our input image infinitely. Without a minSize parameter, our while loop would continue forever (which is not what we want).
"""

image_pyramid(imgBingo)

#image_pyramid(imgBingo).__next__()
image_pyramid(imgBingo)
#Reminder `def image_pyramid(image, scale=1.5, minSize=(224, 224)):`

len(list(image_pyramid(imgBingo)))

for img in image_pyramid(imgBingo):
  #cv2_imshow(imgBingo)
  cv2_imshow(img)

for orig in image_pyramid(imgBingo):

  orig = imutils.resize(orig, width=WIDTH)
  (H, W) = orig.shape[:2]
  print((H, W))
  print(orig.shape)
  cv2_imshow(orig)

"""###LOAD imported ```JaybaselineCNN36_224VGG``` Keras MODEL
```def Jay_baseline_CNN42_448VGGnet():
model =keras.Sequential(name='JaybaselineCNN_W32cL5_224VGG') # Our model  is initialized using the Sequential  API
  [...]
  * Total layers: 21
  * Weigth: len(Jay_baseline_CNN_W34cL5_224VGGnet().weights)...32
  *Total params: 7,606,487 (29.02 MB)
  *Trainable params: 7,605,943 (29.01 MB)
  *Non-trainable params: 544 (2.12 KB)
```
```
from keras.models import load_model
from keras import backend as K

l_kJmodel = load_model('./model/k_Jmodel.h5')
print(l_kJmodel.outputs)
```
"""

pwd

from google.colab import drive, files
drive.mount('/content/drive')

ls

os.path.exists('/content/drive/MyDrive/model.zip')

os.path.exists('/content/CNN2objDetect/model.zip')

if not os.path.exists('/content/CNN2objDetect/model.zip'):
  !cp '/content/drive/MyDrive/model.zip' 'model.zip'

ls

os.path.isdir('/content/CNN2objDetect/model')

#load model.zip  in CNN2objDetect directory and unzip
if not os.path.isdir('/content/CNN2objDetect/model'):
  !unzip model.zip

ls

ls model

from keras.models import load_model
from keras import backend as K

# load our network weights from file k_Jmodel.h5
print("[INFO] loading network...")

Res50_model = ResNet50(weights="imagenet", include_top=True)

if os.path.isdir('/content/CNN2objDetect/model'):
  DJSc_model = load_model('./model/k_Jmodel.h5')
  print(DJSc_model.outputs)
else:
  DJSc_model = load_model('./k_Jmodel.h5')
  print(DJSc_model.outputs)

"""### `DJSc_model` Model EXPLORATION"""

DJSc_model.outputs

DJSc_model.output_shape

DJSc_model.count_params()

#DJSc_model.name_scope #AttributeError: 'Sequential' object has no attribute 'name_scope'

DJSc_model.summary()

pwd

os.path.isdir('/content/CNN2objDetect')

if not os.path.isdir('/content/CNN2objDetect'):
  cd /content/CNN2objDetect

pwd

ls

#!python detect_with_classifier.py --image ./originalPics/2002/07/20/big/img_720.jpg --size "(300, 150)"

import matplotlib.pyplot as plt
from google.colab.patches import cv2_imshow
from tqdm import tqdm

# load our network weights from disk
print("[INFO] loading network...")

#Model SUBSTITUTION ...
#model = ResNet50(weights="imagenet", include_top=True)
#model= DJSc_model

print( 'Used model is ...', DJSc_model.name )

# load the input image from disk, resize it such that it has the supplied width,
# and then grab its dimensions
#orig = cv2.imread(args["image"])
imgBingo_path=np.random.choice(imgDIR_path_list)
imgBingo = cv2.imread(imgBingo_path)

print('imgBingo shape is ...', imgBingo.shape)
cv2_imshow(imgBingo)
#orig = imutils.resize(imgBingo, width=WIDTH)
jayHEIGHT=300
jayWIDTH=300
jay_reSIZE=(jayHEIGHT, jayWIDTH)
print('imgBingo is to resize to Width...{} & Height...{}'.format(jayHEIGHT, jayWIDTH) )
print('imgBingo is to resize to ...{}'.format(jay_reSIZE) )
#imgBingo_rsz = imutils.resize(imgBingo, height=jayHEIGHT, width=jayWIDTH) #DJSc Not happy with the reult!!...Review...def resize `if` not exhaustive
#imgBingo_rsz = imutils.resize(imgBingo)
imgBingo_rsz = imutils.resize(imgBingo, height=jayHEIGHT) #DJSc Not happy with the reult!!...Review...def resize `if` not exhaustive
print('imgBingo resized shape is ...{}'.format(imgBingo_rsz.shape) )
#(H, W) = orig.shape[:2]
(jH, jW) = imgBingo_rsz.shape[:2]
print('imgBingo is NOW resized to imgBingo_rsz to (Height ,Width)...', (jH, jW) )
cv2_imshow(imgBingo_rsz)

"""blCk59_Line3 loads ResNet pre-trained on ImageNet.
**If you choose to use a different pre-trained classifier, you can substitute one here for your particular project.**

To learn how to train your own classifier, I suggest you read [Deep Learning for Computer Vision with Python.](https://www.pyimagesearch.com/deep-learning-computer-vision-python-book/)

We also load our input --image. Once it is loaded, we resize it (while maintaining aspect ratio according to our constant WIDTH) and grab resulting image dimensions.

From here, we’re ready to initialize our ***image pyramid generator object:**

"""

imgBingo_path.dtype #OUTPUT: dtype('<U41') [2025Apr5th REVIEW] path weird data type considered as 'NoneType'
#imgBingo_path=str(imgBingo_path) #(2025Apr5th REVIEW) cast to string
#imgBingo_path

"""REMINDER
```
def image_pyramid(image, scale=1.5, minSize=(224, 224)):
	# yield the original image
	return image
	# keep looping over the image pyramid
	while True:
		# compute the dimensions of the next image in the pyramid
		w = int(image.shape[1] / scale)
		image = imutils.resize(image, width=w)
		# if the resized image does not meet the supplied minimum
		# size, then stop constructing the pyramid
		if image.shape[0] < minSize[1] or image.shape[1] < minSize[0]:
			break
		# yield the next image in the pyramid
		return image


```
"""

PYR_SCALE

ROI_SIZE

imgBingo.shape

# initialize the image pyramid
#pyramid = image_pyramid(imgBingo_rs, scale=PYR_SCALE, minSize=ROI_SIZE) #

#pyramid = image_pyramid(imgBingo, scale=PYR_SCALE, minSize=ROI_SIZE)
pyramid = image_pyramid(imgBingo)

# initialize two lists, one to hold the ROIs generated from the image
# pyramid and sliding window, and another list used to store the
# (x, y)-coordinates of where the ROI was in the original image
rois = []
locs = []
# time how long it takes to loop over the image pyramid layers and
# sliding window locations
start = time.time()

pyramid # strangely non-persistent objet as yielded items disappears.

pyrList=[img for img in pyramid]
pyrList # strangely non-persistent objet as yielded items disappears...to have empty list after few seconds



len(pyrList)

imgBingo.shape

for img in pyramid:
  img.shape
  cv2_imshow(img)

"""On Line 45, we supply the necessary parameters to our image_pyramid generator function. Given that pyramid is a generator object at this point, we can loop over values it produces.

Before we do just that, Lines 50 and 51 initialize two lists:

rois: Holds the regions of interest (ROIs) generated from pyramid + sliding window output
locs: Stores the (x, y)-coordinates of where the ROI was in the original image
And we also set a start timestamp so we can later determine how long our classification-based object detection method (given our parameters) took on the input image (Line 55).

https://www.pyimagesearch.com/2020/06/22/turning-any-cnn-image-classifier-into-an-object-detector-with-keras-tensorflow-and-opencv/

Let’s loop over each image our pyramid produces:

**imagePyramid Loop REFERENCE**
```
# loop over the image pyramid
for image in pyramid:
  #looping start above...
	# determine the scale factor between the *original* image
	# dimensions and the *current* layer of the pyramid
	scale = jW / float(image.shape[1])
	# for each layer of the image pyramid, loop over the sliding
	# window locations
	for (x, y, roiOrig) in sliding_window(image, WIN_STEP, ROI_SIZE):
		# scale the (x, y)-coordinates of the ROI with respect to the
		# *original* image dimensions
		x = int(x * scale)
		y = int(y * scale)
		w = int(ROI_SIZE[0] * scale)
		h = int(ROI_SIZE[1] * scale)
		# take the ROI and preprocess it so we can later classify
		# the region using Keras/TensorFlow
		roi = cv2.resize(roiOrig, INPUT_SIZE)
		roi = img_to_array(roi)
		roi = preprocess_input(roi)
		# update our list of ROIs and associated coordinates
		rois.append(roi) #L80
		locs.append((x, y, x + w, y + h))

```
"""

(jH,jW)

imgBingo.shape, imgBingo.shape[1]

Jscale=jW / float(imgBingo.shape[1])
Jscale



# loop over the image pyramid
for image in pyramid:
  #looping start above L58
	# determine the scale factor between the *original* image
	# dimensions and the *current* layer of the pyramid
	#scale = jW / float(image.shape[1])
	Jscale=jW / float(imgBingo.shape[1])
	print('Jscale is ...', Jscale)
	# for each layer of the image pyramid, loop over the sliding
	# window locations
	for (x, y, roiOrig) in sliding_window(image, WIN_STEP, ROI_SIZE):
		# scale the (x, y)-coordinates of the ROI with respect to the
		# *original* image dimensions
		x = int(x * Jscale)
		y = int(y * Jscale)
		w = int(ROI_SIZE[0] * Jscale)
		h = int(ROI_SIZE[1] * Jscale)
		# take the ROI and preprocess it so we can later classify
		# the region using Keras/TensorFlow
		roi = cv2.resize(roiOrig, INPUT_SIZE)
		roi = img_to_array(roi)
		roi = preprocess_input(roi)
		# update our list of ROIs and associated coordinates
		rois.append(roi) #L80
		locs.append((x, y, x + w, y + h)) #L81

"""Looping over the layers of our image pyramid begins on Line 58.

Our first step in the loop is to compute the scale factor between the original image dimensions (W) and current layer dimensions (image.shape[1]) of our pyramid (Line 61). We need this value to later upscale our object bounding boxes.

Now we’ll cascade into our sliding window loop from this particular layer in our image pyramid. Our sliding_window generator allows us to look side-to-side and up-and-down in our image. For each ROI that it generates, we’ll soon apply image classification.

Line 65 defines our loop over our sliding windows. Inside, we:
* Scale coordinates (Lines 68-71).
*Grab the ROI and preprocess it (Lines 75-77). Preprocessing includes resizing to the CNN’s required INPUT_SIZE, converting the image to array format, and applying Keras’ preprocessing convenience function. This includes adding a batch dimension, converting from RGB to BGR, and zero-centering color channels according to the ImageNet dataset.
*Update the list of rois and associated locs coordinates (Lines 80 and 81).

We also handle optional visualization:

REFERENCE
```
# construct the argument parse and parse the arguments
ap = argparse.ArgumentParser()

ap.add_argument("-i", "--image", required=True,
	help="path to the input image")

ap.add_argument("-s", "--size", type=str, default="(200, 150)",
	help="ROI size (in pixels)")

ap.add_argument("-c", "--min-conf", type=float, default=0.9,
	help="minimum probability to filter weak detections")

ap.add_argument("-v", "--visualize", type=int, default=-1,
	help="whether or not to show extra visualizations for debugging")
args = vars(ap.parse_args())
```

#### PREDICTION with `Resnet50`

Here, we visualize both the original image with a green box indicating where we are “looking” and the resized ROI, which is ready for classification (Lines 85-95). As you can see, we’ll only --visualize when the flag is set via the command line.

Next, we’ll (1) check our benchmark on the pyramid + sliding window process, (2) classify all of our rois in batch, and (3) decode predictions:
"""

#with Resnet50
model=ResNet50(weights="imagenet", include_top=True)

# loop over the image pyramid
# strangely non-persistent pyramid objet as yielded items disappear...to have empty list after few seconds

# initialize the image pyramid
#pyramid = image_pyramid(orig, scale=PYR_SCALE, minSize=ROI_SIZE)
imgBingo4jay_path=np.random.choice(imgDIR_path_list)
imgBingo4jay = cv2.imread(imgBingo4jay_path)
pyramid4jay = image_pyramid(imgBingo4jay)
# initialize two lists, one to hold the ROIs generated from the image
# pyramid and sliding window, and another list used to store the
# (x, y)-coordinates of where the ROI was in the original image

rois=[]
#rois = list([]) ## list Reinforcement is CRUCIAL to avoid error
locs = []
# time how long it takes to loop over the image pyramid layers and
# sliding window locations
start = time.time()

for img in pyramid4jay:
  count=1 #DJSc_count
  scale=jW / float(img.shape[1])
  for (x, y, roiOrig) in sliding_window(img, WIN_STEP, ROI_SIZE):
    if count>2:
      break
    # scale the (x, y)-coordinates of the ROI with respect to the
    #*original* image dimensions
    x = int(x * scale)
    y = int(y * scale)
    w = int(ROI_SIZE[0] * scale)
    h = int(ROI_SIZE[1] * scale)
    # take the ROI and preprocess it so we can later classify
    # the region using Keras/TensorFlow
    roi = cv2.resize(roiOrig, INPUT_SIZE)
    roi = img_to_array(roi)
    roi = preprocess_input(roi)
    # update our list of ROIs and associated coordinates
    rois.append(roi)
    #list(rois).append(roi) #DJSc_debug...reinforced rois as list at declaration
    print(f'length list rois is {len(list(rois))}') #DJSc_test
    locs.append((x, y, x + w, y + h))
    print(f'length locs is {len(locs)}') #DJSc_test

  #Handling optional visualisation
    visual=1

    # check to see if we are visualizing each of the sliding
    # windows in the image pyramid
    #if args["visualize"] > 0:
    if visual > 0:
      clone=roiOrig.copy()
      # clone the original image and then draw a bounding box
	    # surrounding the current region
	    #clone = orig.copy()
      cv2.rectangle(clone, (x, y), (x + w, y + h), (0, 255, 0), 2)
      #show the visualization and current ROI
      cv2_imshow(clone)
      cv2_imshow(roiOrig)
      count+=1
      #cv2.waitKey(0) #ERROR
      #error: OpenCV(4.11.0) /io/opencv/modules/highgui/src/window.cpp:1367: error: (-2:Unspecified error)
      #The function is not implemented. Rebuild the library with Windows, GTK+ 2.x or Cocoa support.
      #If you are on Ubuntu or Debian, install libgtk2.0-dev and pkg-config, then re-run cmake or configure script in function 'cvWaitKey'

    #Above, we visualize both the original image with a green box indicating where we are “looking” and the resized ROI, which is ready for classification (Lines 85-95).
      #As you can see, we’ll only --visualize when the flag is set via the command line.
    if len(rois)==0:
      print(f'rois array is EMPTY as length is {len(rois)}') #DJSc_test
      break

  #Next, we’ll (1) check our benchmark on the pyramid + sliding window process,
    #(2) classify all of our rois in batch, and (3) decode predictions:

  # show how long it took to loop over the image pyramid layers and
# sliding window locations
end = time.time()
print("[INFO] looping over pyramid/windows took {:.5f} seconds".format(end - start))
# convert the ROIs to a NumPy array
rois = np.array(rois, dtype="float32")
# classify each of the proposal ROIs using ResNet and then show how
# long the classifications took

print("[INFO] classifying ROIs...")
start = time.time()
preds = model.predict(rois)
end = time.time()
print("[INFO] classifying ROIs took {:.5f} seconds".format(end - start))
# decode the predictions and initialize a dictionary which maps class
# labels (keys) to any ROIs associated with that label (values)

#Finally, Line 117 below decodes the predictions, grabbing only the top prediction for each ROI.
preds = imagenet_utils.decode_predictions(preds, top=1)

#We’ll need a means to map class labels (keys) to ROI locations associated with that label (values);
#the labels dictionary (Line 118) serves that purpose.

#Let’s go ahead and populate our labels dictionary now:
labels = {}
# loop over the predictions
print(f'preds array looks like... {preds}')
for (i, p) in enumerate(preds):
  # grab the prediction information for the current ROI
  (imagenetID, label, prob) = p[0]
  print(f'p[0]_{i} looks like... {p[0]}')
	# filter out weak detections by ensuring the predicted probability
	# is greater than the minimum probability
  #if prob >= args["min_conf"]:
  if prob >= 0.4:
		# grab the bounding box associated with the prediction and
		# convert the coordinates
    box = locs[i]
		# grab the list of predictions for the label and add the
		# bounding box and probability to the list
    L = labels.get(label, [])
    L.append((box, prob))
    labels[label] = L

"""#### PREDICTION with `DJSc_model`"""

DJSc_model.predict(rois)

#DJSc_model.predict_classes(rois)
#/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning:
#`model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype("int32")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).
#OUTPUT array([6])
np.argmax(DJSc_model.predict(rois), axis=-1)[0] #replacing DJSc_model.predict_classes(rois)

#print(np.argmax(DJSc_model.predict(rois), axis=-1)[0]) #replacing DJSc_model.predict_classes(rois)

def prepaReshape(image):
    imageSize = 224
    #imageArray = cv2.imread(filePath)
    #newImageArray = cv2.resize(image, (imageSize, imageSize))
    #return newImageArray.reshape(-1, imageSize, imageSize, 3)
    return image.reshape(-1, imageSize, imageSize, 3)

classJay={0:'sea', 1:'HumanFace', 2:'street', 3:'glacier', 4:'mountain', 5:'forest' , 6:'buildings' }
classJay  #labels #COOL

#with DJSc_model

from keras import backend as K
 # initialize our list of output images
images = []
#jayAdd for evaluation metrics
testYtrack=[]
predsTrack=[]
failure=0
# randomly select a few testing items in rois

if len(rois)==0:
  print(f'rois array is EMPTY as length is {len(rois)}') #DJSc_test

#for i in tqdm(np.random.choice(np.arange(0, len(testY)), size=(imgNum,))):
i=0
for roi in rois:
  print(f'roi shape is {roi.shape}')
  #prepare the image roi...CRUCIAL...!!!
  prep_roi=prepaReshape(roi)
  # classify the
  print(f'prep_roi shape is {prep_roi.shape}')
  probs = DJSc_model.predict(prep_roi) #array of probabilities for each of the 7 classes
  prediction = probs.argmax(axis=1) # class'index of the highest probability
  #label = labelNames[prediction[0]]
  label=classJay[prediction[0]]  #
  print(f'roi_{i} label prediction is {label}')
  cv2_imshow(roi)
  i+=1

"""##**Load TRAIN and TEST datasets...then Merge** (FFHQ & INTEL)

###**Load TRAIN and TEST Datasets Flickr-Faces-HQ Dataset (FFHQ)**

#### Flickr-Faces-HQ Dataset (FFHQ) **DATA `thumbnails128x128` DOWNLOAD**
"""

from google.colab import drive
drive.mount('/content/gdrive')

pwd

#UnComment Block-code when 'thumbnails128x128.zip' folder not in gdrive
#files.upload()
#path is /content/gdrive/MyDrive/thumbnails128x128.zip
#path='/content/gdrive/MyDrive/thumbnails128x128.zip'
!cp -r '/content/gdrive/MyDrive/thumbnails128x128.zip' 'thumbnails128x128.zip'
!unzip -q thumbnails128x128.zip

ls

"""###**METADATA Leverage & Objects `obj` CREATION** [Flickr-Faces-HQ Dataset (FFHQ)](https://github.com/NVlabs/ffhq-dataset)
https://github.com/NVlabs/ffhq-dataset


"""

!wget https://raw.githubusercontent.com/NVlabs/ffhq-dataset/master/download_ffhq.py

ls

!python download_ffhq.py -h

"""####**CONTINGENCY** Download **PLAN** of missing json file ```./ffhq-dataset-v2.json``` for FFHQ_object generation.

**!!!...Add download link from my ONE_Drive...!!!** to replace gglDriveMounting!!

**```ffhq-dataset-v2.json```** for object generation (metadata)
"""

import json

# read file ##crucial json meta data file
#with open('./gdrive/MyDrive/ffhq-dataset-v2.json', 'r') as myfile:
#above code Not needed if <!cp '/content/gdrive/My Drive/ffhq-dataset-v2.json' 'ffhq-dataset-v2.json'> executed...
with open('/content/gdrive/MyDrive/ffhq-dataset-v2.json', 'r') as myfile:
    data=myfile.read()

# parse file
obj = json.loads(data)

# show values
print("Image index: " + str(obj['0']))
#print("Info about the aligned 1024x1024 image: " + str(obj["image"]))
print("Info about the 128x128 thumbnail: " + str(obj['0']['thumbnail']))

obj['1']['metadata']

obj['0']['thumbnail']['file_path']

obj['0']['thumbnail']['file_path']

"""###Pics DISPLAY Check"""

#DJSc_check
from google.colab.patches import cv2_imshow
img650=obj['650']['image']['face_landmarks']
#cv2_imshow(img650) # AttributeError: 'list' object has no attribute 'clip'
import numpy as np
img650_uint=np.array(img650).astype('uint8') # rounding arrays to int
img650_uint
cv2_imshow(img650_uint)

len(obj)

import random
#randomR=random.choice(range(0,len(obj),11))

i=0
imgNum=20
#imgNum=int(call__imgNumJayForm())

plt.figure(figsize=(20,26))
#for i in range(len(obj)):

for enum in tqdm(np.random.choice(np.arange(0, len(obj)), size=(imgNum,))):
      #retrieve full_path
  print('enum is {enum}'.format(enum=enum))
  thumb_filePath= obj[str(enum)]['thumbnail']["file_path"]
  #image_filePath= obj[str(enum)]['image']["file_path"]
      #read image via full_path

  thumb_ext=thumb_filePath.split('/')[-1] #2025Apr4th_REVISION) as metadata from github, but image from flickr
  img_path=f'./thumbnails128x128/{thumb_ext}'
  print(img_path)

  #thumb_ext_num=thumb_ext.split('.')[0] #retrieve extension number to check if in Jenum_list_strZfill# Not needed as no comparison required

  #2025Apr4th_REVISION) above block code as metadata from github ffhq-dataset-v2.json, but image from flickr / or kaggle downloaded
      #read image via full_path
      #below snippet CRUCIAL...when Not All files have been downloaded.

      #if os.path.isfile(thumb_filePath): #2025Apr4th_REVISION)

  if os.path.isfile(img_path):
    imgMatrix = cv2.imread(img_path)

      #below snippet CRUCIAL...when Not All files have been downloaded.

    plt.subplot(imgNum//5, 5, i+1 )

    #plt.figure(figsize=(12,14))
    imgMatrix= cv2.imread(img_path)
    img=cv2.cvtColor(imgMatrix,cv2.COLOR_BGR2RGB)
    cv2.putText(img,f'enum is {enum}',(5,20), cv2.FONT_HERSHEY_COMPLEX_SMALL, 0.50,color=(0,255,0),thickness=1)
    #cv2_imshow(img)
    #plt.imshow(imageMatrix)
    #cv2_imshow(thumb)

    #plt.subplot(imgNum//5, 5, i + 1)
    plt.imshow(img)
    plt.colorbar()
    plt.grid(False)

    #plt.imshow(imageMatrix)
    #cv2_imshow(thumb)



    i=i+1

"""`FilePath` and `picIndex` RETRIEVAL"""

#`FilePath` and `picIndex` RETRIEVAL
filePath_list=[]
picIndex_list=[]
for elm in tqdm(range(len(obj))):
      #retrieve full_path
  jt_filePath= obj[str(elm)]['thumbnail']["file_path"]
      #read image via full_path

  thumb_ext=jt_filePath.split('/')[-1] #2025Apr4th_REVISION) as metadata from github, but image from flickr
  img_Path=f'./thumbnails128x128/{thumb_ext}'

      #below snippet CRUCIAL...when Not All files have been downloaded.
  if os.path.isfile(img_Path):
    filePath_list.append(img_Path)
    picIndex_list.append(elm)

print('\n Pics Download Expected Amount (PdEa) is: {} \n'.format(len(obj)))
#len(filePath_list)
print('\n Pics download RECEIVED amount (PdRa) is: {} \n'.format(len(filePath_list)))
print('{}% Successfull Fetch'.format(len(filePath_list)/len(obj)*100))

len(picIndex_list),len(filePath_list)

"""###**LOAD Train and SPLIT to Test dataset**(FFHQ)

pSplit

```
#partition
def pSplit_partition():
  pSplit=int(input('Please PROVIDE percentage of Reduction for a total of {} Training records:  '.format(len(train_X))))
  trainSel=np.random.choice(np.arange(0,len(train_X)), size=len(train_X)*pSplit//100)
  trainX, trainY=train_X[trainSel], train_y[trainSel]
  return trainX, trainY
```

to_catego
```
from keras.utils import to_categorical
def to_catego(train_y,test_y):
	# one hot encode target values

	trainY = to_categorical(train_y)
	testY = to_categorical(test_y)
	return trainY, testY
```


"""



"""### self-made METADATA

###**Load TRAIN and TEST datasets INTEL** for self-made METADATA ...
[from kaggle.json Repo ...If...IntelClassImg_GoodOne.zip not unzipped to yield folder (dir) /IntelClassImg_GoodOne]

**!!!...Add download link of [INTEL](https://www.kaggle.com/puneet6060/intel-image-classification) dataset from my ONE_Drive...!!!** to replace gglDriveMounting!!

####GoodOne
IntelClassImg_GoodOne.zip link
https://1drv.ms/u/c/1ceb8815f3cf306e/EW4wz_MViOsggBxSyAAAAAABeo3as1DrEndgt-WQJ0KJJw?e=gxYorS
"""

cp '/content/gdrive/My Drive/IntelClassImg_GoodOne.zip' 'IntelClassImg_GoodOne.zip'

!unzip -q 'IntelClassImg_GoodOne.zip' #to comment once folder is available in 1Drive

ls

ls ./IntelClassImg_GoodOne

ls ./IntelClassImg_GoodOne/NonFace_seg_train/seg_train

"""###**Training /Test Data PREPARATION & MERGENCE from ...** both Datasets **FFHQ** (Positive Samples) & **INTEL**_classImages (Negative Samples)

**Additional LIBRARIES Import**
"""

#necessary libraries import
import sys
import tensorflow as tf
from tensorflow import keras

# Pandas and numpy for data manipulation
import pandas as pd
import numpy as np
np.random.seed(42)

# Matplotlib and seaborn for plotting
import plotly.express as px
import matplotlib.pyplot as plt
#%matplotlib inline
import seaborn as sns

# Scipy helper functions
from scipy.stats import percentileofscore
from scipy import stats

#cat test_dataJay_INTELobj_retrieval.py

DIR_train= './IntelClassImg_GoodOne/NonFace_seg_train/seg_train' #labelsDirectories
#DIR_train= './seg_train/seg_train'

DIR_test= './IntelClassImg_GoodOne/NonFace_seg_test/seg_test' #labelsDirectories
#DIR_test= './seg_test/seg_test'

def call_INTELcategCard_JayForm(): #@title jayForm
  #@markdown Please INPUT the amount of images for each INTELCategory:
  INTELcategCard=50 #@param{type:'integer'}
  #print(imgNum)
  return INTELcategCard

#FFHQusedTest_Pics_set FFHQ pics path

"""`def pSplit__FFHQpartition():`"""

len(picIndex_list)

#len(picIndex_list[:5000])

# FFHQ SPLIT Partition (Train|Test)
def pSplit__FFHQpartition():

  pSplit=float(input('Please PROVIDE a percentage of TRAINING for a total of {} Training-Test records:  '.format(len(filePath_list))))
  train_picIndex_choice=np.random.choice(picIndex_list, size=int((len(filePath_list)*pSplit)//100))
  len(train_picIndex_choice)

  # picsList=[obj[str(num)] for num in picIndex_list]
  # picsList
  # len(picsList)

  train_picIndex_set=set(train_picIndex_choice)

  #CRUCIAL...image path definition as Thumbnails OR Image ...CRUCIAL !!!
  trainPics_set=['./thumbnails128x128/{}'.format(obj[str(enum)]['thumbnail']["file_path"].split('/')[-1]) for enum in train_picIndex_set]
  #len(trainPics_set), trainPics_set[-1], trainPics_set[22089]  #DJSc_testing

  trainPics_set
  print('FFHQTraining Pics Count as {}% Representative...{}'.format(pSplit,len(train_picIndex_choice)))
  print('FFHQTraining Pics Count as Set...{} ... as {}% true representation'.format(len(trainPics_set),len(trainPics_set)*100//len(picIndex_list) ))
  print(len(train_picIndex_choice)-len(trainPics_set), 'representes the amount of duplicated pictures\
  in our random choice...Hence offsetting the value of our Split percentage Repesentative {} ... :-) Never Mind!!'.format(pSplit) )

  #testPics=[obj[str(num)]['image']["file_path"] for num in picIndex_list if num not in train_picIndex_set] #testPics NO Need to be cast as Set!!!
  #print('FFHQTest Pics Count...{}'.format(len(testPics)))
  testPicIndex_list=[num for num in picIndex_list if num not in train_picIndex_set]
  testPics=['./thumbnails128x128/{}'.format(obj[str(enum)]['thumbnail']["file_path"].split('/')[-1]) for enum in testPicIndex_list]
  print('FFHQTest Pics Count...{}'.format(len(testPics)))


  #Test data PARTITIONing@partiSpliTest into used & reserved(res) portions of TestData.
  partiSpliTest=0.01 #TEST Split
  usedTest_picIndex_choice=np.random.choice(testPicIndex_list, size=int(len(testPics)*partiSpliTest))
  len(usedTest_picIndex_choice)

  usedTest_picIndex_set=set(usedTest_picIndex_choice)

  #CRUCIAL...image path definition as Thumbnails OR Image ...CRUCIAL !!!
  usedTest_Pics_set=['./thumbnails128x128/{}'.format(obj[str(enum)]['thumbnail']["file_path"].split('/')[-1]) for enum in usedTest_picIndex_set]
  #trainPics_set=[obj[str(num)]['image']["file_path"] for num in train_picIndex_set]
  usedTest_Pics_set

  print('FFHQusedTest Pics Count as {}% Representative...{}'.format(partiSpliTest*100,len(usedTest_picIndex_choice)))
  print('FFHQusedTest Pics Count as Set...{} ... as {:.2f}% true Representation'.format(len(usedTest_Pics_set),len(usedTest_Pics_set)*100/len(testPicIndex_list) ))
  print(len(usedTest_picIndex_choice)-len(usedTest_Pics_set), 'represents the amount of duplicated pictures\
  in our random choice...Hence offsetting the value of our Partition percentage Repesentative {}% ... :-) Never Mind!!'.format(partiSpliTest*100) )

  resTest_Pics=['./thumbnails128x128/{}'.format(obj[str(enum)]['thumbnail']["file_path"].split('/')[-1]) for enum in testPicIndex_list if enum not in usedTest_picIndex_set]
  #testPics NO Need to be cast as Set!!!


  len(testPics)==len(usedTest_Pics_set) +len(resTest_Pics)
  Total=len(trainPics_set) + len(testPics)

  print('FFHQtrainPics_set Count is ...', len(trainPics_set))
  print('FFHQusedTest_Pics_set Count is ...', len(usedTest_Pics_set))
  print('FFHQresTest_Pics Count is ...', len(resTest_Pics))

  print('{} + {} + {} = {} TotalFFHQ'.format(len(trainPics_set),len(usedTest_Pics_set), len(resTest_Pics), len(trainPics_set) + len(usedTest_Pics_set) + len(resTest_Pics) ))
  print(Total== (len(trainPics_set) + len(usedTest_Pics_set) + len(resTest_Pics)))

  return trainPics_set, len(trainPics_set), testPics, len(testPics),usedTest_Pics_set, len(usedTest_Pics_set), resTest_Pics, len(resTest_Pics) #4x2 (8) returned variables

"""`train_dataJay_INTELobj_retrieval():`"""

import numpy as np

def train_dataJay_INTELobj_retrieval():
  train_dataJay=[]
  labels_list=[]
  INTELobj_train={}
  INTELcategCard=call_INTELcategCard_JayForm()
  train_INTELpath=[]  #train_INTELpath insertion 2025May2nd_review

  subCount=0
  for Sub in os.listdir(DIR_train):
    itemCount=0
    Sub_labels_list=[]
    #print(Sub)
    SubPath=os.path.join(DIR_train,Sub)
    print(f'Train_label <{Sub}> path is:  {SubPath}')
    Sub_DIR_train=os.listdir(SubPath)
    for item in Sub_DIR_train:
      if itemCount> INTELcategCard:         # max<700> per category...reduced amount to avoid CRASH!!!
        break
      #REMINDER: INTELcategCard=call_INTELcategCard_JayForm()

      #print(item)
      #print(item_SubPath)
      elif item.endswith('.jpg'):
        #print(item)
        item_fullPath_train=os.path.join(SubPath,item)# or os.path.join(DIR_train,Sub,item)
        #print(item_fullPath_train)
        train_INTELpath.append(item_fullPath_train) #train_INTELpath insertion 2025May2nd_review....EUREKA!!!
        imageMatrix=cv2.imread(item_fullPath_train)

        imageMatrix_rsz=cv2.resize(imageMatrix, (224,224))
        imageMatrix_rsz_arr=img_to_array(imageMatrix_rsz)
        train_dataJay.append(imageMatrix_rsz_arr) #here... 2be Def RETURN (1)

      # extract the class label from the image path and update the labels list
      label=str(Sub+'_'+item)

      Sub_labels_list.append(label) #....to generate <INTELobj_train[Sub]> subList...???
      labels_list.append(label)

      itemCount+=1


    INTELobj_train[Sub]=Sub_labels_list #here... 2be Def RETURN (2)

    #print(INTELobj_train[Sub])

    #print(INTELobj_train)

    subCount+=1
    #if subcount>1: # reduced count required to avoid ... CRASH!!!!...if needed
    #break

  np_labels_list=np.array(labels_list) #here... 2be in Def RETURN
  #np.array(train_dataJay) ... cause CRASH!!!
  #print(labels_list)
  #print(INTELobj_train)

  return train_dataJay, np_labels_list, INTELobj_train, train_INTELpath
  #train_INTELpath insertion 2025May2nd_review

"""`def test_dataJay_INTELobj_retrieval():`"""

#import numpy as np

def test_dataJay_INTELobj_retrieval():
  test_dataJay=[]
  test_labels_list=[]
  INTELobj_test={}
  INTELcategCard=call_INTELcategCard_JayForm()
  test_INTELpath=[]  #test_INTELpath insertion 2025Apr19_review

  subCount=0
  for Sub in os.listdir(DIR_test):
    itemCount=0
    test_Sub_labels_list=[]
    #print(Sub)
    SubPath=os.path.join(DIR_test,Sub)
    print(f'Test_label <{Sub}> path is:  {SubPath}')
    Sub_DIR_test=os.listdir(SubPath)
    for item in Sub_DIR_test:
      if itemCount> INTELcategCard:         # max<700> per category...reduced amount to avoid CRASH!!!
        break

      #print(item)
      #print(item_SubPath)
      elif item.endswith('.jpg'):
        #print(item)
        item_fullPath_test=os.path.join(SubPath,item)# or os.path.join(DIR_train,Sub,item)
        #print(item_fullPath_train)
        test_INTELpath.append(item_fullPath_test) #test_INTELpath insertion 2025Apr19_review
        imageMatrix=cv2.imread(item_fullPath_test)

        imageMatrix_rsz=cv2.resize(imageMatrix, (224,224))
        imageMatrix_rsz_arr=img_to_array(imageMatrix_rsz)
        test_dataJay.append(imageMatrix_rsz_arr) #here... 2be Def RETURN (1)

      # extract the class label from the image path and update the labels list
      label=str(Sub+'_'+item)
      test_Sub_labels_list.append(label)

      test_labels_list.append(label)

      itemCount+=1


    INTELobj_test[Sub]=test_Sub_labels_list #here... 2be Def RETURN (2)

    #print(INTELobj_train[Sub])

    #print(INTELobj_train)

    subCount+=1
    #if subcount>1: # reduced count required to avoid ... CRASH!!!!...if needed
    #break

  np_test_labels_list=np.array(test_labels_list) #here... 2be in Def RETURN
  #np.array(train_dataJay) ... cause CRASH!!!
  #print(labels_list)
  #print(INTELobj_train)

  return test_dataJay, np_test_labels_list, INTELobj_test, test_INTELpath
  #test_INTELpath insertion 2025Apr19_review

pwd

"""`train_dataJay_INTELobj_retrieval()`"""

train_dataJay, np_labels_list, INTELobj_train, train_INTELpath=train_dataJay_INTELobj_retrieval()
#train_dataJay=train_dataJay_INTELobj_retrieval()[0]

"""`test_dataJay_INTELobj_retrieval()`"""

test_dataJay, np_test_labels_list, INTELobj_test, test_INTELpath=test_dataJay_INTELobj_retrieval() #test_INTELpath insertion 2025Apr19_review
#return test_dataJay, np_test_labels_list, INTELobj_test

#trainPics_set, trainPics_setLength, testPics, testPics_Length,usedTest_Pics_set, usedTest_Pics_setLength, resTest_Pics, resTest_PicsLength=pSplit__FFHQpartition()

# now building a frame of 3072 columns
import pandas as pd

FFHQtrainPics_set, len_FFHQtrainPics_set, FFHQtestPics, len_FFHQtestPics, FFHQusedTest_Pics_set, len_FFHQusedTest_Pics_set, FFHQresTestPics, len_FFHQresTestPics =pSplit__FFHQpartition()

FFHQtrain_Xdf=pd.DataFrame(FFHQtrainPics_set)
FFHQusedTest_Xdf=pd.DataFrame(FFHQusedTest_Pics_set)  #usedTest Data ADDITION

os.listdir(DIR_train)

DIR_train

#os.listdir('/'.join([DIR_train, 'buildings']))

#FFHQusedTest_Xdf

#FFHQtrain_Xdf.info()



"""####INTEL & FFHQ **Readjustment and Mergence**

**Readjustment**
"""



FFHQtrain_Xdf['jayIndex']=FFHQtrain_Xdf.index
FFHQusedTest_Xdf['test_jayIndex']=FFHQusedTest_Xdf.index

"""**Mergence**

`MERGENCE` of INTEL & FFHQ dataframes (Test)
"""

#pd.concat([abstract_CIFARtrain_Xdf,abstract_FFHQtrain_Xdf],axis=1) # all indexes included
#mergedTEST_INTEL_FFHQ_df=pd.concat([INTELtest_Xdf,abstract_FFHQusedTest_Xdf],axis=0) # all indexes included
#mergedTEST_INTEL_FFHQ_df

"""**Data CONCAT & Preprocessing with Pandas** SUMMARY"""

#FFHQtrainPics_set #path index

"""`def pandasINTELdataFFHQconcat_Preprocessing():`"""

## **Data CONCAT & Preprocessing with Pandas** SUMMARY
import pandas

def pandasINTELdataFFHQconcat_Preprocessing():


  #FFHQ / INTEL Load&Pandas

  FFHQtrainPics_set, len_FFHQtrainPics_set, FFHQtestPics, len_FFHQtestPics, FFHQusedTest_Pics_set, len_FFHQusedTest_Pics_set, FFHQresTestPics, len_FFHQresTestPics =pSplit__FFHQpartition()

  # ...TRAIN & TEST...#

  #FFHQ (HumFaces)

  print('len_FFHQtrainPics_set is ... ', len_FFHQtrainPics_set)
  print('len_FFHQusedTest_Pics_set is ...', len_FFHQusedTest_Pics_set)

  #FFHQtrain_Xdf=pd.DataFrame(FFHQtrainPics_set)
  FFHQtrain_Xdf=pd.DataFrame()
  FFHQtrain_Xdf['trainPath']=FFHQtrainPics_set #index of pics path ...2025May2nd_review
  #FFHQtrain_Xdf
  #FFHQtrain_Xdf.head

  FFHQusedTest_Xdf=pd.DataFrame()
  FFHQusedTest_Xdf['testPath']=FFHQusedTest_Pics_set #index of pics path 2025Apr19_review
  #FFHQusedTest_Xdf
  #FFHQusedTest_Xdf.head

  FFHQtr_classeID=['FFHQtr_{}'.format(i) for i in FFHQtrain_Xdf.index]
  #CIFtr_classeID , FFHQtr_classeID
  FFHQusedTest_classeID=['FFHQusedTest_{}'.format(i) for i in FFHQusedTest_Xdf.index]


#FFHQtrain_Xdf.index
  FFHQtrain_Xdf['jayIndex']=FFHQtrain_Xdf.index
  FFHQusedTest_Xdf['test_jayIndex']=FFHQusedTest_Xdf.index


  FFHQtrain_Xdf['ID']=FFHQtrain_Xdf['jayIndex'].apply(lambda x:FFHQtr_classeID[x])
  #FFHQtrain_Xdf.head
  FFHQtrain_Xdf['SubCateg']='HumanFace'
  #FFHQtrain_Xdf

  FFHQusedTest_Xdf['test_ID']=FFHQusedTest_Xdf['test_jayIndex'].apply(lambda x:FFHQusedTest_classeID[x])
  #FFHQusedTest_Xdf.head
  FFHQusedTest_Xdf['test_SubCateg']='HumanFace'
  #FFHQusedTest_Xdf




  abstract_FFHQtrain_Xdf=FFHQtrain_Xdf[["ID","jayIndex","SubCateg"]]
  #abstract_FFHQtrain_Xdf

  #abstract_FFHQusedTest_Xdf=FFHQusedTest_Xdf[["test_ID","test_jayIndex","test_SubCateg"]]
  abstract_FFHQusedTest_Xdf=FFHQusedTest_Xdf[["test_ID","test_jayIndex","test_SubCateg",'testPath']]
  abstract_FFHQusedTest_Xdf


  #np.array(train_dataJay).shape, len(train_dataJay)
  INTELobj_train.keys()
  #np.array(labels_list)
  INTELobj_test.keys()

#INTEL  (Non Faces) sceneries
  INTELtrain_Xdf=pandas.DataFrame()
  INTELtrain_Xdf['ID']=np_labels_list
  INTELtrain_Xdf['jayIndex']=INTELtrain_Xdf.index
  INTELtrain_Xdf['SubCateg']= INTELtrain_Xdf['ID'].apply(lambda x: x.split('_')[0])
  INTELtrain_Xdf['trainPath']=train_INTELpath #test_INTELpath (list of item_full_path) insertion 2025May2nd_review
  #INTELtrain_Xdf

  #INTELtest_Xdf=pandas.DataFrame()
  INTELtest_Xdf=pandas.DataFrame()
  INTELtest_Xdf['test_ID']=np_test_labels_list
  INTELtest_Xdf['test_jayIndex']=INTELtest_Xdf.index
  INTELtest_Xdf['test_SubCateg']= INTELtest_Xdf['test_ID'].apply(lambda x: x.split('_')[0])
  INTELtest_Xdf['testPath']=test_INTELpath #test_INTELpath (list of item_full_path) insertion 2025Apr19_review

  INTELtest_Xdf
  len(INTELtrain_Xdf),len(abstract_FFHQtrain_Xdf), len(INTELtrain_Xdf) + len(abstract_FFHQtrain_Xdf)



  len(INTELtrain_Xdf),len(abstract_FFHQtrain_Xdf), len(INTELtrain_Xdf) + len(abstract_FFHQtrain_Xdf)
  len(INTELtest_Xdf),len(abstract_FFHQusedTest_Xdf), len(INTELtest_Xdf) + len(abstract_FFHQusedTest_Xdf)


      # pd.concat([abstract_CIFARtrain_Xdf,abstract_FFHQtrain_Xdf],axis=1) # all indexes included
  mergedTRAIN_INTEL_FFHQ_df=pd.concat([INTELtrain_Xdf,abstract_FFHQtrain_Xdf],axis=0) # all indexes included
  mergedTRAIN_INTEL_FFHQ_df

  mergedTEST_INTEL_FFHQ_df=pd.concat([INTELtest_Xdf,abstract_FFHQusedTest_Xdf],axis=0) # all indexes included
  mergedTEST_INTEL_FFHQ_df


  print('\n FFHQtrain_Xdf: \n {}'.format(FFHQtrain_Xdf))
  print('\n FFHQusedTest_Xdf: \n {}'.format(FFHQusedTest_Xdf))


  #Readjustment & Mergence (INTEL & FFHQ)

  INTELtrain_Xdf=INTELtrain_Xdf[["ID","jayIndex","SubCateg", 'trainPath']]
  #print('\n abstract_INTELtrain_Xdf is : \n {}'.format(INTELtrain_Xdf))

  #INTELtest_Xdf=INTELtest_Xdf[["test_ID","test_jayIndex","test_SubCateg"]]
  print('\n abstract_INTELtest_Xdf is : \n {}'.format(INTELtest_Xdf))


  abstract_FFHQtrain_Xdf=FFHQtrain_Xdf[["ID","jayIndex","SubCateg", 'trainPath']]
  #print('\n abstract_FFHQtrain_Xdf is: \n {}'.format(abstract_FFHQtrain_Xdf))

  abstract_FFHQusedTest_Xdf=FFHQusedTest_Xdf[["test_ID","test_jayIndex","test_SubCateg", 'testPath']]
  print('\n abstract_FFHQusedTest_Xdf is: \n {}'.format(abstract_FFHQusedTest_Xdf))


  len(INTELtrain_Xdf) + len(abstract_FFHQtrain_Xdf)

  len(INTELtest_Xdf) + len(abstract_FFHQusedTest_Xdf)


  mergedTRAIN_INTEL_FFHQ_df=pd.concat([INTELtrain_Xdf,abstract_FFHQtrain_Xdf],axis=0,ignore_index=True)
  # all indexes included and... ignored to avoid duplication...!!!...crucial
  #print('\n mergedTRAIN_INTEL_FFHQ_df is : \n {}'.format(mergedTRAIN_INTEL_FFHQ_df))

  mergedTEST_INTEL_FFHQ_df=pd.concat([INTELtest_Xdf,abstract_FFHQusedTest_Xdf],axis=0,ignore_index=True)
  #  # all indexes included and... ignored to avoid duplication...!!!...crucial
  print('\n mergedTEST_INTEL_FFHQ_df is : \n {}'.format(mergedTEST_INTEL_FFHQ_df))


  #Check Balances

  #TRAIN
  len(INTELtrain_Xdf) + len(abstract_FFHQtrain_Xdf) == len(mergedTRAIN_INTEL_FFHQ_df)
  print('\n Mergence CheckBalance is : {} as ...\n len(INTELtrain_Xdf) is: {} \n len(abstract_FFHQtrain_Xdf) is {} \
\n len(mergedTRAIN_INTEL_FFHQ_df) is {} '.format((len(INTELtrain_Xdf) + len(abstract_FFHQtrain_Xdf) == len(mergedTRAIN_INTEL_FFHQ_df)),len(INTELtrain_Xdf), len(abstract_FFHQtrain_Xdf),len(mergedTRAIN_INTEL_FFHQ_df)  ))

  #TEST
  len(INTELtest_Xdf) + len(abstract_FFHQusedTest_Xdf) == len(mergedTEST_INTEL_FFHQ_df)
  print('\n Mergence CheckBalance is : {} as ...\n len(INTELtest_Xdf) is: {} \n len(abstract_FFHQusedTest_Xdf) is {} \
\n len(mergedTRAIN_INTEL_FFHQ_df) is {} '.format((len(INTELtest_Xdf) + len(abstract_FFHQusedTest_Xdf) == len(mergedTEST_INTEL_FFHQ_df)),len(INTELtest_Xdf), len(abstract_FFHQusedTest_Xdf),len(mergedTEST_INTEL_FFHQ_df)  ))


  return [INTELtrain_Xdf, len(INTELtrain_Xdf) ,FFHQtrain_Xdf, abstract_FFHQtrain_Xdf, len(abstract_FFHQtrain_Xdf), mergedTRAIN_INTEL_FFHQ_df, len(mergedTRAIN_INTEL_FFHQ_df)] , [INTELtest_Xdf, len(INTELtest_Xdf) ,FFHQusedTest_Xdf, abstract_FFHQusedTest_Xdf, len(abstract_FFHQusedTest_Xdf), mergedTEST_INTEL_FFHQ_df, len(mergedTEST_INTEL_FFHQ_df)]

"""**MERGENCE Execution** ```pandasINTELdataFFHQconcat_Preprocessing()  ```"""

[INTELtrain_Xdf, len_INTELtrain_Xdf ,FFHQtrain_Xdf, abstract_FFHQtrain_Xdf, len_abstract_FFHQtrain_Xdf,  mergedTRAIN_INTEL_FFHQ_df,  len_mergedTRAIN_INTEL_FFHQ_df] , [INTELtest_Xdf, len_INTELtest_Xdf ,FFHQusedTest_Xdf, abstract_FFHQusedTest_Xdf, len_abstract_FFHQusedTest_Xdf,  mergedTEST_INTEL_FFHQ_df,  len_mergedTEST_INTEL_FFHQ_df] = pandasINTELdataFFHQconcat_Preprocessing()

"""####CHECK"""

#Check Balance (TEST)
len(INTELtest_Xdf) + len(abstract_FFHQusedTest_Xdf) == len(mergedTEST_INTEL_FFHQ_df)
print('Mergence CheckBalance is : {} as ...\n len(INTELtest_Xdf) is: {} \n len(abstract_FFHQtusedTest_Xdf) is {} \
\n len(mergedTEST_INTEL_FHQ_df) is {} '.format((len(INTELtest_Xdf) + len(abstract_FFHQusedTest_Xdf) == len(mergedTEST_INTEL_FFHQ_df)),len(INTELtest_Xdf), len(abstract_FFHQusedTest_Xdf),len(mergedTEST_INTEL_FFHQ_df)  ))

"""FFHQ"""

#abstract_FFHQtrain_Xdf

#abstract_FFHQusedTest_Xdf=FFHQusedTest_Xdf[["test_ID","test_jayIndex","test_SubCateg",'testPath']]
abstract_FFHQusedTest_Xdf

"""INTEL"""

INTELtest_Xdf.testPath[0]

INTELtest_Xdf.testPath[198]

INTELtest_Xdf.info()

INTELtest_Xdf.testPath[0]

len(INTELtest_Xdf),len(abstract_FFHQusedTest_Xdf), len(INTELtest_Xdf) + len(abstract_FFHQusedTest_Xdf)

np.array(train_dataJay).shape

abstract_FFHQtrain_Xdf

abstract_FFHQusedTest_Xdf

#imgFFHQusedTest_list=[cv2.imread(path) for path in FFHQusedTest_Xdf['testPath']]

#FFHQusedTest_Xdf
#imgFFHQusedTest_list=[cv2.imread(path) for path in FFHQusedTest_Xdf[0]]
#FFHQusedTest images resized to (448,448,3)
#imgFFHQusedTest_rsz_list=[cv2.resize(img,(224,224)) for img in imgFFHQusedTest_list]
#np.array(imgFFHQusedTest_rsz_list).shape
#FFHQusedTest_X=pd.array(FFHQusedTest_Xdf[0])

#RETURN of pandasINTELdataFFHQconcat_Preprocessing()[1] for TEST
INTELtest_Xdf
len_INTELtest_Xdf
FFHQusedTest_Xdf
abstract_FFHQusedTest_Xdf
len_abstract_FFHQusedTest_Xdf
mergedTEST_INTEL_FFHQ_df
len_mergedTEST_INTEL_FFHQ_df
#REMINDER
#train_dataJay, np_labels_list, INTELobj_train=train_dataJay_INTELobj_retrieval()
#train_dataJay=train_dataJay_INTELobj_retrieval()[0]

def usedTest_FFHQimg_Matrix():
  img_FFHQusedTest_X_list=[]
  for iter in range(len(FFHQusedTest_X)):
    img_FFHQusedTest_X=cv2.imread(FFHQusedTest_X[iter])
    img_FFHQusedTest_X_list.append(img_FFHQusedTest_X)
  #img_FFHQusedTest_X_list=[cv2.imread(FFHQusedTest_X[iter]) for iter in FFHQusedTest_X ]
  #img_FFHQusedTest_X_list
  testFFHQimg_Matrix=np.array(img_FFHQusedTest_X_list)
  return testFFHQimg_Matrix, testFFHQimg_Matrix.shape

mergedTRAIN_INTEL_FFHQ_df

mergedTEST_INTEL_FFHQ_df

"""
##**step_1:**

  a)**Recollect the Merged and Split Datasets** ( train_ALLdataJay & test_ALLdataJay)

  b) **to_catego** target variables to Category"""

classJay

np.array(classJay)

classJay.items()

rev_classJay={v:k  for k,v in classJay.items()}

rev_classJay #crucial for mapping

mergedTEST_INTEL_FFHQ_df['test_cJ']=mergedTEST_INTEL_FFHQ_df.test_SubCateg.map(rev_classJay)

mergedTRAIN_INTEL_FFHQ_df['train_cJ']=mergedTRAIN_INTEL_FFHQ_df.SubCateg.map(rev_classJay) #2025May2nd_review

mergedTEST_INTEL_FFHQ_df

testALL_y=np.array(mergedTEST_INTEL_FFHQ_df.test_cJ)

testALL_y, len(testALL_y)

"""REMINDER
```

# RECOLLECTION Test

#TEST
testALL_y=np.array(mergedTEST_INTEL_FFHQ_df.test_classCateg)
testALL_y, len(testALL_y)

test_ALLdataJay=mergedTEST_INTEL_FFHQ_df.testPath

```
"""

# RECOLLECTION Test
test_ALLdataJay=mergedTEST_INTEL_FFHQ_df.testPath
#test_ALLdataJay

len(test_ALLdataJay)

test_ALLlabels=mergedTEST_INTEL_FFHQ_df.test_SubCateg

test_ALLlabels, len(test_ALLlabels)

"""testData&Labels...Crucial REMINDER...2025Apr19th

test_ALLdataJay
```
test_ALLdataJay=mergedTEST_INTEL_FFHQ_df.testPath

```
test_ALLlabels
`test_ALLlabels=mergedTEST_INTEL_FFHQ_df.test_SubCateg`


2025May2nd_REMINDER `pandasINTELdataFFHQconcat_Preprocessing()`
```
[INTELtrain_Xdf, len_INTELtrain_Xdf ,FFHQtrain_Xdf, abstract_FFHQtrain_Xdf, len_abstract_FFHQtrain_Xdf,  mergedTRAIN_INTEL_FFHQ_df,  len_mergedTRAIN_INTEL_FFHQ_df] , [INTELtest_Xdf, len_INTELtest_Xdf ,FFHQusedTest_Xdf, abstract_FFHQusedTest_Xdf, len_abstract_FFHQusedTest_Xdf,  mergedTEST_INTEL_FFHQ_df,  len_mergedTEST_INTEL_FFHQ_df] = pandasINTELdataFFHQconcat_Preprocessing()
```
"""

len(test_ALLdataJay)==len(test_ALLlabels)

import numpy as np
from tensorflow.keras.utils import to_categorical #to_categorical is now located in keras.utils
#from keras.utils import to_categorical #DEPRECATED
def test_to_catego(testALL_y):
  testALL_y2categ=to_categorical(testALL_y,7)
  #testY=to_categorical(test_y)
  #print('train_ALLdataJay=%s, trainALL_y2categ=%s' % (np.array(train_ALLdataJay).shape, trainALL_y2categ.shape)) # summarize partition-category
  #print('test_X=%s, testY=%s' % (test_X.shape, testY.shape))
  print('test_ALLdataJay=%s' %(testALL_y2categ))

  return testALL_y2categ

testALL_y2categ=test_to_catego(testALL_y)

testALL_y2categ[0]

# TEST Data (image) Cardinality re-Check:
len(test_ALLdataJay), len(testALL_y2categ), len(test_ALLdataJay) == len(testALL_y2categ)

# MOVING Balance CHECK
print('\n abstract_FFHQtrain_Xdf Cardinal is: {} \n'.format(len_abstract_FFHQtrain_Xdf))
#len(filePath_list)
print('\n len_INTELtrain_Xdf Cardinal is: {} \n'.format(len_INTELtrain_Xdf))

print('Mergence CheckBalance is : {} as ...\n len(INTELtrain_Xdf) is: {} \n len(abstract_FFHQtrain_Xdf) is {} \
\n len(mergedTRAIN_INTEL_FHQ_df) is {} '.format((len(INTELtrain_Xdf) + len(abstract_FFHQtrain_Xdf) == len(mergedTRAIN_INTEL_FFHQ_df)),len(INTELtrain_Xdf), len(abstract_FFHQtrain_Xdf),len(mergedTRAIN_INTEL_FFHQ_df)  ))

test_ALLdataJay[4] #CHECK

mergedTEST_INTEL_FFHQ_df

"""REMINDER
```

# RECOLLECTION Test

#TEST
testALL_y=np.array(mergedTEST_INTEL_FFHQ_df.test_classCateg)
testALL_y, len(testALL_y)

test_ALLdataJay=test_dataJay + imgFFHQusedTest_rsz_list

```
"""

ALLtest_SubCateg=mergedTEST_INTEL_FFHQ_df.test_SubCateg
ALLtest_SubCateg

#TEST
#testALL_y, len(testALL_y)

len(test_ALLdataJay)

"""mapping of `test_ALLdataJay` and `testALL_y` required...when dealing with a sublist!!!

"""

DIR_test

testALL_y[20:70]

TmapTestXto_ALLy=dict(zip(test_ALLdataJay,testALL_y))
#TmapTestXto_ALLy

testALL_y[:20]

testY=to_categorical(testALL_y)
testY_revCat=np.argmax(testY,axis=1)

"""## Using DJSc Self-made METADATA

Debugging...
"""

img=cv2.imread(test_ALLdataJay[i])
img

cv2_imshow(img)

#img[0]

imageMatrix = (img[0]*255).astype("uint8")
#imageMatrix

#Debug...
for i in tqdm(np.random.choice(np.arange(0, len(test_ALLdataJay)), size=(5,))):
      # classify the object
      imgMatrix=cv2.imread(test_ALLdataJay[i])
      print(f'imgMatrix shape is {imgMatrix.shape}')

      imgMatrix_rsz=cv2.resize(imgMatrix, (224,224))
      print(f'imgMatrix_rsz shape is {imgMatrix_rsz.shape}')

      prepShape=prepaReshape(imgMatrix_rsz)
      print(f'img prepa shape is {prepShape.shape}')

      #debug...

      if (K.image_data_format() == "channels_first"):
        image = (testALLdataJay[i][0] * 255).astype("uint8")
        print('if...image shape is ...', image.shape)

       # otherwise we are using "channels_last" ordering
      else:
        image = ( prepShape * 255).astype("uint8")
        #image = (testXprep[i] * 255).astype("uint8")
        print('else...image shape is ...', image.shape)
    # initialize the text label color as green (correct)
      color = (0, 255, 0)

      image=image.reshape(224,224,3)

      image = cv2.resize(image, (96, 96), interpolation=cv2.INTER_LINEAR)
      #cv2.putText(image, labelNames, (5, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.75,color, 2)

                #require prediction[0] block code
      #cv2.putText(image, label_str, (5, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.50,color, 2)

      # add the image to our list of output images
      images.append(image)

#testALL_y

# ....!!!! STRANGE...!!!
#def view_NORMALised_prediction(modelJoem,testXprep,testALL_y):

def view_prediction(modelJoem,testX,test_y, test_SubCateg):
  from keras import backend as K
  from tqdm import tqdm #jayAdd for progress bar taqaduma('I love U so much')
  from google.colab.patches import cv2_imshow
  from imutils import build_montages
  from sklearn.metrics import accuracy_score

  #trying to view predicted names of randomly selected images from data
  imgNum=int(input('Please INPUT the amount of images for prediction (no more than 60) to randomly display:'))
  if imgNum>60:
    print('Large number exceeding 60...function closing...sorry!')

  else:
    # initialize our list of output images
    images_Matrix = []
    #jayAdd for evaluation metrics
    testYtrack=[]
    predsTrack=[]
    failure=0

    #Map testX to testALL_y
    mapTestXto_ALLy=dict(zip(testX,test_y))
    mapTestXto_ALLtestSC=dict(zip(testX,test_SubCateg))

    print(f'modelJoem name is... {modelJoem.name}')

    for i in tqdm(np.random.choice(np.arange(0, len(testX)), size=(imgNum,))):
      # classify the object

      #CRUCIAL block of code to prepare img for model prediction
      img=cv2.imread(testX[i])
      print(f'img shape is {img.shape}')

      img_rsz=cv2.resize(img, (224,224))
      print(f'img_rsz shape is {img_rsz.shape}')

      prepShape=prepaReshape(img_rsz)
      #CRUCIAL!!! return image.reshape(-1, imageSize, imageSize, 3)
      print(f'img prepa shape is {prepShape.shape}')

       #imgMatrix=img_to_array(imgMatrix)
      #probs = modelJoem.predict(testXprep[np.newaxis, i]) #predict array of probabilities for each of the 7 classes
      #probs = modelJoem.predict(testX[np.newaxis, i]) #predict array of probabilities for each of the 7 classes

      #probs = modelJoem.predict(np.array(imgMatrix_rsz))
      probs = modelJoem.predict(np.array(prepShape))

      if modelJoem.name=='resnet50':
        #Finally, Line 117 below decodes the predictions, grabbing only the top prediction for each ROI.
        prediction = imagenet_utils.decode_predictions(probs, top=1)
        #We’ll need a means to map class labels (keys) to ROI locations associated with that label (values);
        #the labels dictionary (Line 118) serves that purpose.

        #Let’s go ahead and populate our labels dictionary now:
        #labels = {}
        # loop over the predictions
        print(f'prediction array looks like... {prediction}')

        #for (i, p) in enumerate(prediction):
        # grab the prediction information for the current ROI
        (imagenetID, label, prob) = prediction[0][0]
          #print(f'p[0]_{i} looks like... {p[0]}')
        print(f'prediction[0][0] looks like... {prediction[0][0]}')
        pred_label=label
        print(f'pred_label is {pred_label}')
        prob_H=prob

      else:
        prediction = probs.argmax(axis=1)    # class'index of the highest probability
        print(f'prediction looks like... {prediction}') #prediction returned as an array of 1 element
        #label = labelNames[prediction[0]]
        pred_label=classJay[prediction[0]]
        print(f'prediction is class {prediction[0]}')
        print(f'pred_label is {pred_label}')
        #prob_H=prediction[0]
        #print('prob_H is ...%d'%prob_H)

    # extract the image from the testData if using "channels_first"
	  # ordering # (I_line100-105)
      if (K.image_data_format() == "channels_first"):
        imageMatrix = (img[0] * 255).astype("uint8")
        #image = (testX[i][0] * 255).astype("uint8")

       # otherwise we are using "channels_last" ordering
      else:
        imageMatrix = ( prepShape * 255).astype("uint8")
        #image = (testXprep[i] * 255).astype("uint8")

    # initialize the text label color as green (correct)
      color = (0, 255, 0)

      imageMatrix=imageMatrix.reshape(224,224,3) #Crucial

    # otherwise, the class label prediction is incorrect
      #if prediction[0]!= np.argmax(testY[i]):
      #if prediction[0]!= mapTestXto_ALLy[testX[i]]:
      #if prob_H!= mapTestXto_ALLy[testX[i]]:
      if not (pred_label==mapTestXto_ALLtestSC[testX[i]] or pred_label=='seashore' or pred_label=='alp'):
        color=(0,0,255)
        failure+=1
      # merge the channels into one image and resize the image from
    	# 32x32 to 96x96 so we can better see it and then draw the
	    # predicted label on the image

      #label_str=str(classJay[prediction[0]]) # or <pred_label>
      #label_str=str(labelNames[prediction[0]]) # cast to string to avoid...SystemError:
      # <built-in function putText> returned NULL without setting an error

      imageMatrix = cv2.resize(imageMatrix, (96, 96), interpolation=cv2.INTER_LINEAR)

      #cv2.putText(image, labelNames, (5, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.75,color, 2)
      #cv2.putText(imageMatrix, label_str, (5, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.50,color, 2)
      cv2.putText(imageMatrix, pred_label, (5, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.50,color, 2)
      # add the image to our list of output images
      images_Matrix.append(imageMatrix)

      #JayAdd: test data and prediction tracking
      predsTrack.append(prediction[0])
      testYtrack.append(np.argmax(testY[i]))
      success=imgNum-failure
      successRate=(imgNum-failure)*100/imgNum

     # construct the montage for the images
      canvasWidth=0 # initializing the width of the canvas, while the height is set to 4
      if imgNum%6==0:
        canvasWidth=imgNum//6
      else:
        canvasWidth=imgNum//6 +1

      montage = build_montages(images_Matrix, (96, 96), (canvasWidth, 6))[0]

    print('testYTrack array is ...',testYtrack)
    print('predsTrack array is ...',predsTrack)

    #print("Test accuracy " , accuracy_score(testY_revCat , y_pred ))
    if modelJoem.name=='resnet50':
      pass
    else:
      print("Test accuracy " , accuracy_score(testYtrack , predsTrack ))


    # show the output montage
    print('Successful Guess in GREEN & Failed guess in RED...')
    print(' ...GOOD guess: {} ...Bad Guess: {}...with a success rate of: {:.2f}%'.format(success,failure,successRate))

    cv2_imshow(montage)
    #cv2.waitKey(0)

model.name

view_prediction(model,test_ALLdataJay,testALL_y,ALLtest_SubCateg) #Assess with Resnet50

view_prediction(DJSc_model,test_ALLdataJay,testALL_y,ALLtest_SubCateg) #Assess with DJSc_model...works

testY[:10]

"""testY=to_categorical(testALL_y)
  testY_revCat=np.argmax(testY,axis=1)

  #SLICE PREDICTION

  #Slice partition
  print("test_ALLdataJaylength is ...", len(test_ALLdataJay) )
  startPt=int(input('Please provide the slice Start-Point...'))
  print("input startPt is ...", startPt)
  sliceRange=int(input('Please provide the slice range...'))
  print("input sliceRange is ...", sliceRange)

  if startPt + sliceRange<=len(test_ALLdataJay):
    endPt = startPt + sliceRange - 1
    print("slice last index (endPoint) is ...", endPt)
"""

# scale pixels
def prep_pixels(train, test):
	# convert from integers to floats
	train_norm = train.astype('float32')
	test_norm = test.astype('float32')
	# normalize to range 0-1
	train_norm = train_norm / 255.0
	test_norm = test_norm / 255.0
	# return normalized images
	return train_norm, test_norm

"""###HERE...2025Apr21th....!!!!"""

# scale pixels
def testprep_pixels(test):
  test_normList=[]
  for test_path in test:
    test_img=cv2.imread(test_path)
	# convert from integers to floats
    test_norm = test_img.astype('float32')
	# normalize to range 0-1
    test_norm = test_norm / 255.0
	# return normalized images
    test_normList.append(test_norm)
  return test_normList

#Cool slice as it illustrates bad performance on sceneries (INTEL) and good one faces (FFHQ) databases.

def slicePredict(Jmodel):

  from sklearn.metrics import accuracy_score

  # initialize our list of output images
  images_Matrix = []

  testY=to_categorical(testALL_y)
  #testY_revCat=np.argmax(testY,axis=1) # same as testALL_y

  mapTestXto_ALLtestSC=dict(zip(test_ALLdataJay,ALLtest_SubCateg)) #2025Apr22nd

  #Slice partition
  print("test_ALLdataJaylength is ...", len(test_ALLdataJay) )
  startPt=int(input('Please provide the slice Start-Point...'))
  print("input startPt is ...", startPt)
  sliceRange=int(input('Please provide the slice range...'))
  print("input sliceRange is ...", sliceRange)

  if startPt + sliceRange<=len(test_ALLdataJay):
    endPt = startPt + sliceRange - 1
    print("slice last index (endPoint) is ...", endPt)

  prepList=[]
  slice_testY_list=[]
  sliceJ_y_predList=[]
  #Crucial block of code to PEPARE img for model prediction
  for enum in range(startPt,endPt):
    img=cv2.imread(test_ALLdataJay[enum])
    img_rsz=cv2.resize(img, (224,224))
    print(f'img_rsz shape is {img_rsz.shape}')

    prepShape=prepaReshape(img_rsz)
    #CRUCIAL!!! return image.reshape(-1, imageSize, imageSize, 3)
    print(f'img prepa shape is {prepShape.shape}')
    #prepList.append(prepShape) # prediction is per image! No array prediction...but one by one.

  #Slice Prediction on UnTrained Jmodel
    sliceJ_y_pred  = Jmodel.predict(np.array(prepShape)).argmax(-1)
  #sliceJ_y_predList  = Jmodel.predict(np.array(prepList)).argmax(-1) # prediction is per image! No array prediction...but one by one.
    #print('sliceJ_y_predList array is ...',sliceJ_y_predList)

    print('sliceJ_y_pred is ...',sliceJ_y_pred)
  #slice_testY_revCat=testY_revCat[startPt:endPt]
    #slice_testY_revCat=testALL_y[enum]
    print('slice_testY_revCat is ...', testALL_y[enum])

          #begin_2025Apr22nd
    pred_label=classJay[sliceJ_y_pred[0]]
    print(f'prediction is class {sliceJ_y_pred[0]}')
    print(f'pred_label is {pred_label}')

    #testALL_y=np.array(mergedTEST_INTEL_FFHQ_df.test_classCateg)
    #testALL_y2categ=to_categorical(testALL_y)


    # extract the image from the testData if using "channels_first"
	  # ordering # (I_line100-105)
    if (K.image_data_format() == "channels_first"):
      imageMatrix = (img[0] * 255).astype("uint8")
      #image = (testX[i][0] * 255).astype("uint8")

      # otherwise we are using "channels_last" ordering
    else:
      imageMatrix = ( prepShape * 255).astype("uint8")
      #image = (testXprep[i] * 255).astype("uint8")

    # initialize the text label color as green (correct)
    color = (0, 255, 0)

    imageMatrix=imageMatrix.reshape(224,224,3) #Crucial

    if not (pred_label==mapTestXto_ALLtestSC[test_ALLdataJay[enum]] or pred_label=='seashore' or pred_label=='alp'):
      color=(0,0,255)


    imageMatrix = cv2.resize(imageMatrix, (96, 96), interpolation=cv2.INTER_LINEAR)
    cv2.putText(imageMatrix, pred_label, (5, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.50,color, 2)
    images_Matrix.append(imageMatrix)

    slice_testY_list.append(testALL_y[enum])
     #print('slice_testY_revCat array is ...', slice_testY_revCat)
    sliceJ_y_predList.append(sliceJ_y_pred)



  print('slice_testY_list array is ...',slice_testY_list)
  print('sliceJ_y_predList array is ...',sliceJ_y_predList)

  #print("Test accuracy " , accuracy_score(testY_revCat , y_pred ))
  print("Test accuracy " , accuracy_score(slice_testY_list , sliceJ_y_predList ))

  # construct the montage for the images...#Attention!!..below block of Montage is under <for> loop...No Need!!! 2025MAy5th_#Build Montages debug...SUCCESS!!
  canvasWidth=0 # initializing the width of the canvas, while the height is set to 4

  if imgNum%6==0:
    canvasWidth=sliceRange//6
  else:
    canvasWidth=sliceRange//6 +1

  montages = build_montages(images_Matrix, (96, 96), (canvasWidth, 6))[0]
        #end

  cv2_imshow(montages)
  cv2.waitKey(0)

#prep_pixels turns image black...:-)
from keras import backend as K
from tqdm import tqdm #jayAdd for progress bar taqaduma('I love U so much')
from imutils import build_montages

#testXprep = testprep_pixels(np.array(test_ALLdataJay))
albumSize=input('Please provide the album size...')
albumSize=int(albumSize)
album=[]

for enum in np.random.choice(np.arange(0,len(testXprep)), size=(albumSize,)):
  img=cv2.imread(test_ALLdataJay[enum])
  #img_rsz=cv2.resize(img, (224,224))
  album.append(img)

 # construct the montage for the images
canvasWidth=0 # initializing the width of the canvas, while the height is set to 4
if albumSize%6==0:
  canvasWidth=albumSize//6
else:
  canvasWidth=albumSize//6 +1

montages = build_montages(album, (96, 96), (canvasWidth, 6))[0]  # Cucial [0]...
#...to Avoid AttributeError: 'list' object has no attribute 'clip'
cv2_imshow(montages)

#UnComment when needed....
#slicePredict(DJSc_model) #Cool slice as it illustrates bad performance on sceneries (INTEL) and good one faces (FFHQ) databases.

DJSc_model.name

test_ALLdataJay[:5]

#cv2_imshow(testXprep[6])

"""REMINDER
```
for roi in rois:
  #prepare the image roi...CRUCIAL...!!!
  prep_roi=prepaReshape(roi)
  # classify the object
  probs = DJSc_model.predict(prep_roi) #array of probabilities for each of the 7 classes
  prediction = probs.argmax(axis=1) # class'index of the highest probability
  #label = labelNames[prediction[0]]
  label=classJay[prediction[0]]  #
  print(f'roi_{i} label is {label}')
  cv2_imshow(roi)
  i+=1
```
"""

pyramid4jay

#with DJSc_model

# loop over the image pyramid
# strangely non-persistent pyramid objet as yielded items disappear...to have empty list after few seconds

imgBingo=cv2.resize(imgBingo, (300,300)) #Crucial resize to be above (224,224) and avoid error
print(f'[DJSc_test] rsz_imgBingo shape is... {imgBingo.shape}') #DJSc_test

#2025Apr23rd
(H, W) =imgBingo.shape[:2]
print(f'[DJSc_test](H,W) is {H,W}')
# initialize the image pyramid
#pyramid = image_pyramid(orig, scale=PYR_SCALE, minSize=ROI_SIZE)
pyramid4jay = image_pyramid(imgBingo)
# initialize two lists, one to hold the ROIs generated from the image
# pyramid and sliding window, and another list used to store the
# (x, y)-coordinates of where the ROI was in the original image



rois=[]
#rois = list([]) ## list Reinforcement is CRUCIAL to avoid error
locs = []
# time how long it takes to loop over the image pyramid layers and
# sliding window locations
start = time.time()

for img in pyramid4jay:
  count=1 #DJSc_count
  scale=W / float(img.shape[1])
  print(f'img shape is ...{img.shape}')
  for (x, y, roiOrig) in sliding_window(img, WIN_STEP, ROI_SIZE):
    if count>4:
      break
    # scale the (x, y)-coordinates of the ROI with respect to the
    #*original* image dimensions
    x = int(x * scale)
    y = int(y * scale)
    w = int(ROI_SIZE[0] * scale)
    h = int(ROI_SIZE[1] * scale)
    # take the ROI and preprocess it so we can later classify
    # the region using Keras/TensorFlow
    roi = cv2.resize(roiOrig, INPUT_SIZE)
    roi = img_to_array(roi)
    roi = preprocess_input(roi)
    # update our list of ROIs and associated coordinates
    rois.append(roi)
    #list(rois).append(roi) #DJSc_debug...reinforced rois as list at declaration
    print(f'length list rois is {len(list(rois))}') #DJSc_test
    locs.append((x, y, x + w, y + h))
    print(f'length locs is {len(locs)}') #DJSc_test

  #Handling optional visualisation
    visual=1

    # check to see if we are visualizing each of the sliding
    # windows in the image pyramid
    #if args["visualize"] > 0:
    if visual > 0:
      clone=roiOrig.copy()
      # clone the original image and then draw a bounding box
	    # surrounding the current region
	    #clone = orig.copy()
      #cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), 2) #2025Apr24th_check
      cv2.rectangle(clone, (x, y), (x + w, y + h), (0, 255, 0), 2)
      #show the visualization and current ROI
      cv2_imshow(img) #2025Apr24th_check
      cv2_imshow(clone)
      #cv2_imshow(roiOrig)
      cv2_imshow(roi)
      count+=1
      #cv2.waitKey(0) #ERROR
      #error: OpenCV(4.11.0) /io/opencv/modules/highgui/src/window.cpp:1367: error: (-2:Unspecified error)
      #The function is not implemented. Rebuild the library with Windows, GTK+ 2.x or Cocoa support.
      #If you are on Ubuntu or Debian, install libgtk2.0-dev and pkg-config, then re-run cmake or configure script in function 'cvWaitKey'

    #Above, we visualize both the original image with a green box indicating where we are “looking” and the resized ROI, which is ready for classification (Lines 85-95).
      #As you can see, we’ll only --visualize when the flag is set via the command line.
    if len(rois)==0:
      print(f'rois array is EMPTY as length is {len(rois)}') #DJSc_test
      break

  #Next, we’ll (1) check our benchmark on the pyramid + sliding window process,
    #(2) classify all of our rois in batch, and (3) decode predictions:

  # show how long it took to loop over the image pyramid layers and
# sliding window locations
end = time.time()
print("[INFO] looping over pyramid/windows took {:.5f} seconds".format(end - start))
# convert the ROIs to a NumPy array
rois = np.array(rois, dtype="float32")
# classify each of the proposal ROIs using ResNet and then show how
# long the classifications took

print("[INFO] classifying ROIs...")
start = time.time()

preds = DJSc_model.predict(rois)

end = time.time()
print("[INFO] classifying ROIs took {:.5f} seconds".format(end - start))
# decode the predictions and initialize a dictionary which maps class
# labels (keys) to any ROIs associated with that label (values)

#Finally, Line 117 below decodes the predictions, grabbing only the top prediction for each ROI.
#preds = imagenet_utils.decode_predictions(preds, top=1)
np.argmax(DJSc_model.predict(rois), axis=-1)[0] #replacing DJSc_model.predict_classes(rois)

#We’ll need a means to map class labels (keys) to ROI locations associated with that label (values);
#the labels dictionary (Line 118) serves that purpose.

#Let’s go ahead and populate our labels dictionary now:
labels = {}

"""**def jayMassPrediction REFERENCE**
```
def jayMassPrediction(Jmodel):
  # Jmodel MASS PREDICTION (pending 'test_ALLdataJay' ... DONE)
  from sklearn.metrics import accuracy_score

  #y_pred  = Jmodel.predict(testXprep ).argmax(-1)
  #J_y_pred  = Jmodel.predict(np.array(test_ALLdataJay)).argmax(-1)

  #SLICE PREDICTION

  #Slice partiton
  print("test_ALLdataJaylength is ...", len(test_ALLdataJay) )
  startPt=int(input('Please provide the slice Start-Point...'))
  print("input startPt is ...", startPt)
  sliceRange=int(input('Please provide the slice range...'))
  print("input sliceRange is ...", sliceRange)


  if startPt + sliceRange>len(test_ALLdataJay):    
    print('Last index slice Not Available...function closing...sorry!')

  endPt = startPt + sliceRange - 1
  print("slice last index (endPoint) is ...", endPt)

  #Slice Prediction on UnTraine Jmodel
  sliceJ_y_pred  = Jmodel.predict(np.array(test_ALLdataJay[startPt:endPt])).argmax(-1)
  print('sliceJ_y_pred array is ...',sliceJ_y_pred)
  slice_testY_revCat=testY_revCat[startPt:endPt]
  print('slice_testY_revCat array is ...', slice_testY_revCat)

  #print("Test accuracy " , accuracy_score(testY_revCat , y_pred ))
  print("Test accuracy " , accuracy_score(slice_testY_revCat , sliceJ_y_pred ))


```

REMINDER
```
np.random.choice(imgDIR_path_list)
imgBingo_path=np.random.choice(imgDIR_path_list)
imgBingo_path
OUTPUT_sample >>> ./originalPics/2002/07/20/big/img_656.jpg

pyramid = image_pyramid(imgBingo_rsz, scale=PYR_SCALE, minSize=ROI_SIZE) #

```
"""

imgList=[cv2.imread(imgPath) for imgPath in imgDIR_path_list]
#np.array(imgList).shape
imgList
len(imgList)

DJSc_model.input_shape

to_categorical(3,7)

to_categorical(testALL_y,7) #def to_categorical(x, num_classes=None):

np.argmax(testY,axis=1)

#bingoMassPrediction(bbFACE_model,test_ALLdataJay, test_y)USE with INTEL-FFHQ data

"""
```imagenet_utils.decode_prediction```

Decodes the prediction of an ImageNet model.

**Arguments:**

  **preds:**    
  * Numpy array encoding a batch of predictions.
  *top: Integer, how many top-guesses to return. Defaults to 5.

**Returns:**

  A list of lists of top class prediction tuples
(class_name, class_description, score)."""





pyramid4jay

"""**Resumption** 2025Apr22nd..."""

pyramid4jay

#Debug,,,,,
rand=np.random.randint(0,len(test_ALLdataJay)) #ValueError: math domain error
imgBingo4jay_path=test_ALLdataJay[rand] #ValueError: math domain error
print(f'imgBingo4jay Global-path is... {test_ALLdataJay[rand]}')
#print(f'imgBingo4jay path is... {imgBingo4jay_path}')
orig = cv2.imread(imgBingo4jay_path)
print(f'[DJSc_test] orig shape is... {orig.shape}') #DJSc_test
pyramid4jay = image_pyramid(orig)
rois=[]
#rois = list([]) ## list Reinforcement is CRUCIAL to avoid error...
#... AttributeError: 'numpy.ndarray' object has no attribute 'append'
locs = []
# time how long it takes to loop over the image pyramid layers and
# sliding window locations
start = time.time()
for img in pyramid4jay:
  count=1 #DJSc_count
  scale=jW / float(img.shape[1])
  for (x, y, roiOrig) in sliding_window(img, WIN_STEP, ROI_SIZE):
    if count>6:
      break
    # scale the (x, y)-coordinates of the ROI with respect to the
    #*original* image dimensions
    x = int(x * scale)
    y = int(y * scale)
    w = int(ROI_SIZE[0] * scale)
    h = int(ROI_SIZE[1] * scale)
    # take the ROI and preprocess it so we can later classify
    # the region using Keras/TensorFlow
    roi = cv2.resize(roiOrig, INPUT_SIZE)
    roi = img_to_array(roi)
    roi = preprocess_input(roi)
    # update our list of ROIs and associated coordinates
    rois.append(roi)
    #list(rois).append(roi) #DJSc_debug...reinforced rois as list at declaration
  print(f'[DJSc_test] length list rois is {len(rois)}') #DJSc_test

os.listdir('./thumbnails128x128')[:5]

imgBingo4jay_path=test_ALLdataJay[rand]
imgBingo4jay_path

orig = cv2.imread(imgBingo4jay_path)
orig

"""###for ...ResNet50"""

# for ...ResNet50
# initialize variables used for the object detection procedure
#WIDTH = 600
WIDTH = 400
PYR_SCALE = 1.5
WIN_STEP = 16
#ROI_SIZE = eval(args["size"])
ROI_SIZE = (224,224)
INPUT_SIZE = (224, 224)
model=Res50_model
#Reminder model = ResNet50(weights="imagenet", include_top=True)
print(f'model name is... {model.name}')

# load the input image from disk, resize it such that it has the
# has the supplied width, and then grab its dimensions

rand=np.random.randint(0,len(test_ALLdataJay)) #ValueError: math domain error

# !!!...[DJSc_ATTENTION]....Amend <test_ALLdataJay> after data reSplit....!!!!
imgBingo4jay_path=test_ALLdataJay[rand] #ValueError: math domain error


#print(f'imgBingo4jay Global-path is... {test_ALLdataJay[rand]}')
print(f'imgBingo4jay path is... {imgBingo4jay_path}')

#imgBingo4jay_path=np.random.choice(imgDIR_path_list)
#print(f'imgBingo4jay DIRpath is... {imgBingo4jay_path}')

orig = cv2.imread(imgBingo4jay_path)
#deactivate (comment) below as (maybe) source of error with <rois> list...
#... #... AttributeError: 'numpy.ndarray' object has no attribute 'append'
#orig = imutils.resize(orig, width=WIDTH)
#(H, W) = orig.shape[:2]
print(f'[DJSc_test] orig shape is... {orig.shape}') #DJSc_test

orig=cv2.resize(orig, (300,300)) #Crucial resize to be above (224,224) and avoid error
print(f'[DJSc_test] rsz_orig shape is... {orig.shape}') #DJSc_test

#2025Apr23rd
(H, W) = orig.shape[:2]
print(f'[DJSc_test](H,W) is {H,W}')


# initialize the image pyramid
#pyramid = image_pyramid(orig, scale=PYR_SCALE, minSize=ROI_SIZE)
pyramid4jay = image_pyramid(orig)
# initialize two lists, one to hold the ROIs generated from the image
# pyramid and sliding window, and another list used to store the
# (x, y)-coordinates of where the ROI was in the original image

rois=[]
#rois = list([]) ## list Reinforcement is CRUCIAL to avoid error...
#... AttributeError: 'numpy.ndarray' object has no attribute 'append'
locs = []
# time how long it takes to loop over the image pyramid layers and
# sliding window locations
start = time.time()
for img in pyramid4jay:
  count=1 #DJSc_count
  scale=W / float(img.shape[1]) #2025Apr23rd...jW for W
  for (x, y, roiOrig) in sliding_window(img, WIN_STEP, ROI_SIZE):
    if count>6:
      break
    # scale the (x, y)-coordinates of the ROI with respect to the
    #*original* image dimensions
    x = int(x * scale)
    y = int(y * scale)
    w = int(ROI_SIZE[0] * scale)
    h = int(ROI_SIZE[1] * scale)
    # take the ROI and preprocess it so we can later classify
    # the region using Keras/TensorFlow
    roi = cv2.resize(roiOrig, INPUT_SIZE)
    roi = img_to_array(roi)
    roi = preprocess_input(roi)
    # update our list of ROIs and associated coordinates
    rois.append(roi)
    #list(rois).append(roi) #DJSc_debug...reinforced rois as list at declaration
    print(f'[DJSc_test] length list rois is {len(list(rois))}') #DJSc_test
    locs.append((x, y, x + w, y + h))
    print(f'length locs is {len(locs)}') #DJSc_test

  #Handling optional visualisation
    visual=1

    # check to see if we are visualizing each of the sliding
    # windows in the image pyramid
    #if args["visualize"] > 0:
    if visual > 0:
      # clone the original image and then draw a bounding box
	    # surrounding the current region
      clone = orig.copy()
      cv2.rectangle(clone, (x, y), (x + w, y + h), (0, 255, 0), 2)
      #show the visualization and current ROI
      cv2_imshow(clone)
      cv2_imshow(roiOrig)
      count+=1
      #cv2.waitKey(0) #ERROR
      #error: OpenCV(4.11.0) /io/opencv/modules/highgui/src/window.cpp:1367: error: (-2:Unspecified error)
      #The function is not implemented. Rebuild the library with Windows, GTK+ 2.x or Cocoa support.
      #If you are on Ubuntu or Debian, install libgtk2.0-dev and pkg-config, then re-run cmake or configure script in function 'cvWaitKey'

    #Above, we visualize both the original image with a green box indicating where we are “looking” and the resized ROI, which is ready for classification (Lines 85-95).
      #As you can see, we’ll only --visualize when the flag is set via the command line.
    if len(rois)==0:
      print(f'rois array is EMPTY as length is {len(rois)}') #DJSc_test
      break

  #Next, we’ll (1) check our benchmark on the pyramid + sliding window process,
    #(2) classify all of our rois in batch, and (3) decode predictions:

  # show how long it took to loop over the image pyramid layers and
# sliding window locations
end = time.time()
print("[INFO] looping over pyramid/windows took {:.5f} seconds".format(end - start))
# convert the ROIs to a NumPy array
rois = np.array(rois, dtype="float32")
# classify each of the proposal ROIs using ResNet and then show how
# long the classifications took

print("[INFO] classifying ROIs...")
start = time.time()
preds = model.predict(rois)
# loop over the image pyramid4jay
# strangely non-persistent pyramid objet as yielded items disappear...to have empty list after few seconds
end = time.time()
print("[INFO] classifying ROIs took {:.5f} seconds".format(	end - start))
# decode the predictions and initialize a dictionary which maps class
# labels (keys) to any ROIs associated with that label (values)

#Finally, Line 117 below decodes the predictions, grabbing only the top prediction for each ROI.
preds = imagenet_utils.decode_predictions(preds, top=1)

labels = {}

  # loop over the predictions
print(f'prediction array looks like... {preds}')
for (i, p) in enumerate(preds):
  # grab the prediction information for the current ROI
  (imagenetID, label, prob) = p[0]

	# filter out weak detections by ensuring the predicted probability
	# is greater than the minimum probability
  #if prob >= args["min_conf"]:
  if prob >= 0.4:
		# grab the bounding box associated with the prediction and
		# convert the coordinates
    box = locs[i]
    # grab the list of predictions for the label and add the
		# bounding box and probability to the list
    L = labels.get(label, [])
    L.append((box, prob))
    labels[label] = L
'''
We’re not quite done yet with turning our image classifier into an object detector with Keras, TensorFlow, and OpenCV.
Now, we need to visualize the results.

This is the time where you would implement logic to do something useful with the results (labels), whereas in our case,
we’re simply going to annotate the objects. We will also have to handle our overlapping detections by means of non-maxima suppression(NMS).

Let’s go ahead and loop over over all keys in our labels list:

'''
# loop over the labels for each of detected objects in the image #L139
for label in labels.keys():
	# clone the original image so that we can draw on it
  print("[INFO] showing results for '{}'".format(label))
  clone = orig.copy()

  # loop over all bounding boxes for the current label
  for (box, prob) in labels[label]:
    # draw the bounding box on the image
    (startX, startY, endX, endY) = box
    cv2.rectangle(clone, (startX, startY), (endX, endY), (0, 255, 0), 2)
  # show the results *before* applying non-maxima suppression, then
  # clone the image again so we can display the results *after*
  # applying non-maxima suppression
  #cv2.imshow("Before", clone)
  print('Before...')
  cv2_imshow(clone)
  clone = orig.copy()

  '''
  Our loop over the labels for each of the detected objects begins on Line 139.

  We make a copy of the original input image so that we can annotate it (Line 142).

  We then annotate all bounding boxes for the current label (Lines 145-149).

  So that we can visualize the before/after applying NMS, Line 154 displays the “before” image,
  and then we proceed to make another copy (Line 155).

  Now, let’s apply NMS and display our “after” NMS visualization:

  '''

  # extract the bounding boxes and associated prediction
	# probabilities, then apply non-maxima suppression
  boxes = np.array([p[0] for p in labels[label]])
  proba = np.array([p[1] for p in labels[label]])
  boxes = non_max_suppression(boxes, proba)
  # loop over all bounding boxes that were kept after applying
	# non-maxima suppression
  for (startX, startY, endX, endY) in boxes:
    # draw the bounding box and label on the image
    cv2.rectangle(clone, (startX, startY), (endX, endY),
			(0, 255, 0), 2)
    y = startY - 10 if startY - 10 > 10 else startY + 10
    cv2.putText(clone, label, (startX, y),cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 255, 0), 2)
	  # show the output after apply non-maxima suppression
    #cv2.imshow("After", clone)
    print('After...')
    cv2_imshow(clone)
    #cv2.waitKey(0)

"""###Master Keras’ `predict_on_batch` Method: A Complete Guide for AI Enthusiasts
Karthik Karunakaran, Ph.D.
Jan 4, 2025
https://medium.com/@iitkarthik/master-keras-predict-on-batch-method-a-complete-guide-for-ai-enthusiasts-48666239b759

####**What is the predict_on_batch Method?**
The predict_on_batch method in Keras allows you to perform predictions on a batch of data at once. Unlike methods like model.predict, which process data in a more generalized way, predict_on_batch is tailored for batch-wise operations, making it ideal for:

* Handling large datasets that need real-time or iterative predictions.
*Optimizing memory usage by processing data in smaller chunks.
*Ensuring efficient computation during model inference in production environments.

[..]

3. Make Batch Predictions
Invoke the predict_on_batch method:
```
# Get predictions for the batch
predictions = model.predict_on_batch(data_batch)
print("Predictions:", predictions)
```
4. Interpret Results
The predictions array contains probability scores for each class. Use numpy to derive class labels:
```
predicted_classes = np.argmax(predictions, axis=1)
print("Predicted Classes:", predicted_classes)
```

###for ...DJSc_model
"""

rois.shape, len(rois)

rois

DJSc_model.predict_on_batch(rois)

DJSc_model.predict_on_batch(rois).argmax(-1)

np.argmax(DJSc_model.predict_on_batch(rois), axis=-1)

np.argmax(DJSc_model.predict_on_batch(rois), axis=1)

test_ALLdataJay[:10], len(test_ALLdataJay)

testALL_y[:150], len(testALL_y)

ALLtest_SubCateg[:10], len(ALLtest_SubCateg)

"""```
'''
We’re not quite done yet with turning our image classifier into an object detector with Keras, TensorFlow, and OpenCV.
Now, we need to visualize the results.

This is the time where you would implement logic to do something useful with the results (labels), whereas in our case,
we’re simply going to annotate the objects. We will also have to handle our overlapping detections by means of non-maxima suppression(NMS).

Let’s go ahead and loop over over all keys in our labels list:

'''
```

```
# for ...DJSc_model

# initialize variables used for the object detection procedure
#WIDTH = 600
WIDTH = 300
PYR_SCALE = 1.5
WIN_STEP = 16
#ROI_SIZE = eval(args["size"])
ROI_SIZE = (224,224)
INPUT_SIZE = (224, 224)
model=DJSc_model

# initialize our list of output images
images_Matrix = []
#jayAdd for evaluation metrics
testYtrack=[]
predsTrack=[]
#failure=0

#Map testX to testALL_y
mapTestXto_ALLy=dict(zip(test_ALLdataJay,testALL_y))
mapTestXto_ALLtestSC=dict(zip(test_ALLdataJay,ALLtest_SubCateg))

#Reminder model = ResNet50(weights="imagenet", include_top=True)
print(f'model name is... {model.name}')

#GroundTruth COLLECTOR DJSc_2025Apr29
#groundTruth={}



# load the input image from disk, resize it such that it has the
# has the supplied width, and then grab its dimensions
#imgBingo4jay_path=np.random.choice(imgDIR_path_list)
rand=np.random.randint(0,len(test_ALLdataJay))
img4jay_path=test_ALLdataJay[rand]
#img4jay_path=np.random.random_(mapTestXto_ALLy.keys())
print(f'img4jay path is... {img4jay_path}')
print(f'img4jay class is... {mapTestXto_ALLy[img4jay_path]}')
print(f'img4jay label is... {mapTestXto_ALLtestSC[img4jay_path]}')
orig = cv2.imread(img4jay_path)
#deactivate (comment) below as (maybe) source of error with <rois> list...
#... #... AttributeError: 'numpy.ndarray' object has no attribute 'append'
#orig = imutils.resize(orig, width=WIDTH)
#(H, W) = orig.shape[:2]

orig=cv2.resize(orig, (400,400)) #...!!!,,,Crucial resize to be above (224,224) and avoid error
#print(f'[DJSc_test] rsz_orig shape is... {orig.shape}') #DJSc_test

#2025Apr23rd
(H, W) = orig.shape[:2]
print(f'[DJSc_test](H,W) is {H,W}')

# initialize the image pyramid
#pyramid = image_pyramid(orig, scale=PYR_SCALE, minSize=ROI_SIZE)# avoid ROI_size...!!!
pyramid4jay = image_pyramid(orig)
# initialize two lists, one to hold the ROIs generated from the image
# pyramid and sliding window, and another list used to store the
# (x, y)-coordinates of where the ROI was in the original image

rois=[]
#rois = list([]) ## list Reinforcement is CRUCIAL to avoid error...
#... AttributeError: 'numpy.ndarray' object has no attribute 'append'
locs = []
# time how long it takes to loop over the image pyramid layers and
# sliding window locations
start = time.time()
print(f'pyramind4jay is... {pyramid4jay}')
for img in pyramid4jay:
  count=1 #DJSc_count
  scale=W / float(img.shape[1]) #2025Apr23rd...jW for W

  for (x, y, roiOrig) in sliding_window(img, WIN_STEP, ROI_SIZE):
    if count>6:
      break
      #continue

    # scale the (x, y)-coordinates of the ROI with respect to the
    #*original* image dimensions
    x = int(x * scale)
    y = int(y * scale)
    w = int(ROI_SIZE[0] * scale)
    h = int(ROI_SIZE[1] * scale)
    # take the ROI and preprocess it so we can later classify
    # the region using Keras/TensorFlow
    roi = cv2.resize(roiOrig, INPUT_SIZE)
    print(f'roi shape is {roi.shape}')
    roi = img_to_array(roi) ##
    #roi = preprocess_input(roi)
    #prep_roi=prepaReshape(roi) #maybe Not required
    # classify the
    #print(f'[DJSc_test]count_{count} roi shape is {roi.shape}')
    #print(f'[DJSc_test]count_{count} prep_roi shape is {prep_roi.shape}')
    # update our list of ROIs and associated coordinates

    rois.append(roi) #NOT appending...!!!

    #print(f'[DJSc_test]count_{count} shape list rois is {rois.shape}') #DJSc_test
    #list(rois).append(roi) #DJSc_debug...reinforced rois as list at declaration
    #print(f'[DJSc_test]count_{count} length list rois is {len(list(rois))}') #DJSc_test
    locs.append((x, y, x + w, y + h))
    #print(f'[DJSc_test]count_{count} length locs is {len(locs)}') #DJSc_test

    #Handling optional visualisation
    visual=1

    # check to see if we are visualizing each of the sliding
    # windows in the image pyramid
    #if args["visualize"] > 0:
    if visual > 0:
      # clone the original image and then draw a bounding box
	    # surrounding the current region
      clone = orig.copy()
      cv2.rectangle(clone, (x, y), (x + w, y + h), (0, 255, 0), 2)
      #show the visualization and current ROI
      cv2_imshow(clone)
      cv2_imshow(roiOrig)
      count+=1
      #cv2.waitKey(0) #ERROR
      #error: OpenCV(4.11.0) /io/opencv/modules/highgui/src/window.cpp:1367: error: (-2:Unspecified error)
      #The function is not implemented. Rebuild the library with Windows, GTK+ 2.x or Cocoa support.
      #If you are on Ubuntu or Debian, install libgtk2.0-dev and pkg-config, then re-run cmake or configure script in function 'cvWaitKey'

    #Above, we visualize both the original image with a green box indicating where we are “looking” and the resized ROI, which is ready for classification (Lines 85-95).
      #As you can see, we’ll only --visualize when the flag is set via the command line.

  print(f'rois array with <roi> (before DJSc_test) length is {len(rois)}') #DJSc_test

  if len(rois)==0:
    print(f'rois array with <roi> is EMPTY as length is {len(rois)}') #DJSc_test
    break
    #continue

  #Next, we’ll (1) check our benchmark on the pyramid + sliding window process,
    #(2) classify all of our rois in batch, and (3) decode predictions:

  # show how long it took to loop over the image pyramid layers and
# sliding window locations
end = time.time()
print("[INFO] looping over pyramid/windows took {:.5f} seconds".format(end - start))
# convert the ROIs to a NumPy array
rois = np.array(rois, dtype="float32")


# classify each of the proposal ROIs using ResNet and then show how
# long the classifications took

print("[INFO] classifying ROIs...")
#print(f'rois array with <roi> length is {len(rois)}') #DJSc_test
#print(f'rois array with <roi> shape is {rois.shape}') #DJSc_test

start = time.time()
#DJSc_model specific...!!!
#preds = model.predict(rois)
j_rois=rois#2025Apr23rd Debug...
preds=model.predict_on_batch(j_rois)
# loop over the image pyramid4jay
# strangely non-persistent pyramid objet as yielded items disappear...to have empty list after few seconds
end = time.time()
print("[INFO] classifying ROIs took {:.5f} seconds".format(	end - start))
# decode the predictions and initialize a dictionary which maps class
# labels (keys) to any ROIs associated with that label (values)

print(f'preds array looks like... {preds}')

#Finally, Line 117 below decodes the predictions, grabbing only the top prediction for each ROI.
#preds = imagenet_utils.decode_predictions(preds, top=1)
predictions=np.argmax(preds, axis=-1)
print(f'[DJSc_check] predictions length is... {len(predictions)}')
print(f'[DJSc_check] predictions array looks like... {predictions}')
probs=np.max(preds, axis=-1)
print(f'probs length is... {len(probs)}')
print(f'probs array looks like... {probs}')

labels = {}
pred_labels = {}

print(predictions.dtype)
  # loop over the predictions
  #2025Apr23th_FIX...

# loop over the predictions
#for (i, p) in enumerate(preds):
for loc, i, p in zip(locs,predictions, probs):
  # grab the prediction information for the current ROI
  #(imagenetID, label, prob) = p[0]
  print(f'loc is {loc}')
  #print(f'[DJSc_test] i is {i}')
  prob= p
  #print(f'[DJSc_test] prob is {p}')
  labels[i]=classJay[i]
  #print(f'[DJSc_test] pred_labels[{i}] is ...{pred_labels[i]}')
	# filter out weak detections by ensuring the predicted probability
	# is greater than the minimum probability
  #if prob >= args["min_conf"]:
  if prob >= 0.4:
		# grab the bounding box associated with the prediction and
		# convert the coordinates
    box = loc
    # grab the list of predictions for the label and add the
		# bounding box and probability to the list
    #L = labels.get(label, [])
    L = pred_labels.get(labels[i], [])
    L.append((box, prob))
    #labels[label] = L
    pred_labels[labels[i]] = L
    #print(f'[DJSc_test] pred_labels[pred_labels[{i}]] is ...{L}')
  print(f'[DJSc_test_test] pred_labels[{labels[i]}] is ...{pred_labels[labels[i]]}')
    #print(f'[DJSc_test] pred_labels is {pred_labels}')

# loop over the labels for each of detected objects in the image #L139
for label in pred_labels.keys():
	# clone the original image so that we can draw on it
  print("[INFO] showing results for predicted label '{}'".format(label))
  clone = orig.copy()

  # loop over all bounding boxes for the current label
  for (box, prob) in pred_labels[label]:
    # draw the bounding box on the image
    (startX, startY, endX, endY) = box
    cv2.rectangle(clone, (startX, startY), (endX, endY), (0, 255, 0), 2)
  # show the results *before* applying non-maxima suppression, then
  # clone the image again so we can display the results *after*
  # applying non-maxima suppression
  #cv2.imshow("Before", clone)
  print(f'Before NMS (Non-Maxima Suppression) for predicted label {label}...')
  cv2_imshow(clone)
  clone = orig.copy()

  '''
  Our loop over the labels for each of the detected objects begins on Line 139.

  We make a copy of the original input image so that we can annotate it (Line 142).

  We then annotate all bounding boxes for the current label (Lines 145-149).

  So that we can visualize the before/after applying NMS, Line 154 displays the “before” image,
  and then we proceed to make another copy (Line 155).

  Now, let’s apply NMS and display our “after” NMS visualization:

  '''

  # extract the bounding boxes and associated prediction
	# probabilities, then apply non-maxima suppression
  boxes = np.array([p[0] for p in pred_labels[label]])
  proba = np.array([p[1] for p in pred_labels[label]])
  boxes = non_max_suppression(boxes, proba)
  # loop over all bounding boxes that were kept after applying
	# non-maxima suppression
  for (startX, startY, endX, endY) in boxes:
    # draw the bounding box and label on the image
    cv2.rectangle(clone, (startX, startY), (endX, endY),
			(0, 255, 0), 2)
    y = startY - 10 if startY - 10 > 10 else startY + 10
    #y = startY - 5 if startY - 5 > 5 else startY + 5
    cv2.putText(clone, label, (startX, y),cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 255, 0), 2)
	# show the output after apply non-maxima suppression
  #cv2.imshow("After", clone)
  print('[DJSc_nms] After NMS (Non-Maxima Suppression)...')
  cv2_imshow(clone)
  cv2.waitKey(0)






```

# Make (Ground Thruth) TRAINING Data for... `DJSc_cls7W36L23model_ObjectDetect_fromScratch_pureCNN_Seachaos.ipynb`

###GroundTruth SINGLE Collection (from DJSc_2025Apr29)
GroundTruth COLLECTOR

`groundTruth={}`



!!!...Eureka....!! (2025May1st)

Gr8 counter example img4jay path is...
./IntelClassImg_GoodOne/NonFace_seg_test/seg_test/sea/24311.jpg
"""

#GroundTruth COLLECTOR DJSc_2025Apr29
groundTruth={}

# for SINGLE Collection from...DJSc_model

# initialize variables used for the object detection procedure
#WIDTH = 600
WIDTH = 300
PYR_SCALE = 1.5
WIN_STEP = 16
#ROI_SIZE = eval(args["size"])
ROI_SIZE = (224,224)
INPUT_SIZE = (224, 224)
model=DJSc_model

# initialize our list of output images
images_Matrix = []
#jayAdd for evaluation metrics
testYtrack=[]
predsTrack=[]
#failure=0

#Map testX to testALL_y
mapTestXto_ALLy=dict(zip(test_ALLdataJay,testALL_y))
mapTestXto_ALLtestSC=dict(zip(test_ALLdataJay,ALLtest_SubCateg))

#Reminder model = ResNet50(weights="imagenet", include_top=True)
print(f'model name is... {model.name}')

#GroundTruth COLLECTOR DJSc_2025Apr29
#groundTruth={}



# load the input image from disk, resize it such that it has the
# has the supplied width, and then grab its dimensions
#imgBingo4jay_path=np.random.choice(imgDIR_path_list)
rand=np.random.randint(0,len(test_ALLdataJay))
img4jay_path=test_ALLdataJay[rand]
#img4jay_path=np.random.random_(mapTestXto_ALLy.keys())
print(f'img4jay path is... {img4jay_path}')
print(f'img4jay class is... {mapTestXto_ALLy[img4jay_path]}')
print(f'img4jay label is... {mapTestXto_ALLtestSC[img4jay_path]}')
orig = cv2.imread(img4jay_path)
#deactivate (comment) below as (maybe) source of error with <rois> list...
#... #... AttributeError: 'numpy.ndarray' object has no attribute 'append'
#orig = imutils.resize(orig, width=WIDTH)
#(H, W) = orig.shape[:2]

orig=cv2.resize(orig, (400,400)) #...!!!,,,Crucial resize to be above (224,224) and avoid error
#print(f'[DJSc_test] rsz_orig shape is... {orig.shape}') #DJSc_test

#2025Apr23rd
(H, W) = orig.shape[:2]
print(f'[DJSc_test](H,W) is {H,W}')

# initialize the image pyramid
#pyramid = image_pyramid(orig, scale=PYR_SCALE, minSize=ROI_SIZE)# avoid ROI_size...!!!
pyramid4jay = image_pyramid(orig)
# initialize two lists, one to hold the ROIs generated from the image
# pyramid and sliding window, and another list used to store the
# (x, y)-coordinates of where the ROI was in the original image

rois=[]
#rois = list([]) ## list Reinforcement is CRUCIAL to avoid error...
#... AttributeError: 'numpy.ndarray' object has no attribute 'append'
locs = []
# time how long it takes to loop over the image pyramid layers and
# sliding window locations
start = time.time()
print(f'pyramind4jay is... {pyramid4jay}')
for img in pyramid4jay:
  count=1 #DJSc_count
  scale=W / float(img.shape[1]) #2025Apr23rd...jW for W

  for (x, y, roiOrig) in sliding_window(img, WIN_STEP, ROI_SIZE):
    if count>6:
      break
      #continue

    # scale the (x, y)-coordinates of the ROI with respect to the
    #*original* image dimensions
    x = int(x * scale)
    y = int(y * scale)
    w = int(ROI_SIZE[0] * scale)
    h = int(ROI_SIZE[1] * scale)
    # take the ROI and preprocess it so we can later classify
    # the region using Keras/TensorFlow
    roi = cv2.resize(roiOrig, INPUT_SIZE)
    print(f'roi shape is {roi.shape}')
    roi = img_to_array(roi) ##
    #roi = preprocess_input(roi)
    #prep_roi=prepaReshape(roi) #maybe Not required
    # classify the
    #print(f'[DJSc_test]count_{count} roi shape is {roi.shape}')
    #print(f'[DJSc_test]count_{count} prep_roi shape is {prep_roi.shape}')
    # update our list of ROIs and associated coordinates

    rois.append(roi) #NOT appending...!!!

    #print(f'[DJSc_test]count_{count} shape list rois is {rois.shape}') #DJSc_test
    #list(rois).append(roi) #DJSc_debug...reinforced rois as list at declaration
    #print(f'[DJSc_test]count_{count} length list rois is {len(list(rois))}') #DJSc_test
    locs.append((x, y, x + w, y + h))
    #print(f'[DJSc_test]count_{count} length locs is {len(locs)}') #DJSc_test

    #Handling optional visualisation
    visual=1

    # check to see if we are visualizing each of the sliding
    # windows in the image pyramid
    #if args["visualize"] > 0:
    if visual > 0:
      # clone the original image and then draw a bounding box
	    # surrounding the current region
      clone = orig.copy()
      cv2.rectangle(clone, (x, y), (x + w, y + h), (0, 255, 0), 2)
      #show the visualization and current ROI
      cv2_imshow(clone)
      cv2_imshow(roiOrig)
      count+=1
      #cv2.waitKey(0) #ERROR
      #error: OpenCV(4.11.0) /io/opencv/modules/highgui/src/window.cpp:1367: error: (-2:Unspecified error)
      #The function is not implemented. Rebuild the library with Windows, GTK+ 2.x or Cocoa support.
      #If you are on Ubuntu or Debian, install libgtk2.0-dev and pkg-config, then re-run cmake or configure script in function 'cvWaitKey'

    #Above, we visualize both the original image with a green box indicating where we are “looking” and the resized ROI, which is ready for classification (Lines 85-95).
      #As you can see, we’ll only --visualize when the flag is set via the command line.

  print(f'rois array with <roi> (before DJSc_test) length is {len(rois)}') #DJSc_test

  if len(rois)==0:
    print(f'rois array with <roi> is EMPTY as length is {len(rois)}') #DJSc_test
    break
    #continue

  #Next, we’ll (1) check our benchmark on the pyramid + sliding window process,
    #(2) classify all of our rois in batch, and (3) decode predictions:

  # show how long it took to loop over the image pyramid layers and
# sliding window locations
end = time.time()
print("[INFO] looping over pyramid/windows took {:.5f} seconds".format(end - start))
# convert the ROIs to a NumPy array
rois = np.array(rois, dtype="float32")


# classify each of the proposal ROIs using ResNet and then show how
# long the classifications took

print("[INFO] classifying ROIs...")
#print(f'rois array with <roi> length is {len(rois)}') #DJSc_test
#print(f'rois array with <roi> shape is {rois.shape}') #DJSc_test

start = time.time()
#DJSc_model specific...!!!
#preds = model.predict(rois)
j_rois=rois#2025Apr23rd Debug...
preds=model.predict_on_batch(j_rois)
# loop over the image pyramid4jay
# strangely non-persistent pyramid objet as yielded items disappear...to have empty list after few seconds
end = time.time()
print("[INFO] classifying ROIs took {:.5f} seconds".format(	end - start))
# decode the predictions and initialize a dictionary which maps class
# labels (keys) to any ROIs associated with that label (values)

print(f'preds array looks like... {preds}')

#Finally, Line 117 below decodes the predictions, grabbing only the top prediction for each ROI.
#preds = imagenet_utils.decode_predictions(preds, top=1)
predictions=np.argmax(preds, axis=-1)
print(f'[DJSc_check] predictions length is... {len(predictions)}')
print(f'[DJSc_check] predictions array looks like... {predictions}')
probs=np.max(preds, axis=-1)
print(f'probs length is... {len(probs)}')
print(f'probs array looks like... {probs}')

labels = {}
pred_labels = {}

print(predictions.dtype)
  # loop over the predictions
  #2025Apr23th_FIX...

# loop over the predictions
#for (i, p) in enumerate(preds):
for loc, i, p in zip(locs,predictions, probs):
  # grab the prediction information for the current ROI
  #(imagenetID, label, prob) = p[0]
  print(f'loc is {loc}')
  #print(f'[DJSc_test] i is {i}')
  prob= p
  #print(f'[DJSc_test] prob is {p}')
  labels[i]=classJay[i]
  #print(f'[DJSc_test] pred_labels[{i}] is ...{pred_labels[i]}')
	# filter out weak detections by ensuring the predicted probability
	# is greater than the minimum probability
  #if prob >= args["min_conf"]:
  if prob >= 0.4:
		# grab the bounding box associated with the prediction and
		# convert the coordinates
    box = loc
    # grab the list of predictions for the label and add the
		# bounding box and probability to the list
    #L = labels.get(label, [])
    L = pred_labels.get(labels[i], [])
    L.append((box, prob))
    #labels[label] = L
    pred_labels[labels[i]] = L
    #print(f'[DJSc_test] pred_labels[pred_labels[{i}]] is ...{L}')
  print(f'[DJSc_test_test] pred_labels[{labels[i]}] is ...{pred_labels[labels[i]]}')
    #print(f'[DJSc_test] pred_labels is {pred_labels}')

# loop over the labels for each of detected objects in the image #L139
for label in pred_labels.keys():
	# clone the original image so that we can draw on it
  print("[INFO] showing results for predicted label '{}'".format(label))
  clone = orig.copy()

  # loop over all bounding boxes for the current label
  for (box, prob) in pred_labels[label]:
    # draw the bounding box on the image
    (startX, startY, endX, endY) = box
    cv2.rectangle(clone, (startX, startY), (endX, endY), (0, 255, 0), 2)
  # show the results *before* applying non-maxima suppression, then
  # clone the image again so we can display the results *after*
  # applying non-maxima suppression
  #cv2.imshow("Before", clone)
  print(f'Before NMS (Non-Maxima Suppression) for predicted label {label}...')
  cv2_imshow(clone)
  clone = orig.copy()

  '''
  Our loop over the labels for each of the detected objects begins on Line 139.

  We make a copy of the original input image so that we can annotate it (Line 142).

  We then annotate all bounding boxes for the current label (Lines 145-149).

  So that we can visualize the before/after applying NMS, Line 154 displays the “before” image,
  and then we proceed to make another copy (Line 155).

  Now, let’s apply NMS and display our “after” NMS visualization:

  '''


  # extract the bounding boxes and associated prediction
	# probabilities, then apply non-maxima suppression
  boxes = np.array([p[0] for p in pred_labels[label]])
  proba = np.array([p[1] for p in pred_labels[label]])
  boxes = non_max_suppression(boxes, proba)
  # loop over all bounding boxes that were kept after applying
	# non-maxima suppression
  for (startX, startY, endX, endY) in boxes:
    print(f'[DJSc_test_test] boxes length is {len(boxes)}')
    # draw the bounding box and label on the image
    cv2.rectangle(clone, (startX, startY), (endX, endY),
			(0, 255, 0), 2)
    y = startY - 10 if startY - 10 > 10 else startY + 10
    #y = startY - 5 if startY - 5 > 5 else startY + 5
    cv2.putText(clone, label, (startX, y),cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 255, 0), 2)
	# show the output after apply non-maxima suppression
  #cv2.imshow("After", clone)
  print('[DJSc_nms] After NMS (Non-Maxima Suppression)...')
  cv2_imshow(clone)
  #cv2.waitKey(0)

    #GroundTruth (gT) COLLECTION (#2025Apr28-30)
  if label==mapTestXto_ALLtestSC[img4jay_path]:
    gT=groundTruth.get(img4jay_path, [])
    gT.append((1,boxes,to_categorical(mapTestXto_ALLy[img4jay_path],7))) #2025Apr30
    groundTruth[img4jay_path]=gT
    print(f'[DJSc_test] groundTruth[{img4jay_path}] with <label> {label} is {groundTruth[img4jay_path]} ') #2025Apr30

classJay

"""####**Good TestCase** (paths Manual Collection) pMC

model name is... JaybaselineCNN_W32cL10_224VGG
img4jay path is... ./IntelClassImg_GoodOne/NonFace_seg_test/seg_test/buildings/20601.jpg
img4jay class is... 6
img4jay label is... buildings
[DJSc_test](H,W) is (400, 400)
pyramind4jay is... <generator object image_pyramid at 0x7fdca99626b0>
roi shape is (224, 224, 3)
pMC=['./IntelClassImg_GoodOne/NonFace_seg_test/seg_test/buildings/20601.jpg',
  './thumbnails128x128/07665.png',  
      ]

*False Positives:*
./IntelClassImg_GoodOne/NonFace_seg_test/seg_test/street/20642.jpg

"""

#img4jay_path

groundTruth.keys()

if label==mapTestXto_ALLtestSC[img4jay_path]:
  print(f'groundTruth[{img4jay_path}] is ... {groundTruth[img4jay_path]}')
  print(f'groundTruth[{img4jay_path}][0] with <label> {label} is {groundTruth[img4jay_path][0]} ')
  print(f'groundTruth[{img4jay_path}][0][0] as gTprob with <label> {label} is {groundTruth[img4jay_path][0][0]} ')
  print(f'groundTruth[{img4jay_path}][0][1][0] as gTloc with <label> {label} is {groundTruth[img4jay_path][0][1][0]} ')
  print(f'groundTruth[{img4jay_path}][0][2] as gTtocateg_kls with <label> {label} is {groundTruth[img4jay_path][0][2]} ')

#gTprob=groundTruth[img4jay_path][0][0]
#gTprob

#gTloc=groundTruth[img4jay_path][0][1][0]
#gTloc

#gTloc[0]

#gTtocateg_kls=groundTruth[img4jay_path][0][2]
#gTtocateg_kls

#kls=np.argmax(gTtocateg_kls)
#kls

#kls_name=classJay[kls]
#kls_name

"""###GroundTruth MASS Collection (from DJSc_2025Apr29)
GroundTruth COLLECTOR

`groundTruth_mass={}`

Checking element in list of dictionaries
https://www.geeksforgeeks.org/python-check-list-elements-from-dictionary-list/
```
def check_ele(ele, test_list):

    for sub in test_list:
        for item in sub.values():
            if ele == item:
                return True
    return False
  ```
"""

def check_ele(ele, test_list):

    for sub in test_list:
      #for item in sub.values():
        #if ele == item:
      if ele in sub.keys():
          return [True, ele]
    return [False,ele]

test_list = [{'Name' : 'Apple', 'Price' : 18, 'Color' : 'Red'},
             {'Name' : 'Mango', 'Price' : 20, 'Color' : 'Yellow'},
             {'Name' : 'Orange', 'Price' : 24, 'Color' : 'Orange'},
             {'Name' : 'Plum', 'Price' : 28, 'Color' : 'Red'}]

check_ele('Apple', test_list)

check_ele('Name', test_list)

check_ele('Name', test_list)[0]

check_ele('Name', test_list)[1]

for sub in test_list:
  print(sub)

for sub in test_list:
  print(sub.items())

for sub in test_list:
  print(sub.keys())

"""...CRASH Alert...2025May3rd

`mass_size=int(input('Please input size of Mass collection: '))`

!!!

`mass_size above 50 (epoch>15)`... causes crash at T4 (SysRAM=12.7GB & GPU_RAM=15.0GB)

!!!!...Try `mass_size=30 (epoch=20)`...no crash...

!!!!...Try `mass_size=35 (epoch=20)`...



"""

# for MASS Collection from...DJSc_model

# initialize variables used for the object detection procedure
#WIDTH = 600
WIDTH = 300
PYR_SCALE = 1.5
WIN_STEP = 16
#ROI_SIZE = eval(args["size"])
ROI_SIZE = (224,224)
INPUT_SIZE = (224, 224)
model=DJSc_model

# initialize our list of output images
images_Matrix = []
#jayAdd for evaluation metrics
testYtrack=[]
predsTrack=[]
#failure=0

#Map testX to testALL_y
mapTestXto_ALLy=dict(zip(test_ALLdataJay,testALL_y))
mapTestXto_ALLtestSC=dict(zip(test_ALLdataJay,ALLtest_SubCateg))

#Reminder model = ResNet50(weights="imagenet", include_top=True)
print(f'model name is... {model.name}')

groundTruth={}
#GroundTruth MASS Collector DJSc_2025May1st
groundTruth_mass=[]
mass_size=int(input('Please input size of Mass collection: '))
print('mass_size is ... ',mass_size)

repeatedPath=[]

for enum in range(mass_size):
  # load the input image from disk, resize it such that it has the
  # has the supplied width, and then grab its dimensions
  rand=np.random.randint(0,len(test_ALLdataJay))
  img4jay_path=test_ALLdataJay[rand]

  #print('[DJSc_test] groundTruth_mass length is...', len(groundTruth_mass))
  #print('[DJSc_test] groundTruth_mass items are...', groundTruth_mass)

  #to aavoid path repetition
  if bool(check_ele(img4jay_path,groundTruth_mass)[0]):
    print(f'[DJSc_test_INFO] img4jay_path already in groundTruth_mass as... {check_ele(img4jay_path,groundTruth_mass)[1]}')
    print(f'[DJSc_test_INFO] img4jay class is... {mapTestXto_ALLy[img4jay_path]}')
    repeatedPath.append(img4jay_path)
    imgRepeated=cv2.imread(img4jay_path)
    cv2_imshow(imgRepeated)

    continue
  #img4jay_path=np.random.random_(mapTestXto_ALLy.keys())
  print(f'img4jay path is... {img4jay_path}')
  print(f'img4jay class is... {mapTestXto_ALLy[img4jay_path]}')
  print(f'img4jay label is... {mapTestXto_ALLtestSC[img4jay_path]}')
  orig = cv2.imread(img4jay_path)
  #deactivate (comment) below as (maybe) source of error with <rois> list...
  #... #... AttributeError: 'numpy.ndarray' object has no attribute 'append'
  #orig = imutils.resize(orig, width=WIDTH)
  #(H, W) = orig.shape[:2]

  orig=cv2.resize(orig, (400,400)) #...!!!,,,Crucial resize to be above (224,224) and avoid error
  #print(f'[DJSc_test] rsz_orig shape is... {orig.shape}') #DJSc_test

  #2025Apr23rd
  (H, W) = orig.shape[:2]
  print(f'[DJSc_test](H,W) is {H,W}')

  # initialize the image pyramid
  #pyramid = image_pyramid(orig, scale=PYR_SCALE, minSize=ROI_SIZE)# avoid ROI_size...!!!
  pyramid4jay = image_pyramid(orig)
  # initialize two lists, one to hold the ROIs generated from the image
  # pyramid and sliding window, and another list used to store the
  # (x, y)-coordinates of where the ROI was in the original image

  rois=[]
  #rois = list([]) ## list Reinforcement is CRUCIAL to avoid error...
  #... AttributeError: 'numpy.ndarray' object has no attribute 'append'
  locs = []
  # time how long it takes to loop over the image pyramid layers and
  # sliding window locations
  start = time.time()
  print(f'pyramind4jay is... {pyramid4jay}')
  for img in pyramid4jay:
    count=1 #DJSc_count
    scale=W / float(img.shape[1]) #2025Apr23rd...jW for W

    for (x, y, roiOrig) in sliding_window(img, WIN_STEP, ROI_SIZE):
      #Uncomment when ready...
      if count>6:
        break
        #continue

      # scale the (x, y)-coordinates of the ROI with respect to the
      #*original* image dimensions
      x = int(x * scale)
      y = int(y * scale)
      w = int(ROI_SIZE[0] * scale)
      h = int(ROI_SIZE[1] * scale)
      # take the ROI and preprocess it so we can later classify
      # the region using Keras/TensorFlow
      roi = cv2.resize(roiOrig, INPUT_SIZE)
      print(f'roi shape is {roi.shape}')
      roi = img_to_array(roi) ##
      #roi = preprocess_input(roi)
      #prep_roi=prepaReshape(roi) #maybe Not required
      # classify the
      #print(f'[DJSc_test]count_{count} roi shape is {roi.shape}')
      #print(f'[DJSc_test]count_{count} prep_roi shape is {prep_roi.shape}')
      # update our list of ROIs and associated coordinates

      rois.append(roi) #NOT appending...!!!

      #print(f'[DJSc_test]count_{count} shape list rois is {rois.shape}') #DJSc_test
      #list(rois).append(roi) #DJSc_debug...reinforced rois as list at declaration
      #print(f'[DJSc_test]count_{count} length list rois is {len(list(rois))}') #DJSc_test
      locs.append((x, y, x + w, y + h))
      #print(f'[DJSc_test]count_{count} length locs is {len(locs)}') #DJSc_test

      #Handling optional visualisation
      visual=1

      # check to see if we are visualizing each of the sliding
      # windows in the image pyramid
      #if args["visualize"] > 0:
      if visual > 0:
        # clone the original image and then draw a bounding box
	      # surrounding the current region
        clone = orig.copy()
        cv2.rectangle(clone, (x, y), (x + w, y + h), (0, 255, 0), 2)
        #show the visualization and current ROI

        #Uncomment if needed
        #cv2_imshow(clone)
        #cv2_imshow(roiOrig)

        count+=1
        #cv2.waitKey(0) #ERROR
        #error: OpenCV(4.11.0) /io/opencv/modules/highgui/src/window.cpp:1367: error: (-2:Unspecified error)
        #The function is not implemented. Rebuild the library with Windows, GTK+ 2.x or Cocoa support.
        #If you are on Ubuntu or Debian, install libgtk2.0-dev and pkg-config, then re-run cmake or configure script in function 'cvWaitKey'

        #Above, we visualize both the original image with a green box indicating where we are “looking” and the resized ROI, which is ready for classification (Lines 85-95).
        #As you can see, we’ll only --visualize when the flag is set via the command line.

    print(f'rois array with <roi> (before DJSc_test) length is {len(rois)}') #DJSc_test

    if len(rois)==0:
      print(f'rois array with <roi> is EMPTY as length is {len(rois)}') #DJSc_test
      #break
      continue

    #Next, we’ll (1) check our benchmark on the pyramid + sliding window process,
      #(2) classify all of our rois in batch, and (3) decode predictions:

    # show how long it took to loop over the image pyramid layers and
  # sliding window locations
  end = time.time()
  print("[INFO] looping over pyramid/windows took {:.5f} seconds".format(end - start))
  # convert the ROIs to a NumPy array
  rois = np.array(rois, dtype="float32")


  # classify each of the proposal ROIs using ResNet and then show how
  # long the classifications took

  print("[INFO] classifying ROIs...")
  #print(f'rois array with <roi> length is {len(rois)}') #DJSc_test
  #print(f'rois array with <roi> shape is {rois.shape}') #DJSc_test

  start = time.time()
  #DJSc_model specific...!!!
  #preds = model.predict(rois)
  j_rois=rois#2025Apr23rd Debug...
  preds=model.predict_on_batch(j_rois)
  # loop over the image pyramid4jay
  # strangely non-persistent pyramid objet as yielded items disappear...to have empty list after few seconds
  end = time.time()
  print("[INFO] classifying ROIs took {:.5f} seconds".format(	end - start))
  # decode the predictions and initialize a dictionary which maps class
  # labels (keys) to any ROIs associated with that label (values)

  print(f'preds array looks like... {preds}')

  #Finally, Line 117 below decodes the predictions, grabbing only the top prediction for each ROI.
  #preds = imagenet_utils.decode_predictions(preds, top=1)
  predictions=np.argmax(preds, axis=-1)
  print(f'[DJSc_check] predictions length is... {len(predictions)}')
  print(f'[DJSc_check] predictions array looks like... {predictions}')
  probs=np.max(preds, axis=-1)
  print(f'probs length is... {len(probs)}')
  print(f'probs array looks like... {probs}')

  labels = {}
  pred_labels = {}

  print(predictions.dtype)
    # loop over the predictions
    #2025Apr23th_FIX...

  # loop over the predictions
  #for (i, p) in enumerate(preds):
  for loc, i, p in zip(locs,predictions, probs):
    # grab the prediction information for the current ROI
    #(imagenetID, label, prob) = p[0]
    print(f'loc is {loc}')
    #print(f'[DJSc_test] i is {i}')
    prob= p
    #print(f'[DJSc_test] prob is {p}')
    labels[i]=classJay[i]
    #print(f'[DJSc_test] pred_labels[{i}] is ...{pred_labels[i]}')
    # filter out weak detections by ensuring the predicted probability
    # is greater than the minimum probability
    #if prob >= args["min_conf"]:
    if prob >= 0.4:
      # grab the bounding box associated with the prediction and
      # convert the coordinates
      box = loc
      # grab the list of predictions for the label and add the
      # bounding box and probability to the list
      #L = labels.get(label, [])
      L = pred_labels.get(labels[i], [])
      L.append((box, prob))
      #labels[label] = L
      pred_labels[labels[i]] = L
      #print(f'[DJSc_test] pred_labels[pred_labels[{i}]] is ...{L}')
      print(f'[DJSc_test_test] pred_labels[{labels[i]}] is ...{pred_labels[labels[i]]}')
      #print(f'[DJSc_test] pred_labels is {pred_labels}')

  # loop over the labels for each of detected objects in the image #L139
  for label in pred_labels.keys():
    # clone the original image so that we can draw on it
    print("[INFO] showing results for predicted label '{}'".format(label))
    clone = orig.copy()

    # loop over all bounding boxes for the current label
    for (box, prob) in pred_labels[label]:
      # draw the bounding box on the image
      (startX, startY, endX, endY) = box
      cv2.rectangle(clone, (startX, startY), (endX, endY), (0, 255, 0), 2)
    # show the results *before* applying non-maxima suppression, then
    # clone the image again so we can display the results *after*
    # applying non-maxima suppression
    #cv2.imshow("Before", clone)

    print(f'Before NMS (Non-Maxima Suppression) for predicted label {label}...')
    cv2_imshow(clone) # Uncomment if needed...

    clone = orig.copy()

    '''
    Our loop over the labels for each of the detected objects begins on Line 139.

    We make a copy of the original input image so that we can annotate it (Line 142).

    We then annotate all bounding boxes for the current label (Lines 145-149).

    So that we can visualize the before/after applying NMS, Line 154 displays the “before” image,
    and then we proceed to make another copy (Line 155).

    Now, let’s apply NMS and display our “after” NMS visualization:

    '''


    # extract the bounding boxes and associated prediction
    # probabilities, then apply non-maxima suppression
    boxes = np.array([p[0] for p in pred_labels[label]])
    proba = np.array([p[1] for p in pred_labels[label]])
    boxes = non_max_suppression(boxes, proba)
    # loop over all bounding boxes that were kept after applying
    # non-maxima suppression
    for (startX, startY, endX, endY) in boxes:
      print(f'[DJSc_test_test] boxes length is {len(boxes)}')

      if label!=mapTestXto_ALLtestSC[img4jay_path]:
          # draw the bounding box and label on the image
        print(f'clone shape is...{clone.shape}') #to compare scale in show_predictDJSc
        cv2.rectangle(clone, (startX, startY), (endX, endY),
          (0, 0, 255), 2)
        y = startY - 10 if startY - 10 > 10 else startY + 10
        #y = startY - 5 if startY - 5 > 5 else startY + 5
        cv2.putText(clone, label, (startX, y),cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 0, 255), 2)
        continue

      # draw the bounding box and label on the image
      cv2.rectangle(clone, (startX, startY), (endX, endY),
        (0, 255, 0), 2)
      y = startY - 10 if startY - 10 > 10 else startY + 10
      #y = startY - 5 if startY - 5 > 5 else startY + 5
      cv2.putText(clone, label, (startX, y),cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 255, 0), 2)
    # show the output after apply non-maxima suppression
    #cv2.imshow("After", clone)

    print('[DJSc_nms] After NMS (Non-Maxima Suppression)...')
    cv2_imshow(clone)
    #cv2.waitKey(0)

      #GroundTruth (gT) Single Collection (#2025Apr28-30)
    if label==mapTestXto_ALLtestSC[img4jay_path]:
      gT=groundTruth.get(img4jay_path, [])
      gT.append((1,boxes,to_categorical(mapTestXto_ALLy[img4jay_path],7),label)) #2025Apr30 & May1st
      groundTruth[img4jay_path]=gT
      print(f'[DJSc_test] groundTruth[{img4jay_path}] with <label> {label} is {groundTruth[img4jay_path]} ') #2025Apr30

      #GroundTruth MASS Collection 2025May1st
      groundTruth_mass.append({img4jay_path:(groundTruth[img4jay_path])})
      #groundTruth_mass.append({img4jay_path:(groundTruth[img4jay_path], label)})# label difficult to retrieve from y_true
      print(f'[DJSc_test] groundTruth_mass length is {len(groundTruth_mass)}')
      print(f'[DJSc_test] groundTruth_mass is {groundTruth_mass}')
      print('    ##########################   ')

print(f'[DJSc_test] completed groundTruth_mass length is {len(groundTruth_mass)}')
print(f'[DJSc_test] completed groundTruth_mass is {groundTruth_mass}') #to return
print(f'[DJSc_test] mass_size is {mass_size}')
print(f'[DJSc_test_INFO] lenght of repated path is {len(repeatedPath)}' )
print(f'[DJSc_test_INFO] repeated path is {repeatedPath}' )
print('[DJSc_test] Does mass_size equal to groundTruth_mass ?... ', mass_size==len(groundTruth_mass))

"""!!!!...Try `mass_size=30 (epoch=20)`...no crash...

!!!!...Try `mass_size=35 (epoch=20)`...
"""

groundTruth_mass

groundTruth_mass[0]

groundTruth_mass[0].items()
#same as groundTruth_mass[0].values()

groundTruth_mass[0].keys()

"""###SPECIFICITY of `DJSc_cls7W36L22model` with 7 classes
1. `to_categorical` transformation may Not be required as
`channels[5 + kls] = 1.0`  map 1.0 position in `to_categorical` array to channel
2. `def make_numbers(X, y):` Not needed...
3. `def make_data(size=64):` Not needed...
4. `def get_color_by_probability(p):` & `def show_predict(X, y, threshold=0.1):` are Not needed... as we will use local method `view_predictions`.

##Make Model

ValueError REMEDY
```
ValueError: The `{arg_name}` of this `Lambda` layer is a Python lambda. Deserializing it is unsafe. If you trust the source of the config artifact, you can override this error by passing `safe_mode=False` to `from_config()`, or calling `keras.config.enable_unsafe_deserialization().

```

```
import tensorflow as tf
import keras
from keras.layers import Input, Lambda
from keras.models import Model

# Define the Lambda function separately
def my_lambda_function(x):
    # Your lambda function logic here
    return x * 2

# Create the model
inputs = Input(shape=(10,))
outputs = Lambda(my_lambda_function)(inputs)
box_model = Model(inputs=inputs, outputs=outputs)

# Save the model using SavedModel format
box_model.save('./box_model')

# ... later, load the model
loaded_model = tf.keras.models.load_model('./box_model')

```
"""

#import tensorflow as tf
#import keras
from keras.layers import Input, Lambda
#from keras.models import Model

# Define the Lambda function separately
def my_lambda_function(x):
    gate=tf.where(x > 0.5, tf.ones_like(x), tf.zeros_like(x))
    return gate


#lambda x: tf.where(x > 0.5, tf.ones_like(x), tf.zeros_like(x))

"""###Model Structure"""

'''
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.layers import BatchNormalization

from keras.layers import Input, Lambda
# Define the Lambda function separately

#REMINDER
#lambda x: tf.where(x > 0.5, tf.ones_like(x), tf.zeros_like(x))
#Jay_CNNcls7W36L22_128VGGnet:
   #Total params: 41,996 (164.05 KB)
   #Trainable params: 41,676 (162.80 KB)
   #Non-trainable params: 320 (1.25 KB)
   #len(model.weights)...36
   #len(model.layers)...23

inputs = layers.Input(shape=(128, 128, 3))

def my_lambda_function(x):
    #CONVOLUTION Block_1st
    #2 layers => Max-pooling (RELU => stride 2 layers)
  #x_input=model.add(keras.layers.Input(shape=(128,128,3)))

  x = layers.Conv2D(32, kernel_size=3, padding='same', activation='relu')(x)
  x = layers.MaxPool2D()(x)
  x = layers.BatchNormalization()(x) # size: 64x64

  #CONVOLUTION Block_2nd
    # 2 layers => Max-pooling (RELU => stride 2 layers)
  x = layers.Conv2D(32, kernel_size=3, padding='same', activation='relu')(x)
  x = layers.BatchNormalization()(x)  # size: 64x64

  #CONVOLUTION Block_3rd
    # 2 layers => Max-pooling (RELU => stride 2 layers)
  x = layers.Conv2D(32, kernel_size=3, padding='same', activation='relu')(x)
  x = layers.MaxPool2D()(x)
  x = layers.BatchNormalization()(x)  # size: 32x32

  #CONVOLUTION Block_4th
    # 2 layers => Max-pooling (RELU => stride 2 layers)
  x = layers.Conv2D(32, kernel_size=3, padding='same', activation='relu')(x)
  x = layers.MaxPool2D()(x)
  x = layers.BatchNormalization()(x)  # size: 16x16

  #CONVOLUTION Block_5th
    # 2 layers => Max-pooling (RELU => stride 2 layers)
    #inputs=keras.Input(shape=(14,14,512))
  x = layers.Conv2D(32, kernel_size=3, padding='same', activation='relu')(x)
  x = layers.BatchNormalization()(x) # size: 8x8x
  x = layers.MaxPool2D()(x)  # maxpooling  layer of block to be flattened

  x=layers.Flatten()(x) # the output of the preceding MaxPooling2D  layer and flatten it into a single vector.
  #x=layers.Dense(512, activation='relu')(x)
  #x=layers.Dropout(0.5)(x)
  x_prob = layers.Dense(1, activation='sigmoid', name='x_prob')(x)
  x_boxes = layers.Dense(4, name='x_boxes')(x)
  x_cls = layers.Dense(7, activation='sigmoid', name='x_cls')(x)
  #7 for 7 classes

  # Wrap the TensorFlow operation `tf.where` inside a Lambda layer.
  # This will ensure the operation is executed within the Keras symbolic graph.

  x_prob_gate = layers.Lambda(lambda x: tf.where(x > 0.5, tf.ones_like(x), tf.zeros_like(x)))(x_prob)
  #x_prob_gate = layers.Lambda(my_lambda_function)(x_prob)

  x_boxes =keras.layers.Multiply()([x_boxes, x_prob_gate])
  x_cls = layers.Multiply()([x_cls, x_prob_gate])
  # ---

  x_lambda = layers.Concatenate()([x_prob, x_boxes, x_cls])

  return x_lambda



outputs=layers.Lambda(my_lambda_function)(inputs)
box_model = tf.keras.models.Model(inputs=inputs, outputs=outputs, name='lambda_JayCNNcls7W36L23_128VGGnet')
box_model.summary()


#box_model = tf.keras.models.Model(x_input, x, name='JayCNNcls7W36L23_128VGGnet')
#box_model = tf.keras.models.Model(x_input, x, name='JayCNNcls7W36L23_128VGGnet')
#box_model.summary()

'''

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.layers import BatchNormalization

from keras.layers import Input, Lambda
'''
# Define the Lambda function separately
def my_lambda_function(x):
    gate=tf.where(x > 0.5, tf.ones_like(x), tf.zeros_like(x))
    return gate
#REMINDER
#lambda x: tf.where(x > 0.5, tf.ones_like(x), tf.zeros_like(x))
#Jay_CNNcls7W36L22_128VGGnet:
   #Total params: 41,996 (164.05 KB)
   #Trainable params: 41,676 (162.80 KB)
   #Non-trainable params: 320 (1.25 KB)
   #len(model.weights)...36
   #len(model.layers)...22
'''

#CONVOLUTION Block_1st
  #2 layers => Max-pooling (RELU => stride 2 layers)
#x_input=model.add(keras.layers.Input(shape=(128,128,3)))
x_input = layers.Input(shape=(128, 128, 3))


x = layers.Conv2D(32, kernel_size=3, padding='same', activation='relu')(x_input)
x = layers.MaxPool2D()(x)
x = layers.BatchNormalization()(x) # size: 64x64

 #CONVOLUTION Block_2nd
  # 2 layers => Max-pooling (RELU => stride 2 layers)
x = layers.Conv2D(32, kernel_size=3, padding='same', activation='relu')(x)
x = layers.BatchNormalization()(x)  # size: 64x64

#CONVOLUTION Block_3rd
  # 2 layers => Max-pooling (RELU => stride 2 layers)
x = layers.Conv2D(32, kernel_size=3, padding='same', activation='relu')(x)
x = layers.MaxPool2D()(x)
x = layers.BatchNormalization()(x)  # size: 32x32

#CONVOLUTION Block_4th
  # 2 layers => Max-pooling (RELU => stride 2 layers)
x = layers.Conv2D(32, kernel_size=3, padding='same', activation='relu')(x)
x = layers.MaxPool2D()(x)
x = layers.BatchNormalization()(x)  # size: 16x16

#CONVOLUTION Block_5th
  # 2 layers => Max-pooling (RELU => stride 2 layers)
  #inputs=keras.Input(shape=(14,14,512))
x = layers.Conv2D(32, kernel_size=3, padding='same', activation='relu')(x)
x = layers.BatchNormalization()(x) # size: 8x8x
x = layers.MaxPool2D()(x)  # maxpooling  layer of block to be flattened

x=layers.Flatten()(x) # the output of the preceding MaxPooling2D  layer and flatten it into a single vector.
#x=layers.Dense(512, activation='relu')(x)
#x=layers.Dropout(0.5)(x)
x_prob = layers.Dense(1, activation='sigmoid', name='x_prob')(x)
x_boxes = layers.Dense(4, name='x_boxes')(x)
x_cls = layers.Dense(7, activation='sigmoid', name='x_cls')(x)
#7 for 7 classes

# Wrap the TensorFlow operation `tf.where` inside a Lambda layer.
# This will ensure the operation is executed within the Keras symbolic graph.

#x_prob_gate = layers.Lambda(lambda x: tf.where(x > 0.5, tf.ones_like(x), tf.zeros_like(x)))(x_prob)
#x_prob_gate = layers.Lambda(my_lambda_function)(x_prob)

#x_boxes =keras.layers.Multiply()([x_boxes, x_prob_gate])
#x_cls = layers.Multiply()([x_cls, x_prob_gate])
# ---

x = layers.Concatenate()([x_prob, x_boxes, x_cls])

#box_model = tf.keras.models.Model(x_input, x, name='JayCNNcls7W36L23_128VGGnet')
box_model = tf.keras.models.Model(x_input, x, name='JayCNNcls7W36L23_128VGGnet')
box_model.summary()

"""len(model.weights)...36

len(model.layers)...23
"""

box_model.name

len(box_model.weights) #, model.weights

len(box_model.layers)

"""Here are the differences from traditional CNN.
we need:

1. probability of object exists ( x_prob )
2.bounding box position ( four channels, x_boxes )
3.classification of the digit. ( ten channels, x_cls )

So here are the codes:
```
x_prob_gate = layers.Lambda(lambda x: tf.where(x > 0.5, tf.ones_like(x), tf.zeros_like(x)))(x_prob)
x_boxes = layers.Multiply()([x_boxes, x_prob_gate])  
x_cls = layers.Multiply()([x_cls, x_prob_gate])

```
and here are the points:

1. we use sigmoid to do probability channel output, so its range keep in 0~1
2.the bounding boxes channels, like linear regression, don’t use any activation function
3.Classification channels are also using sigmoid to make probability predictions.

###The gate of the output.
This code is super important.

we don’t want backward propagation or wrong object output that doesn’t exist.
So we do something like “gate” for bounding boxes channels and classification channels.

When the probability is less than 0.5, the gate will be 0 otherwise is 1.
```
gate = tf.where(x_prob > 0.5, tf.ones_like(x_prob), tf.zeros_like(x_prob))
x_boxes = x_boxes * gate
x_cls = x_cls * gate
```
This code will output zero and stop gradient of low probability area.

Then combine the outputs and the model.
```
x = layers.Concatenate()([x_prob, x_boxes, x_cls])
model = tf.keras.models.Model(x_input, x)
model.summary()
```

###Loss functions & `compile`

CRUCIAL function decoration with `@tf.....`
"""

#RUCIAL function decoration with `@tf.....`
@tf.keras.utils.register_keras_serializable()
def djsc_loss(y_true, y_pred):

  idx_p = [0]
  idx_bb = [1, 2, 3, 4]
  idx_cls = [5, 6, 7, 8, 9, 10, 11]

  @tf.function
  def loss_bb(y_true, y_pred):
      y_true = tf.gather(y_true, idx_bb, axis=-1)
      y_pred = tf.gather(y_pred, idx_bb, axis=-1)

      #loss = tf.keras.losses.mean_squared_error(y_true, y_pred)
      #AttributeError: module 'keras._tf_keras.keras.losses' has no attribute 'mean_squared_error'

      #Gr8 WALL...toFIX...!!!
      print(f'[DJSc] y_true shape is {y_true.shape}')
      print(f'[DJSc] 2nd item y_true[1] shape is {y_true[1].shape}')
      print(f'[DJSc] 2nd item y_true[1] length is {len(y_true[1])}')
      print(f'[DJSc] 2nd item y_true[1] is {y_true[1]}')

      print(f'[DJSc] y_pred shape is {y_pred.shape}')

      loss=tf.keras.losses.mse(y_true, y_pred)
      print(f'[DJSc] loss shape is {loss.shape}')
      print(f'[DJSc] loss is {loss}')

      #loss = tf.keras.losses.MeanSquaredError()(y_true, y_pred)
      '''
      The error message "ValueError: mask cannot be scalar" indicates that the tf.boolean_mask
      function in the loss_bb function is receiving a scalar value for the mask argument,
      when it expects a tensor with at least one dimension. This is happening because the loss variable,
      calculated using tf.keras.losses.MeanSquaredError()(y_true, y_pred), is a scalar representing the mean squared
      error across all elements of the tensors. Subsequently,
      the mask variable, calculated as loss > 0.0, is also a scalar (True or False).
      '''
      #...EUREKA...!!!...I have been using the wrong `MeanSquaredError`....
      #loss = tf.keras.losses.MeanSquaredError()(y_true, y_pred)[lambda x: tf.where(x>0.0, loss[x])]
      #print(f'[DJSc] loss is {loss} ')
      #print(f'[DJSc] loss length are {len(loss)} ')
      #print(f'[DJSc] loss shape is {loss.shape}')
      #condition=loss[:,0]>0.0
      mask=loss>0.0
      #print(f'[DJSc] condition is {condition} ')
      print(f'[DJSc] mask is {mask} ')
      #lossWhere=tf.boolean_mask(loss, condition)
      lossWhere=tf.boolean_mask(loss, mask)
      #lossWhere=tf.boolean_mask(loss, loss>0.0)
      print(f'[DJSc] lossWhere is {lossWhere}')
      #print(f'[DJSc] x is {x} & lossWhere is {lossWhere} ')
      #cast_lossWhere= tf.dtypes.cast(lossWhere, tf.float32)
      #print(f'[DJSc] cast_lossWhere is {cast_lossWhere} ')
      redM_lossWhere= tf.reduce_mean(lossWhere)
      #redM_cast_lossWhere= tf.reduce_mean(cast_lossWhere)
      print(f'[DJSc] tf.reduce_mean(cast_lossWhere) is {redM_lossWhere} ')
      #print(f'[DJSc] tf.reduce_mean(cast_lossWhere) is {redM_cast_lossWhere} ')
      return redM_lossWhere
      #return redM_cast_lossWhere
      #return tf.reduce_mean(tf.cast((lambda x: tf.where(x>0.0, loss[x])), tf.float32))
      #return tf.reduce_mean(loss[loss > 0.0])
      #return tf.reduce_mean(loss)

  @tf.function
  def loss_p(y_true, y_pred):
      y_true = tf.gather(y_true, idx_p, axis=-1)
      y_pred = tf.gather(y_pred, idx_p, axis=-1)

      loss = tf.losses.binary_crossentropy(y_true, y_pred)
      return tf.reduce_sum(loss)

  @tf.function
  def loss_cls(y_true, y_pred):
      y_true = tf.gather(y_true, idx_cls, axis=-1)
      y_pred = tf.gather(y_pred, idx_cls, axis=-1)

      loss = tf.losses.binary_crossentropy(y_true, y_pred)
      return tf.reduce_sum(loss)

  @tf.function
  def loss_func(y_true, y_pred):
      return loss_bb(y_true, y_pred) + loss_p(y_true, y_pred) + loss_cls(y_true, y_pred)


  loss_func.get_config = lambda: {}  # Returns an empty config for simplicity

  @classmethod
  def from_config(cls, config):
      return cls # Assuming no configuration is needed for reconstruction

  loss_func.from_config = from_config

  return loss_func(y_true, y_pred)


opt = tf.keras.optimizers.Adam(learning_rate=0.003)
#box_model.compile(loss=loss_func, optimizer=opt)
box_model.compile(loss=djsc_loss, optimizer=opt)

"""##Preview model prediction

###View single prediction
"""

#groundTruth_mass #uncomment when necessary

#cv2.imread('./thumbnails128x128/55821.png')

groundTruth_mass[0]

len(groundTruth_mass)

"""CRUCIAL Unpacking"""

keys=[]
probas=[]
locs=[]
clss=[]
clss_name=[]
for sub in groundTruth_mass:
  #print(sub)
  for k,v in sub.items():
    print(k,v)
    #print(k)
    prob=sub[k][0][0]
    #print(prob)
    loc=sub[k][0][1]
    #print(loc)
    cls=sub[k][0][2]
    #print(cls)
    cls_n=sub[k][0][3]
    #print(cls_n)
    #filling lists
    keys.append(k)
    probas.append(prob)
    locs.append(loc)
    clss.append(cls)
    clss_name.append(cls_n)

y_trueGround={key:values for key, values in zip(keys, zip(probas, locs, clss, clss_name))}
#y_trueGround

len(keys), len(probas), len(locs), len(clss), len(clss_name)

"""Unpacking for y_true"""

probas[0] #1st degree fetch

locs[0]

locs[0][0][0], locs[0][0][1], locs[0][0][2], locs[0][0][3] #3rd degree fetch

clss[0][0], clss[0][1], clss[0][2], clss[0][3], clss[0][4], clss[0][5], clss[0][6] #2nd degree fetch

keys=[]
probas=[]
locs=[]
clss=[]
clss_name=[]
for sub in groundTruth_mass:
  #print(sub)
  for k,v in sub.items():
    #print(k,v)
    #print(k)
    prob=sub[k][0][0]
    #print(prob)
    loc=sub[k][0][1]
    #print(loc)
    cls=sub[k][0][2]
    #print(cls)
    cls_n=sub[k][0][3]
    #print(cls_n)
    #filling lists
    keys.append(k)
    probas.append(prob)
    locs.append(loc)
    clss.append(cls)
    clss_name.append(cls_n)


print(len(keys), len(probas), len(locs), len(clss), len(clss_name))

keys_true=keys

#listing the 12 channels
probas_true=[]
x1s_true=[]
y1s_true=[]
x2s_true=[]
y2s_true=[]
kls1s_true=[]
kls2s_true=[]
kls3s_true=[]
kls4s_true=[]
kls5s_true=[]
kls6s_true=[]
kls7s_true=[]

clss_name=[]
for i in range(len(keys_true)):
  prob=probas[i]
  #print(prob)
  x1=locs[i][0][0]
  #print(x1)
  y1=locs[i][0][1]
  #print(y1)
  x2=locs[i][0][2]
  #print(x2)
  y2=locs[i][0][3]
  #print(y2)
  kls1=clss[i][0]
  #print(kls1)
  kls2=clss[i][1]
  #print(kls2)
  kls3=clss[i][2]
  #print(kls3)
  kls4=clss[i][3]
  #print(kls4)
  kls5=clss[i][4]
  #print(kls5)
  kls6=clss[i][5]
  #print(kls6)
  kls7=clss[i][6]
  #print(kls7)

  #filling lists
  probas_true.append(prob)
  x1s_true.append(x1)
  y1s_true.append(y1)
  x2s_true.append(x2)
  y2s_true.append(y2)
  kls1s_true.append(kls1)
  kls2s_true.append(kls2)
  kls3s_true.append(kls3)
  kls4s_true.append(kls4)
  kls5s_true.append(kls5)
  kls6s_true.append(kls6)
  kls7s_true.append(kls7)

channels=list(zip(probas_true,x1s_true,y1s_true,x2s_true,y2s_true,kls1s_true,kls2s_true,kls3s_true,kls4s_true,kls5s_true,kls6s_true,kls7s_true ))
channels[:2]

len(keys_true), len(channels)

y_true={key:values for key, values in zip(keys_true, channels)}
#y_true

len(y_true)

y_true.keys()

randKey=np.random.choice(list(y_true.keys()))
randKey

pathSingle=randKey
pathSingle

y_trueSingle=y_true[randKey]
y_trueSingle

#pathSingle='./thumbnails128x128/07665.png'
#pathSingle=img4jay_path
imgMatrix=cv2.imread(pathSingle)
print(imgMatrix.shape)
cv2_imshow(imgMatrix)

imgMatrix_rsz=cv2.resize(imgMatrix, (128,128))
#imgMatrix.resize(128,128,3)

width=imgMatrix_rsz.shape[0]
width

height=imgMatrix_rsz.shape[1]
height

imgMatrix_rsz.shape

#...ERROR to expect...!!!
#y_predicting=box_model.predict(imgMatrix) #ValueError: as_list() is not defined on an unknown TensorShape.

#...CRUCIAL...!!!!single value imgMatrix should be a list of single value

prepShape=imgMatrix_rsz.reshape(-1,width,height,3) #reshape to prepShape to indicate array of single value
print(f'img prepa shape is {prepShape.shape}')

y_predicting=box_model.predict(prepShape) #reshape to prepShape to indicate array of single value
y_predicting

y_predicting.shape

y_predicting[0][:5]

y_predicting[0].shape

prepShape.shape

prepShape[0].shape

cv2_imshow(prepShape[0])

classJay

"""####single_prepShape
```
randKey=np.random.choice(list(y_true.keys()))
pathSingle=randKey
y_trueSingle=y_true[randKey]
imgMatrix=cv2.imread(pathSingle)
print(imgMatrix.shape)
cv2_imshow(imgMatrix)
imgMatrix_rsz=cv2.resize(imgMatrix, (128,128))
width=imgMatrix_rsz.shape[0]
height=imgMatrix_rsz.shape[1]
prepShape=imgMatrix_rsz.reshape(-1,width,height,3)
print(f'img prepa shape is {prepShape.shape}')
y_predicting=box_model.predict(prepShape)
#y_predicting
#cv2_imshow(prepShape[0])
```
"""

def single_prepShape(y_true):
  randKey=np.random.choice(list(y_true.keys()))
  pathSingle=randKey
  y_trueSingle=y_true[randKey]
  imgMatrix=cv2.imread(pathSingle)
  print(imgMatrix.shape)
  cv2_imshow(imgMatrix)
  imgMatrix_rsz=cv2.resize(imgMatrix, (128,128))
  width=imgMatrix_rsz.shape[0]
  height=imgMatrix_rsz.shape[1]
  prepShape=imgMatrix_rsz.reshape(-1,width,height,3)
  print(f'img prepa shape is {prepShape.shape}')
  #predict
  #y_predicting=box_model.predict(prepShape)
  #y_predicting
  #cv2_imshow(prepShape[0])
  return pathSingle, imgMatrix, prepShape,  y_trueSingle

pathSingle, imgMatrix, prepShape,  y_trueSingle=single_prepShape(y_true)

y_trueSingle

y_trueSingle[0], y_trueSingle[1], y_trueSingle[2], y_trueSingle[3], y_trueSingle[4], y_trueSingle[5], y_trueSingle[6],y_trueSingle[7]

prepShape.shape

imgMatrix.shape

imgMatrix.shape[0]

y_trueSingle[:5]

list(y_trueSingle[5:])

np.argmax(list(y_trueSingle[5:]))

#!!!...Working on SENSIBLE Display...
def show_yTrueSingle_DJSc(y_true, threshold=0.1):

    pathSingle, imgMatrix, prepShape,  y_trueSingle=single_prepShape(y_true)

    cloneLocal=imgMatrix.copy()
    '''
    clone=orig.copy() # from groundTruth generator
    orig=cv2.resize(orig, (400,400)) #...!!!,,,Crucial resize to be above (224,224) and avoid error
    #print(f'[DJSc_test] rsz_orig shape is... {orig.shape}') #DJSc_test
    '''
    #..Hence resize cloneLocal to 400x400 for rectangle fitting
    clone_rsz=cv2.resize(cloneLocal, (400,400))

    y= y_trueSingle
    prob, x1, y1, x2, y2 = y[:5]
    print(f'[DJSc_test_test] pathSingle is ... {pathSingle}')
    print(f'[DJSc_test_test] prob is ... {y[0]}')
    print(f'[DJSc_test_test] x1 is ... {y[1]}')
    print(f'[DJSc_test_test] y1 is ... {y[2]}')
    print(f'[DJSc_test_test] x2 is ... {y[3]}')
    print(f'[DJSc_test_test] y2 is ... {y[4]}')
    print(f'[DJSc_test_test] channels are... {y}')
    #scale=32
    #scale=1
    #scale=clone.shape[1]/32
    width=clone_rsz.shape[0]
    print('width is ...',width)
    height=clone_rsz.shape[1]
    print('height is ...',height)
    starX=int(x1)
    starY=int(y1)
    endX=int(x2)
    endY=int(y2)

    # Window name in which image is displayed
    #window_name = 'Image'

    # if prob < threshold we won't show any thing

    #Uncomment after box_model(fit)training
    #if prob < threshold:
      #return

    # label
    cv2.rectangle(clone_rsz, (startX, startY), (endX, endY),(0, 255, 0), 2)
    kls = np.argmax(list(y[5:]))
    print(  )
    print(f'[DJSc_test_test] clone_rsz shape is ... {clone_rsz.shape}')
    kls_name=classJay[kls]
    print(f'[DJSc_test_test] kls_name is ... {kls_name}')
    alt_startY = startY - 10 if startY - 10 > 10 else startY + 10
    cv2.putText(clone_rsz, f'{kls_name}', (startX,alt_startY), cv2.FONT_HERSHEY_PLAIN, 1.3, (0.0, 255, 0.0),2)
    #cv2.putText(clone, label, (startX, alt_startY),cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 255, 0), 2)
    cv2_imshow(clone_rsz)
    #plt.imshow(clone_rsz)
    #plt.imshow(window_name, clone)
#Displayed image does not make any sense due to 8x8 observation...!!!!

show_yTrueSingle_DJSc(y_true)

prepShape[0]

"""###TRAINING `box_model.fit()` (2025May2nd)"""

def mass_prepShape(y_true):
  path_array=[]
  imgMatrix_array=[]
  #prepShape_array=[] ...array of prepshape  ...Not needed.. as imgMatrix array yield an array of values
  y_true_channels=[]
  for path in y_true.keys():
    imgMatrix=cv2.imread(path)
    #print('imgMatrix shape is...',imgMatrix.shape)
    #cv2_imshow(imgMatrix)
    imgMatrix_rsz=cv2.resize(imgMatrix, (128,128))
    width=imgMatrix_rsz.shape[0]
    height=imgMatrix_rsz.shape[1]
    #prepShape=imgMatrix_rsz.reshape(-1,width,height,3)#...array of prepshape  ...Not needed..
    #print(f'img prepa shape is... {prepShape.shape}')
    path_array.append(path)
    imgMatrix_array.append(imgMatrix_rsz)
    #prepShape_array.append(prepShape) #...array of prepshape  ...Not needed.. as imgMatrix array yield an array of values
    y_true_channels.append(y_true[path])
  print('imgMatrix_rsz shape is...',imgMatrix_rsz.shape)
  print('img prepShape is not needed...!!!...as imgMatrix_array yield an array of values')
  #print(f'imgMatrix_array shape is... {imgMatrix_array.shape}') #Error on yielding shape
  #mass predict
  #y_predicting=box_model.predict(imgMatrix_array)
  #y_predicting
  #cv2_imshow(imgMatrix)
  return path_array, imgMatrix_array, y_true_channels

path_array, imgMatrix_array, y_true_channels=mass_prepShape(y_true)

len(imgMatrix_array),imgMatrix_array[0].shape

X_train=np.array(imgMatrix_array) #POWERFUL Numpy Array<np.array()> again...!!! to yield X_train.shape for cardinality.
len(X_train),X_train[0].shape, X_train.shape

X_train[13].shape# CHECK...for cardinality... to avoid error..
#ValueError: Data cardinality is ambiguous. Make sure all arrays contain the same number of samples.'x' sizes: 128, 128,
# 128, 128, 128, 128, 128, 128, 128, 128, 128, 150, 150, 150, 128, 128, 128, 128
#'y' sizes: 18

#len(y_true_channels),y_true_channels[:2]

#y_train=y_true_channels
y_train=np.array(list(y_true.values())) #POWERFUL Numpy Array<np.array()> ...!!! to yield y_train.shape for cardinality.
#y_train

y_train.shape

#y_true # uncomment when necessary

X_train=tf.convert_to_tensor(X_train, dtype=tf.float32)
y_train=tf.convert_to_tensor(y_train, dtype=tf.float32)
#to avoid below error
'''
TypeError: Expected float32, but got
<tensorflow.python.eager.polymorphic_function.polymorphic_function.Function object at 0x78706b2b8350> of type 'Function'
'''
#box_model.fit(X_train, y_train, epochs=30)
box_model.fit(X_train, y_train, epochs=20)

import os
os.makedirs('./box_model', exist_ok=True)
#box_model.save('./box_model/hdf5_boXmodel.h5')
box_model.save('./box_model/keras_boXmodel.keras')

ls



"""ValueError REMEDY
```
ValueError: The `{arg_name}` of this `Lambda` layer is a Python lambda. Deserializing it is unsafe. If you trust the source of the config artifact, you can override this error by passing `safe_mode=False` to `from_config()`, or calling `keras.config.enable_unsafe_deserialization().

```

```
import tensorflow as tf
import keras
from keras.layers import Input, Lambda
from keras.models import Model

# Define the Lambda function separately
def my_lambda_function(x):
    # Your lambda function logic here
    return x * 2

# Create the model
inputs = Input(shape=(10,))
outputs = Lambda(my_lambda_function)(inputs)
box_model = Model(inputs=inputs, outputs=outputs)

# Save the model using SavedModel format
box_model.save('./box_model')

# ... later, load the model
loaded_model = tf.keras.models.load_model('./box_model')

```
"""

ls box_model

"""TypeError DEBUG
```
TypeError: Unable to reconstruct an instance of 'Function' because the class is missing a `from_config()` method. Full object config: {'module': 'tensorflow.python.eager.polymorphic_function.polymorphic_function', 'class_name': 'Function', 'config': 'loss_func', 'registered_name': 'Function'}
```

```
#import tensorflow as tf

def loss_func(y_true, y_pred):
    # Your custom loss function logic here
    pass

# Load the model with custom objects
l_kboXmodel = tf.keras.models.load_model(
    './box_model/keras_boXmodel.keras',
    custom_objects={'loss_func': loss_func}
)
print(l_kboXmodel.outputs)

```

```
#import tensorflow as tf
idx_p = [0]
idx_bb = [1, 2, 3, 4]
idx_cls = [5, 6, 7, 8, 9, 10, 11]

@tf.function
def loss_bb(y_true, y_pred):
    y_true = tf.gather(y_true, idx_bb, axis=-1)
    y_pred = tf.gather(y_pred, idx_bb, axis=-1)

    #loss = tf.keras.losses.mean_squared_error(y_true, y_pred)
    #AttributeError: module 'keras._tf_keras.keras.losses' has no attribute 'mean_squared_error'

    #Gr8 WALL...toFIX...!!!
    print(f'[DJSc] y_true shape is {y_true.shape}')
    print(f'[DJSc] 2nd item y_true[1] shape is {y_true[1].shape}')
    print(f'[DJSc] 2nd item y_true[1] length is {len(y_true[1])}')
    print(f'[DJSc] 2nd item y_true[1] is {y_true[1]}')

    print(f'[DJSc] y_pred shape is {y_pred.shape}')

    loss=tf.keras.losses.mse(y_true, y_pred)
    print(f'[DJSc] loss shape is {loss.shape}')
    print(f'[DJSc] loss is {loss}')

    #loss = tf.keras.losses.MeanSquaredError()(y_true, y_pred)
    '''
    '''
    The error message "ValueError: mask cannot be scalar" indicates that the tf.boolean_mask
    function in the loss_bb function is receiving a scalar value for the mask argument,
    when it expects a tensor with at least one dimension. This is happening because the loss variable,
    calculated using tf.keras.losses.MeanSquaredError()(y_true, y_pred), is a scalar representing the mean squared
    error across all elements of the tensors. Subsequently,
    the mask variable, calculated as loss > 0.0, is also a scalar (True or False).
    '''
    '''
    #...EUREKA...!!!...I have been using the wrong `MeanSquaredError`....
    #loss = tf.keras.losses.MeanSquaredError()(y_true, y_pred)[lambda x: tf.where(x>0.0, loss[x])]
    #print(f'[DJSc] loss is {loss} ')
    #print(f'[DJSc] loss length are {len(loss)} ')
    #print(f'[DJSc] loss shape is {loss.shape}')
    #condition=loss[:,0]>0.0
    mask=loss>0.0
    #print(f'[DJSc] condition is {condition} ')
    print(f'[DJSc] mask is {mask} ')
    #lossWhere=tf.boolean_mask(loss, condition)
    lossWhere=tf.boolean_mask(loss, mask)
    #lossWhere=tf.boolean_mask(loss, loss>0.0)
    print(f'[DJSc] lossWhere is {lossWhere}')
    #print(f'[DJSc] x is {x} & lossWhere is {lossWhere} ')
    #cast_lossWhere= tf.dtypes.cast(lossWhere, tf.float32)
    #print(f'[DJSc] cast_lossWhere is {cast_lossWhere} ')
    redM_lossWhere= tf.reduce_mean(lossWhere)
    #redM_cast_lossWhere= tf.reduce_mean(cast_lossWhere)
    print(f'[DJSc] tf.reduce_mean(cast_lossWhere) is {redM_lossWhere} ')
    #print(f'[DJSc] tf.reduce_mean(cast_lossWhere) is {redM_cast_lossWhere} ')
    return redM_lossWhere
    #return redM_cast_lossWhere
    #return tf.reduce_mean(tf.cast((lambda x: tf.where(x>0.0, loss[x])), tf.float32))
    #return tf.reduce_mean(loss[loss > 0.0])
    #return tf.reduce_mean(loss)

@tf.function
def loss_p(y_true, y_pred):
    y_true = tf.gather(y_true, idx_p, axis=-1)
    y_pred = tf.gather(y_pred, idx_p, axis=-1)

    loss = tf.losses.binary_crossentropy(y_true, y_pred)
    return tf.reduce_sum(loss)

@tf.function
def loss_cls(y_true, y_pred):
    y_true = tf.gather(y_true, idx_cls, axis=-1)
    y_pred = tf.gather(y_pred, idx_cls, axis=-1)

    loss = tf.losses.binary_crossentropy(y_true, y_pred)
    return tf.reduce_sum(loss)

@tf.function
def loss_func(y_true, y_pred):
  return loss_bb(y_true, y_pred) + loss_p(y_true, y_pred) + loss_cls(y_true, y_pred)

  pass
'''
```
"""

l_kboXmodel = tf.keras.models.load_model( './box_model/keras_boXmodel.keras')

print(l_kboXmodel.outputs)

'''
loss_func.get_config = lambda: {}  # Returns an empty config for simplicity

@classmethod
def from_config(cls, config):
    return cls # Assuming no configuration is needed for reconstruction

loss_func.from_config = from_config

# Load the model with custom objects
#l_kboXmodel = tf.keras.models.load_model( './box_model/keras_boXmodel.keras',
    #custom_objects={'loss_func': djsc_loss})
'''

#l_kboXmodel = tf.keras.models.load_model( './box_model/keras_boXmodel.keras')

#print(l_kboXmodel.outputs)

'''
TypeError: Could not locate function 'djsc_loss'.
Make sure custom classes are decorated with `@keras.saving.register_keras_serializable()`.
Full object config: {'module': 'builtins', 'class_name': 'function', 'config': 'djsc_loss', 'registered_name': 'functio

'''

#from keras.models import load_model
#l_h5boXmodel = load_model('./box_model/hdf5_boXmodel.h5')
#l_kboXmodel = load_model('./box_model/keras_boXmodel.keras')
#l_kboXmodel = tf.keras.models.load_model('./box_model/keras_boXmodel.keras')
#print(l_kboXmodel.outputs)

"""Custom Loss functions
https://nulldog.com/custom-loss-functions-in-keras-a-complete-guide
"""

path_array[0], imgMatrix_array[0].shape, y_true_channels[0]



"""### Show MASS Predict from TEST Data

####Generate boxTEST Data (boxX_test) from...
...train_X random pics choice ( as groundTruth is generated from `test_ALLdata`...mapped to `testALL_y` & `ALLtest_SubCateg`)
"""

# initialize our list of output images
images_Matrix = []

'''
#Map testX to testALL_y
#mapTestXto_ALLy=dict(zip(test_ALLdataJay,testALL_y))
mapTestXto_ALLtestSC=dict(zip(test_ALLdataJay,ALLtest_SubCateg))
'''

#Map train_X to testALL_y
mapTestXto_ALLy=dict(zip(test_ALLdataJay,testALL_y))
mapTestXto_ALLtestSC=dict(zip(test_ALLdataJay,ALLtest_SubCateg))

#partition
def pSplit_partition():
  pSplit=int(input('Please PROVIDE percentage of Reduction for a total of {} Training records:  '.format(len(train_X))))
  trainSel=np.random.choice(np.arange(0,len(train_X)), size=len(train_X)*pSplit//100)
  trainX, trainY=train_X[trainSel], train_y[trainSel]
  return trainX, trainY

mergedTEST_INTEL_FFHQ_df

mergedTRAIN_INTEL_FFHQ_df

Xtrain4boxTestPath_array=mergedTRAIN_INTEL_FFHQ_df.trainPath.values
len(Xtrain4boxTestPath_array)
#Xtrain4boxTestPath_array

Xtrain4boxTestSubCateg_array=mergedTRAIN_INTEL_FFHQ_df.SubCateg.values
len(Xtrain4boxTestSubCateg_array)
#Xtrain4boxTestSubCateg_array

Xtrain4boxTestSubCateg_array[[2,200,300,900]]

mapXtrainPathTest_toSubC=dict(zip(Xtrain4boxTestPath_array,Xtrain4boxTestSubCateg_array))
#mapXtrainPathTest_toSubC

pSp=float(input('Please PROVIDE percentage of Selection for a total of {} Xtrain4boxTestPath records:  '.format(len(Xtrain4boxTestPath_array))))
len(Xtrain4boxTestPath_array)*pSp//100

#pSplit-selection for Xtrain4boxTest

def pSplit_selection_Xtrain4boxTest():
  pSp=float(input('Please PROVIDE percentage of Selection for a total of {} Xtrain4boxTestPath records:  '.format(len(Xtrain4boxTestPath_array))))
  testSel=np.random.choice(np.arange(0,len(Xtrain4boxTestPath_array)), size=int(len(Xtrain4boxTestPath_array)*pSp//100))
  Xtrain4boxTestPath_selection=Xtrain4boxTestPath_array[testSel]
  selectSize=len(Xtrain4boxTestPath_selection)
  print(f'Xtrain4boxTestPath_selection size is... {selectSize}')
  return Xtrain4boxTestPath_selection, testSel, selectSize

Xtrain4boxTestPath_selection, testSel, selectSize=pSplit_selection_Xtrain4boxTest()
#len(Xtrain4boxTestPath_selection) #, Xtrain4boxTestPath_selection

Xtrain4boxTestPath_selection[:2]

"""`build_montages`
https://pyimagesearch.com/2017/05/29/montages-with-opencv/
```
# initialize the list of images
images = []
# loop over the list of image paths
for imagePath in imagePaths:
	# load the image and update the list of images
	image = cv2.imread(imagePath)
	images.append(image)
# construct the montages for the images
montages = build_montages(images, (128, 196), (7, 3))

# loop over the montages and display each of them
for montage in montages:
	cv2.imshow("Montage", montage)
	cv2.waitKey(0)

```
"""

#!!!...Working on SENSIBLE Display...
def show_massPredictDisplay_DJSc(threshold=0.1):

  from keras import backend as K
  from tqdm import tqdm #jayAdd for progress bar taqaduma('I love U so much')
  from google.colab.patches import cv2_imshow
  from imutils import build_montages
  from sklearn.metrics import accuracy_score

  #trying to view predicted names of randomly selected images from data
  Xtrain4boxTestPath_selection, testSel, selectSize=pSplit_selection_Xtrain4boxTest()

  mapXtrainPathTest_toSubC=dict(zip(Xtrain4boxTestPath_array,Xtrain4boxTestSubCateg_array))

   # initialize our list of output images
  images_Matrices = []
  #jayAdd for evaluation metrics
  #testYtrack=[]
  predsTrack=[]
  failure=0

  print(f'box_model name is... {box_model.name}')

  imgNum=len(Xtrain4boxTestPath_selection)
  images4viz=[]

  for i in range(len(Xtrain4boxTestPath_selection)):
    pathSingle=Xtrain4boxTestPath_selection[i]
    #print(pathSingle)
    imgMatrix=cv2.imread(pathSingle)
    #print(imgMatrix.shape)
    #cv2_imshow(imgMatrix)
    imgMatrix_rsz=cv2.resize(imgMatrix, (128,128))
    images_Matrices.append(imgMatrix_rsz)

  print(f'imgMatrix_rsz shape is... {imgMatrix_rsz.shape}')
  print(f'images_Matrices shape is... {np.array(images_Matrices).shape}')

  predsTrack=box_model.predict(np.array(images_Matrices))

  for i, preds in enumerate(predsTrack):
    #print(i,preds)
    cloneLocal=images_Matrices[i].copy()
    #cloneLocal=imgMatrix.copy()
    '''
    clone=orig.copy() # from groundTruth generator
    orig=cv2.resize(orig, (400,400)) #...!!!,,,Crucial resize to be above (224,224) and avoid error
    #print(f'[DJSc_test] rsz_orig shape is... {orig.shape}') #DJSc_test

    ....Maybe NA....
    '''
    #..Hence resize cloneLocal to 400x400 for rectangle fitting
    clone_rsz=cv2.resize(cloneLocal, (400,400))
    #clone_rsz=cv2.resize(cloneLocal, (300,300))
    #clone_rsz=cloneLocal

    y= preds
    #y= y_trueSingle
    prob, x1, y1, x2, y2 = preds[:5]
    #print(f'[DJSc_INFO] pathSingle is ... {Xtrain4boxTestPath_selection[i]}')
    #print(f'[DJSc_INFO] pathSingle is ... {pathSingle}')
    #print(f'[DJSc_INFO] prob is ... {y[0]}')
    #print(f'[DJSc_INFO] x1 is ... {y[1]}')
    #print(f'[DJSc_INFO] y1 is ... {y[2]}')
    #print(f'[DJSc_INFO] x2 is ... {y[3]}')
    #print(f'[DJSc_INFO] y2 is ... {y[4]}')
    #print(f'[DJSc_INFO] channels are... {y}')
    #scale=32
    #scale=1
    #scale=clone.shape[1]/32
    W=clone_rsz.shape[0]
    #print('clone_rsz width (W) is ...',width)
    H=clone_rsz.shape[1]
    #print('clone_rsz height (H) is ...',height)
    local_width=cloneLocal.shape[0]
    local_height=cloneLocal.shape[1]
    #print('local_width is ...',local_width)
    #print('local_height is ...',local_height)
    scale=W/local_width #...CRUCIAL Scale for Rectangle fit..!!
    #print('scale is ...',scale)
    x1=int(x1*scale)
    y1=int(y1*scale)
    x2=int(x2*scale)
    y2=int(y2*scale)
    #starX=int(x1)
    #starY=int(y1)
    #endX=int(x2)
    #endY=int(y2)



    # if prob < threshold we won't show any thing

    #Uncomment after box_model(fit)training
    #if prob < threshold:
      #continue
      #return

    # label
    kls = np.argmax(list(y[5:]))
    kls_name=classJay[kls]

    if kls_name!=mapXtrainPathTest_toSubC[Xtrain4boxTestPath_selection[i]]:

      cv2.rectangle(clone_rsz, (startX, startY), (endX, endY),(0, 0, 255), 2)
      kls = np.argmax(list(y[5:]))
      #print(  )
      #print(f'[DJSc_test_test] clone_rsz shape is ... {clone_rsz.shape}')
      kls_name=classJay[kls]
      #print(f'[DJSc_test_test] kls_name is ... {kls_name}')

      #print(f'[DJSc_test_test] kls_name is ... {kls_name}')
      #print(f'[DJSc_test_test] mapXtrainPathTest_toSubC[{Xtrain4boxTestPath_selection[i]}] is ... {mapXtrainPathTest_toSubC[Xtrain4boxTestPath_selection[i]]}')
      failure=failure+1
      alt_startY = startY - 10 if startY - 10 > 10 else startY + 10
      #alt_startY = startY - 12 if startY - 12 > 12 else startY + 12
      cv2.putText(clone_rsz, f'{kls_name}', (startX,alt_startY), cv2.FONT_HERSHEY_PLAIN, 1.5, (0.0,0.0,255),2)

      images4viz.append(clone_rsz)

    else:

      cv2.rectangle(clone_rsz, (startX, startY), (endX, endY),(0, 255, 0), 2)
      kls = np.argmax(list(y[5:]))
      #print(  )
      #print(f'[DJSc_test_test] clone_rsz shape is ... {clone_rsz.shape}')
      kls_name=classJay[kls]
      #print(f'[DJSc_test_test] kls_name is ... {kls_name}')

      alt_startY = startY - 10 if startY - 10 > 10 else startY + 10
      #alt_startY = startY - 6 if startY - 6 > 6 else startY +6
      cv2.putText(clone_rsz, f'{kls_name}', (startX,alt_startY), cv2.FONT_HERSHEY_PLAIN, 1.5, (0.0, 255, 0.0),2)
      #cv2.putText(clone, label, (startX, alt_startY),cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 255, 0), 2)

      images4viz.append(clone_rsz)


  print('quantity of Failure is ...', failure)
  print(f'Success rate is...{(len(predsTrack)-failure)*100/len(predsTrack)}%')
  #DISPLAY
  # construct the MONTAGE for the images
  #canvasWidth=0 # initializing the width of the canvas, while the height is set to 6
  canvasHeight=0 # initializing the height of the canvas, while the width is set to 5
  if imgNum%5==0:
    #canvasWidth=imgNum//6
    canvasHeight=imgNum//5
  else:
    #canvasWidth=imgNum//6 +1
    canvasHeight=imgNum//5 +1

  #montage = build_montages(clone_rsz, (96, 96), (canvasWidth, 6))[0]
  montages = build_montages(images4viz, (300, 300), (5, canvasHeight))[0]

  #cv2_imshow(clone_rsz)
  cv2_imshow(montages)

#show_massPredictDisplay_DJSc(threshold=0.1)
show_massPredictDisplay_DJSc() #ADVISE: test around 50%

fall here...

"""###FROZEN Graph...

###SENSELESS Display

Maybe NOT Forced to use...
`def get_color_by_probability(p):` &  `def show_predict(X,y. threshold=0.1):` to view prediction

```
def get_color_by_probability(p):
    if p < 0.3:
        return (1., 0., 0.)
    if p < 0.7:
        return (1., 1., 0.)
    return (0., 1., 0.)
```

```
#!!!...Display SENSELESS...!!!
grid_size = 16  # image_size / mask_size
def show_predict(X, y, threshold=0.1):
    X = X.copy()
    for mx in range(8):
        for my in range(8):
            channels = y[my][mx]
            prob, x1, y1, x2, y2 = channels[:5]
            print(f'[DJSc_test_test] prob is ... {channels[0]}')
            print(f'[DJSc_test_test] x1 is ... {channels[1]}')
            print(f'[DJSc_test_test] y1 is ... {channels[2]}')
            print(f'[DJSc_test_test] x2 is ... {channels[3]}')
            print(f'[DJSc_test_test] y2 is ... {channels[4]}')
            print(f'[DJSc_test_test] channels are... {channels}')

            # if prob < threshold we won't show any thing
            if prob < threshold:
                continue

            color = get_color_by_probability(prob)
            # bounding box
            px, py = (mx * grid_size) + x1, (my * grid_size) + y1
            cv2.rectangle(X, (int(px), int(py)), (int(px + x2), int(py + y2)), color, 1)

            # label
            cv2.rectangle(X, (int(px), int(py - 10)), (int(px + 12), int(py)), color, -1)
            kls = np.argmax(channels[5:])
            cv2.putText(X, f'{kls}', (int(px + 2), int(py-2)), cv2.FONT_HERSHEY_PLAIN, 0.7, (0.0, 0.0, 0.0))

    plt.imshow(X)
#Displayed image does not make any sense due to 8x8 observation...!!!!

```
"""



#!!!..SENSELESS Display...!!!
grid_size = 16  # image_size / mask_size
def show_predict(X, y, threshold=0.1):
    X = X.copy()
    for mx in range(8):
        for my in range(8):
            channels = y[my][mx]
            prob, x1, y1, x2, y2 = channels[:5]
            print(f'[DJSc_test_test] prob is ... {channels[0]}')
            print(f'[DJSc_test_test] x1 is ... {channels[1]}')
            print(f'[DJSc_test_test] y1 is ... {channels[2]}')
            print(f'[DJSc_test_test] x2 is ... {channels[3]}')
            print(f'[DJSc_test_test] y2 is ... {channels[4]}')
            print(f'[DJSc_test_test] channels are... {channels}')

            # if prob < threshold we won't show any thing
            if prob < threshold:
                continue

            color = get_color_by_probability(prob)
            # bounding box
            px, py = (mx * grid_size) + x1, (my * grid_size) + y1
            cv2.rectangle(X, (int(px), int(py)), (int(px + x2), int(py + y2)), color, 1)

            # label
            cv2.rectangle(X, (int(px), int(py - 10)), (int(px + 12), int(py)), color, -1)
            kls = np.argmax(channels[5:])
            cv2.putText(X, f'{kls}', (int(px + 2), int(py-2)), cv2.FONT_HERSHEY_PLAIN, 0.7, (0.0, 0.0, 0.0))

    plt.imshow(X)
#Displayed image does not make any sense due to 8x8 observation...!!!!

y_predicting.shape

y_predicting[0].shape

prepShape.shape

prepShape[0].shape

#show_predict(prepShape[0], y_predicting[0]) # default threshold=0.1

threshold=0.2
#show_predict(X[0], y[0], threshold=threshold)

"""###let’s use a function `view_prediction` for mass preview:

####Model reference
As we mentioned before, this is a Convolutional Neural Network (CNN).
Therefore, the bottom of the model is nothing special.
( We use ReLU as activation function )

Here are the codes of the Model, I will explain it.

```
x = x_input = layers.Input(shape=(128, 128, 3))

x = layers.Conv2D(32, kernel_size=3, padding='same', activation='relu')(x)
x = layers.MaxPool2D()(x)
x = layers.BatchNormalization()(x) # size: 64x64

x = layers.Conv2D(32, kernel_size=3, padding='same', activation='relu')(x)
x = layers.BatchNormalization()(x)  # size: 64x64

x = layers.Conv2D(32, kernel_size=3, padding='same', activation='relu')(x)
x = layers.MaxPool2D()(x)
x = layers.BatchNormalization()(x)  # size: 32x32

x = layers.Conv2D(32, kernel_size=3, padding='same', activation='relu')(x)
x = layers.MaxPool2D()(x)
x = layers.BatchNormalization()(x)  # size: 16x16

x = layers.Conv2D(32, kernel_size=3, padding='same', activation='relu')(x)
x = layers.MaxPool2D()(x)
x = layers.BatchNormalization()(x) # size: 8x8x

# ---

x_prob = layers.Conv2D(1, kernel_size=3, padding='same', activation='sigmoid', name='x_prob')(x)
x_boxes = layers.Conv2D(4, kernel_size=3, padding='same', name='x_boxes')(x)
x_cls = layers.Conv2D(10, kernel_size=3, padding='same', activation='sigmoid', name='x_cls')(x)

# ---

#gate = tf.where(x_prob > 0.5, tf.ones_like(x_prob), tf.zeros_like(x_prob))
#x_boxes = x_boxes * gate
#x_cls = x_cls * gate
'''
ValueError: A KerasTensor cannot be used as input to a TensorFlow function.
A KerasTensor is a symbolic placeholder for a shape and dtype,
used when constructing Keras Functional models or Keras Functions.
You can only use it as input to a Keras layer or a Keras operation
(from the namespaces `keras.layers` and `keras.operations`). You are likely doing something like:

```
x = Input(...)
...
tf_fn(x)  # Invalid.
```
'''
# ---
# Wrap the TensorFlow operation `tf.where` inside a Lambda layer.
# This will ensure the operation is executed within the Keras symbolic graph.

x_prob_gate = layers.Lambda(lambda x: tf.where(x > 0.5, tf.ones_like(x), tf.zeros_like(x)))(x_prob)
x_boxes = layers.Multiply()([x_boxes, x_prob_gate])
x_cls = layers.Multiply()([x_cls, x_prob_gate])
# ---

x = layers.Concatenate()([x_prob, x_boxes, x_cls])

model = tf.keras.models.Model(x_input, x)
model.summary()

```

# for later...
"""

j_rois

DJSc_model.predict_on_batch(j_rois)

print(predictions.dtype)

#Debug...
for i, p in zip(predictions, probs):
  # grab the prediction information for the current ROI
  #(imagenetID, label, prob) = p[0]
  print(i)
  print(p)
  prob= p
  pred_labels[i]=classJay[i]
  print(pred_labels[i])

"""To apply NMS, we first extract the bounding boxes and associated prediction probabilities (proba) via Lines 159 and 160. We then pass those results into my imultils implementation of NMS (Line 161). For more details on non-maxima suppression, be sure to refer to my blog post.

After NMS has been applied, Lines 165-171 annotate bounding box rectangles and labels on the “after” image. Lines 174 and 175 display the results until a key is pressed, at which point all GUI windows close, and the script exits.

Great job! In the next section, we’ll analyze results of our method for using an image classifier for object detection purposes.

## **Image classifier to object detector results using Keras and TensorFlow**
At this point, we are ready to see the results of our hard work.

Make sure you use the ***“Downloads”*** section of this tutorial to download the source code and example images from this blog post.

From there, open up a terminal, and execute the following command:
```
Turning any CNN image classifier into an object detector with Keras, TensorFlow, and OpenCV
$ python detect_with_classifier.py --image images/stingray.jpg --size "(300, 150)"
```
OUTPUT
```
[INFO] loading network...
[INFO] looping over pyramid/windows took 0.19142 seconds
[INFO] classifying ROIs...
[INFO] classifying ROIs took 9.67027 seconds
[INFO] showing results for 'stingray'

```

####REMINDER
Project structure
Once you extract the .zip from the “Downloads” section of this blog post, your directory will be organized as follows:
```
Turning any CNN image classifier into an object detector with Keras, TensorFlow, and OpenCV
.
├── images
│   ├── hummingbird.jpg
│   ├── lawn_mower.jpg
│   └── stingray.jpg
├── pyimagesearch
│   ├── __init__.py
│   └── detection_helpers.py
└── detect_with_classifier.py
2 directories, 6 files
```
"""

pwd

imgBingo4jay_path

#cp ./originalPics/2002/07/20/big/img_624.jpg

#Turning any CNN image classifier into an object detector with Keras, TensorFlow, and OpenCV
#!python detect_with_classifier.py --image images/stingray.jpg --size "(300, 150)"
!python detect_with_classifier.py --image ./originalPics/2002/07/20/big/img_624.jpg --size "(300, 150)"

"""First, we end our pyramid + sliding window timer and show how long the process took (Lines 99-101).

Then, we take the ROIs and pass them (in batch) through our pre-trained image classifier (i.e., ResNet) via predict (Lines 104-118). As you can see, we print out a benchmark for the inference process here too.

Finally, Line 117 decodes the predictions, grabbing only the top prediction for each ROI.

We’ll need a means to map class labels (keys) to ROI locations associated with that label (values); the labels dictionary (Line 118) serves that purpose.

Let’s go ahead and populate our labels dictionary now:

```
# loop over the predictions
#L121 next line...
for (i, p) in enumerate(preds):
	# grab the prediction information for the current ROI
	(imagenetID, label, prob) = p[0]
	# filter out weak detections by ensuring the predicted probability
	# is greater than the minimum probability
  # L127 next...
	if prob >= args["min_conf"]:
		# grab the bounding box associated with the prediction and
		# convert the coordinates
		box = locs[i]
		# grab the list of predictions for the label and add the
		# bounding box and probability to the list
		L = labels.get(label, [])
		L.append((box, prob))
		labels[label] = L


```
"""

#REASSESS...
visual=0
# check to see if we are visualizing each of the sliding
# windows in the image pyramid2iB
#if args["visualize"] > 0:
if visual > 0:

	# clone the original image and then draw a bounding box
	# surrounding the current region
  for (x, y, roiOrig) in sliding_window(imgBingo, WIN_STEP, ROI_SIZE):
    # scale the (x, y)-coordinates of the ROI with respect to the
    # *original* image dimensions
    x = int(x * scale)
    y = int(y * scale)
    w = int(ROI_SIZE[0] * scale)
    h = int(ROI_SIZE[1] * scale)

	  #clone = orig.copy()
    clone = imgBingo_rsz.copy()
    cv2.rectangle(clone, (x, y), (x + w, y + h), (0, 255, 0), 2)

    #show the visualization and current ROI
    cv2_imshow(clone)
    cv2_imshow(roiOrig)
    cv2.waitKey(0)

#from google.colab import files
#files.upload()

pwd

ls

#!python detect_with_classifier.py --image images/stingray.jpg --size "(300, 150)"

"""Output
```
[INFO] loading network...
[INFO] looping over pyramid/windows took 0.19142 seconds
[INFO] classifying ROIs...
[INFO] classifying ROIs took 9.67027 seconds
[INFO] showing results for 'stingray'
```
"""



"""https://www.pyimagesearch.com/2020/06/22/turning-any-cnn-image-classifier-into-an-object-detector-with-keras-tensorflow-and-opencv/"""





"""**for Py file**
```
# import the necessary packages
import imutils

def sliding_window(image, step, ws):
	# slide a window across the image
	for y in range(0, image.shape[0] - ws[1], step):
		for x in range(0, image.shape[1] - ws[0], step):
			# yield the current window
			yield (x, y, image[y:y + ws[1], x:x + ws[0]])

```

```
def image_pyramid(image, scale=1.5, minSize=(224, 224)):
	# yield the original image
	yield image
	# keep looping over the image pyramid
	while True:
		# compute the dimensions of the next image in the pyramid
		w = int(image.shape[1] / scale)
		image = imutils.resize(image, width=w)
		# if the resized image does not meet the supplied minimum
		# size, then stop constructing the pyramid
		if image.shape[0] < minSize[1] or image.shape[1] < minSize[0]:
			break
		# yield the next image in the pyramid
		yield image

```

```
# import the necessary packages
from tensorflow.keras.applications import ResNet50
from tensorflow.keras.applications.resnet import preprocess_input
from tensorflow.keras.preprocessing.image import img_to_array
from tensorflow.keras.applications import imagenet_utils
from imutils.object_detection import non_max_suppression
from pyimagesearch.detection_helpers import sliding_window
from pyimagesearch.detection_helpers import image_pyramid
import numpy as np
import argparse
import imutils
import time
import cv2

```

```
# construct the argument parse and parse the arguments
ap = argparse.ArgumentParser()
ap.add_argument("-i", "--image", required=True,
	help="path to the input image")
ap.add_argument("-s", "--size", type=str, default="(200, 150)",
	help="ROI size (in pixels)")
ap.add_argument("-c", "--min-conf", type=float, default=0.9,
	help="minimum probability to filter weak detections")
ap.add_argument("-v", "--visualize", type=int, default=-1,
	help="whether or not to show extra visualizations for debugging")
args = vars(ap.parse_args())


```

```
# initialize variables used for the object detection procedure
WIDTH = 600
PYR_SCALE = 1.5
WIN_STEP = 16
ROI_SIZE = eval(args["size"])
INPUT_SIZE = (224, 224)

```
```
# load our network weights from disk
print("[INFO] loading network...")
model = ResNet50(weights="imagenet", include_top=True)
# load the input image from disk, resize it such that it has the
# has the supplied width, and then grab its dimensions
orig = cv2.imread(args["image"])
orig = imutils.resize(orig, width=WIDTH)
(H, W) = orig.shape[:2]

```
```
# initialize the image pyramid
pyramid = image_pyramid(orig, scale=PYR_SCALE, minSize=ROI_SIZE)
# initialize two lists, one to hold the ROIs generated from the image
# pyramid and sliding window, and another list used to store the
# (x, y)-coordinates of where the ROI was in the original image
rois = []
locs = []
# time how long it takes to loop over the image pyramid layers and
# sliding window locations
start = time.time()

```
```
# loop over the image pyramid
for image in pyramid:
	# determine the scale factor between the *original* image
	# dimensions and the *current* layer of the pyramid
	scale = W / float(image.shape[1])
	# for each layer of the image pyramid, loop over the sliding
	# window locations
	for (x, y, roiOrig) in sliding_window(image, WIN_STEP, ROI_SIZE):
		# scale the (x, y)-coordinates of the ROI with respect to the
		# *original* image dimensions
		x = int(x * scale)
		y = int(y * scale)
		w = int(ROI_SIZE[0] * scale)
		h = int(ROI_SIZE[1] * scale)
		# take the ROI and preprocess it so we can later classify
		# the region using Keras/TensorFlow
		roi = cv2.resize(roiOrig, INPUT_SIZE)
		roi = img_to_array(roi)
		roi = preprocess_input(roi)
		# update our list of ROIs and associated coordinates
		rois.append(roi)
		locs.append((x, y, x + w, y + h))

```
```
# check to see if we are visualizing each of the sliding
		# windows in the image pyramid
		if args["visualize"] > 0:
			# clone the original image and then draw a bounding box
			# surrounding the current region
			clone = orig.copy()
			cv2.rectangle(clone, (x, y), (x + w, y + h),
				(0, 255, 0), 2)
			# show the visualization and current ROI
			cv2.imshow("Visualization", clone)
			cv2.imshow("ROI", roiOrig)
			cv2.waitKey(0)

```
```
# show how long it took to loop over the image pyramid layers and
# sliding window locations
end = time.time()
print("[INFO] looping over pyramid/windows took {:.5f} seconds".format(
	end - start))
# convert the ROIs to a NumPy array
rois = np.array(rois, dtype="float32")
# classify each of the proposal ROIs using ResNet and then show how
# long the classifications took
print("[INFO] classifying ROIs...")
start = time.time()
preds = model.predict(rois)
end = time.time()
print("[INFO] classifying ROIs took {:.5f} seconds".format(
	end - start))
# decode the predictions and initialize a dictionary which maps class
# labels (keys) to any ROIs associated with that label (values)
preds = imagenet_utils.decode_predictions(preds, top=1)
labels = {}

```
```
# loop over the predictions
for (i, p) in enumerate(preds):
	# grab the prediction information for the current ROI
	(imagenetID, label, prob) = p[0]
	# filter out weak detections by ensuring the predicted probability
	# is greater than the minimum probability
	if prob >= args["min_conf"]:
		# grab the bounding box associated with the prediction and
		# convert the coordinates
		box = locs[i]
		# grab the list of predictions for the label and add the
		# bounding box and probability to the list
		L = labels.get(label, [])
		L.append((box, prob))
		labels[label] = L

```

```
# loop over the labels for each of detected objects in the image
for label in labels.keys():
	# clone the original image so that we can draw on it
	print("[INFO] showing results for '{}'".format(label))
	clone = orig.copy()
	# loop over all bounding boxes for the current label
	for (box, prob) in labels[label]:
		# draw the bounding box on the image
		(startX, startY, endX, endY) = box
		cv2.rectangle(clone, (startX, startY), (endX, endY),
			(0, 255, 0), 2)
	# show the results *before* applying non-maxima suppression, then
	# clone the image again so we can display the results *after*
	# applying non-maxima suppression
	cv2.imshow("Before", clone)
	clone = orig.copy()

```

```
	# extract the bounding boxes and associated prediction
	# probabilities, then apply non-maxima suppression
	boxes = np.array([p[0] for p in labels[label]])
	proba = np.array([p[1] for p in labels[label]])
	boxes = non_max_suppression(boxes, proba)
	# loop over all bounding boxes that were kept after applying
	# non-maxima suppression
	for (startX, startY, endX, endY) in boxes:
		# draw the bounding box and label on the image
		cv2.rectangle(clone, (startX, startY), (endX, endY),
			(0, 255, 0), 2)
		y = startY - 10 if startY - 10 > 10 else startY + 10
		cv2.putText(clone, label, (startX, y),
			cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 255, 0), 2)
	# show the output after apply non-maxima suppression
	cv2.imshow("After", clone)
	cv2.waitKey(0)

```

```



```

Apply path (2025Apr19th_review)
https://www.geeksforgeeks.org/applying-lambda-functions-to-pandas-dataframe/

`
df = df.assign(Product=lambda x: (x['Field_1'] * x['Field_2'] * x['Field_3']))
`
```
# Apply function numpy.square() to square
# the values of one row only i.e. row
# with index name 'd'
df = df.apply(lambda x: np.square(x) if x.name == 'd' else x, axis=1)
```
"""

#TRIAL
# importing pandas and numpy libraries
import pandas as pd
import numpy as np

# creating and initializing a nested list
values_list = [[15, 2.5, 100], [20, 4.5, 50], [25, 5.2, 80],
               [45, 5.8, 48], [40, 6.3, 70], [41, 6.4, 90],
               [51, 2.3, 111]]

# creating a pandas dataframe
df = pd.DataFrame(values_list, columns=['Field_1', 'Field_2', 'Field_3'],
                  index=['a', 'b', 'c', 'd', 'e', 'f', 'g'])


# Apply function numpy.square() to square
# the values of one row only i.e. row
# with index name 'd'
df = df.apply(lambda x: np.square(x) if x.name == 'd' else x, axis=1)


# printing dataframe
#df



"""#### OLD...Baseline version of define_model()
```


def Jay_baseline_CNN_W40cL6_224VGGnet():
  #len(Jay_baseline_CNN_W40cL6_224VGGnet().weights)...40
  #Total params: 24,342,023
  #Trainable params: 24,342,023
  #Non-trainable params: 0

  model =keras.Sequential(name='JaybaselineCNN_W40cL6_224VGG') # Our model  is initialized using the Sequential  API

    #CONVOLUTION Block_1st
  #3 layers => Max-pooling (RELU => stride 2 layers)
  #Input(I)=224, Filter(F)|Kernel(K)=3, Stride(S)=2 .... Output=[(I-F)/S]+1 =[224-3]/2 + 1=111.5 ~ 112
  #inputs=keras.Input(shape=(224,224,3))
  #model.add(keras.layers.Conv2D(64, (3, 3), name='jay1layer', activation='relu', kernel_initializer='he_uniform', padding='same'))
  model.add(keras.layers.Conv2D(64, (3, 3), name='jay1layer', activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(224,224,3))) #input_shape...CRUCIAL!!!
  model.add(keras.layers.Conv2D(64, (3, 3), name='jay2layer', activation='relu', kernel_initializer='he_uniform', padding='same'))
  model.add(keras.layers.Conv2D(64, (3, 3), name='jay3layer', activation='relu', kernel_initializer='he_uniform', padding='same'))
  #model.add(BatchNormalization())
  model.add(keras.layers.MaxPooling2D((2, 2)))

      #CONVOLUTION Block_2nd
  # 3 layers => Max-pooling (RELU => stride 2 layers)
    #inputs=keras.Input(shape=(112,112,128))
  model.add(keras.layers.Conv2D(128, (3, 3), name='jay4layer', activation='relu', kernel_initializer='he_uniform', padding='same'))
  #model.add(keras.layers.Conv2D(128, (3, 3), name='jay4layer', activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(224,224,3))) #input_shape...CRUCIAL!!!
  model.add(keras.layers.Conv2D(128, (3, 3), name='jay5layer', activation='relu', kernel_initializer='he_uniform', padding='same'))
  model.add(keras.layers.Conv2D(128, (3, 3), name='jay6layer', activation='relu', kernel_initializer='he_uniform', padding='same'))
  #model.add(BatchNormalization())
  model.add(keras.layers.MaxPooling2D((2, 2)))

    #CONVOLUTION Block_3rd
  # 3 layers => Max-pooling (RELU => stride 2 layers)
  #inputs=keras.Input(shape=(56,56,256))
  model.add(keras.layers.Conv2D(256, (3, 3), name='jay7layer', activation='relu', kernel_initializer='he_uniform', padding='same'))
  model.add(keras.layers.Conv2D(256, (3, 3), name='jay8layer', activation='relu', kernel_initializer='he_uniform', padding='same'))
  model.add(keras.layers.Conv2D(256, (3, 3), name='jay9layer', activation='relu', kernel_initializer='he_uniform', padding='same'))
  model.add(keras.layers.MaxPooling2D((2, 2)))

    #CONVOLUTION Block_4th
  # 3 layers => Max-pooling (RELU => stride 2 layers)
  #inputs=keras.Input(shape=(28,28,512))
  model.add(keras.layers.Conv2D(512, (3, 3), name='jay10layer', activation='relu', kernel_initializer='he_uniform', padding='same'))
  model.add(keras.layers.Conv2D(512, (3, 3), name='jay11layer', activation='relu', kernel_initializer='he_uniform', padding='same'))
  model.add(keras.layers.Conv2D(512, (3, 3), name='jay12layer', activation='relu', kernel_initializer='he_uniform', padding='same'))
  model.add(keras.layers.MaxPooling2D((2, 2)))

   #CONVOLUTION Block_5th
  # 3 layers => Max-pooling (RELU => stride 2 layers)
  #inputs=keras.Input(shape=(14,14,512))
  model.add(keras.layers.Conv2D(512, (3, 3), name='jay13layer', activation='relu', kernel_initializer='he_uniform', padding='same'))
  #model.add(keras.layers.Conv2D(512, (3, 3), name='jay13layer', activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(14,14,512)))
  model.add(keras.layers.Conv2D(512, (3, 3), name='jay14ayer', activation='relu', kernel_initializer='he_uniform', padding='same'))
  model.add(keras.layers.Conv2D(512, (3, 3), name='jay15layer', activation='relu', kernel_initializer='he_uniform', padding='same'))
  model.add(keras.layers.MaxPooling2D((2, 2)))

   #CONVOLUTION Block_6th
  # 3 layers => Max-pooling (RELU => stride 2 layers)
  #inputs=keras.Input(shape=(7,7,512))
  #model.add(keras.layers.Conv2D(512, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))

  model.add(keras.layers.Conv2D(512, (3, 3), name='jay16layer', activation='relu', kernel_initializer='he_uniform', padding='same'))
  model.add(keras.layers.Conv2D(512, (3, 3), name='jay17layer', activation='relu', kernel_initializer='he_uniform', padding='same'))
  #model.add(keras.layers.Conv2D(512, (3, 3), name='jay18layer', activation='relu', kernel_initializer='he_uniform', padding='same'))
  model.add(keras.layers.Conv2D(512, (3, 3), name='jay18layer', activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(7,7,3)))
  model.add(keras.layers.MaxPooling2D((2, 2)))


    #first (and only) set of FC (Final Coding) => RELU layers...below block CRUCIAL!!!
  model.add(keras.layers.Flatten()) # the output of the preceding MaxPooling2D  layer and flatten it into a single vector.
  model.add(keras.layers.Dense(512, activation='relu', kernel_initializer='he_uniform')) # Our fully-connected layer contains 128 nodes
  model.add(keras.layers.Dense(7, activation='softmax')) # another fully-connected layer, but this one is special; num nodes= num classes =7

    # compile model
    #opt = SGD(lr=0.001, momentum=0.9)
  opt=keras.optimizers.RMSprop(learning_rate=0.0001, decay=1e-6)
  model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])

  return model

```
"""

#### OLD...Baseline version of define_model()
```


def Jay_baseline_CNN_W40cL6_224VGGnet():
  #len(Jay_baseline_CNN_W40cL6_224VGGnet().weights)...40
  #Total params: 24,342,023
  #Trainable params: 24,342,023
  #Non-trainable params: 0

  model =keras.Sequential(name='JaybaselineCNN_W40cL6_224VGG') # Our model  is initialized using the Sequential  API

    #CONVOLUTION Block_1st
  #3 layers => Max-pooling (RELU => stride 2 layers)
  #Input(I)=224, Filter(F)|Kernel(K)=3, Stride(S)=2 .... Output=[(I-F)/S]+1 =[224-3]/2 + 1=111.5 ~ 112
  #inputs=keras.Input(shape=(224,224,3))
  #model.add(keras.layers.Conv2D(64, (3, 3), name='jay1layer', activation='relu', kernel_initializer='he_uniform', padding='same'))
  model.add(keras.layers.Conv2D(64, (3, 3), name='jay1layer', activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(224,224,3))) #input_shape...CRUCIAL!!!
  model.add(keras.layers.Conv2D(64, (3, 3), name='jay2layer', activation='relu', kernel_initializer='he_uniform', padding='same'))
  model.add(keras.layers.Conv2D(64, (3, 3), name='jay3layer', activation='relu', kernel_initializer='he_uniform', padding='same'))
  #model.add(BatchNormalization())
  model.add(keras.layers.MaxPooling2D((2, 2)))

      #CONVOLUTION Block_2nd
  # 3 layers => Max-pooling (RELU => stride 2 layers)
    #inputs=keras.Input(shape=(112,112,128))
  model.add(keras.layers.Conv2D(128, (3, 3), name='jay4layer', activation='relu', kernel_initializer='he_uniform', padding='same'))
  #model.add(keras.layers.Conv2D(128, (3, 3), name='jay4layer', activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(224,224,3))) #input_shape...CRUCIAL!!!
  model.add(keras.layers.Conv2D(128, (3, 3), name='jay5layer', activation='relu', kernel_initializer='he_uniform', padding='same'))
  model.add(keras.layers.Conv2D(128, (3, 3), name='jay6layer', activation='relu', kernel_initializer='he_uniform', padding='same'))
  #model.add(BatchNormalization())
  model.add(keras.layers.MaxPooling2D((2, 2)))

    #CONVOLUTION Block_3rd
  # 3 layers => Max-pooling (RELU => stride 2 layers)
  #inputs=keras.Input(shape=(56,56,256))
  model.add(keras.layers.Conv2D(256, (3, 3), name='jay7layer', activation='relu', kernel_initializer='he_uniform', padding='same'))
  model.add(keras.layers.Conv2D(256, (3, 3), name='jay8layer', activation='relu', kernel_initializer='he_uniform', padding='same'))
  model.add(keras.layers.Conv2D(256, (3, 3), name='jay9layer', activation='relu', kernel_initializer='he_uniform', padding='same'))
  model.add(keras.layers.MaxPooling2D((2, 2)))

    #CONVOLUTION Block_4th
  # 3 layers => Max-pooling (RELU => stride 2 layers)
  #inputs=keras.Input(shape=(28,28,512))
  model.add(keras.layers.Conv2D(512, (3, 3), name='jay10layer', activation='relu', kernel_initializer='he_uniform', padding='same'))
  model.add(keras.layers.Conv2D(512, (3, 3), name='jay11layer', activation='relu', kernel_initializer='he_uniform', padding='same'))
  model.add(keras.layers.Conv2D(512, (3, 3), name='jay12layer', activation='relu', kernel_initializer='he_uniform', padding='same'))
  model.add(keras.layers.MaxPooling2D((2, 2)))

   #CONVOLUTION Block_5th
  # 3 layers => Max-pooling (RELU => stride 2 layers)
  #inputs=keras.Input(shape=(14,14,512))
  model.add(keras.layers.Conv2D(512, (3, 3), name='jay13layer', activation='relu', kernel_initializer='he_uniform', padding='same'))
  #model.add(keras.layers.Conv2D(512, (3, 3), name='jay13layer', activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(14,14,512)))
  model.add(keras.layers.Conv2D(512, (3, 3), name='jay14ayer', activation='relu', kernel_initializer='he_uniform', padding='same'))
  model.add(keras.layers.Conv2D(512, (3, 3), name='jay15layer', activation='relu', kernel_initializer='he_uniform', padding='same'))
  model.add(keras.layers.MaxPooling2D((2, 2)))

   #CONVOLUTION Block_6th
  # 3 layers => Max-pooling (RELU => stride 2 layers)
  #inputs=keras.Input(shape=(7,7,512))
  #model.add(keras.layers.Conv2D(512, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))

  model.add(keras.layers.Conv2D(512, (3, 3), name='jay16layer', activation='relu', kernel_initializer='he_uniform', padding='same'))
  model.add(keras.layers.Conv2D(512, (3, 3), name='jay17layer', activation='relu', kernel_initializer='he_uniform', padding='same'))
  #model.add(keras.layers.Conv2D(512, (3, 3), name='jay18layer', activation='relu', kernel_initializer='he_uniform', padding='same'))
  model.add(keras.layers.Conv2D(512, (3, 3), name='jay18layer', activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(7,7,3)))
  model.add(keras.layers.MaxPooling2D((2, 2)))


    #first (and only) set of FC (Final Coding) => RELU layers...below block CRUCIAL!!!
  model.add(keras.layers.Flatten()) # the output of the preceding MaxPooling2D  layer and flatten it into a single vector.
  model.add(keras.layers.Dense(512, activation='relu', kernel_initializer='he_uniform')) # Our fully-connected layer contains 128 nodes
  model.add(keras.layers.Dense(7, activation='softmax')) # another fully-connected layer, but this one is special; num nodes= num classes =7

    # compile model
    #opt = SGD(lr=0.001, momentum=0.9)
  opt=keras.optimizers.RMSprop(learning_rate=0.0001, decay=1e-6)
  model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])

  return model

```